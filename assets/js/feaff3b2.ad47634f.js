"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[936],{2397:e=>{e.exports=JSON.parse('{"permalink":"/blog/local-ai-agent-deployment","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-08-20-local-ai-agent-deployment.md","source":"@site/blog/2025-08-20-local-ai-agent-deployment.md","title":"Complete Guide to Local AI Agent Deployment","description":"Introduction","date":"2025-08-20T00:00:00.000Z","tags":[{"inline":false,"label":"AI","permalink":"/blog/tags/ai-alt","description":"Artificial Intelligence"},{"inline":false,"label":"Machine Learning","permalink":"/blog/tags/machine-learning","description":"Machine learning techniques and applications"},{"inline":false,"label":"Deployment","permalink":"/blog/tags/deployment","description":"Software deployment and infrastructure"},{"inline":false,"label":"Tutorial","permalink":"/blog/tags/tutorial","description":"Tutorial tag description"},{"inline":false,"label":"Local Development","permalink":"/blog/tags/local-development","description":"Local development environments and tools"}],"readingTime":8.88,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"local-ai-agent-deployment","title":"Complete Guide to Local AI Agent Deployment","authors":["liangchao"],"tags":["ai","machine learning","deployment","tutorial","local development"]},"unlisted":false,"nextItem":{"title":"Curriculum Vitae","permalink":"/blog/curriculum-vitae"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}},8578:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var a=t(2397),i=t(4848),o=t(8453);const s={slug:"local-ai-agent-deployment",title:"Complete Guide to Local AI Agent Deployment",authors:["liangchao"],tags:["ai","machine learning","deployment","tutorial","local development"]},r="Complete Guide to Local AI Agent Deployment",l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Prerequisites",id:"software-prerequisites",level:3},{value:"Method 1: Ollama - The Simplest Approach",id:"method-1-ollama---the-simplest-approach",level:2},{value:"Installation",id:"installation",level:3},{value:"Basic Usage",id:"basic-usage",level:3},{value:"API Integration",id:"api-integration",level:3},{value:"Available Models",id:"available-models",level:3},{value:"Method 2: Docker-based Deployment",id:"method-2-docker-based-deployment",level:2},{value:"Create Docker Environment",id:"create-docker-environment",level:3},{value:"FastAPI Application",id:"fastapi-application",level:3},{value:"Method 3: LangChain with Local Models",id:"method-3-langchain-with-local-models",level:2},{value:"Setup LangChain Environment",id:"setup-langchain-environment",level:3},{value:"Method 4: Multi-Modal AI Agent",id:"method-4-multi-modal-ai-agent",level:2},{value:"Vision-Language Model Setup",id:"vision-language-model-setup",level:3},{value:"Method 5: RAG (Retrieval-Augmented Generation) System",id:"method-5-rag-retrieval-augmented-generation-system",level:2},{value:"Vector Database Setup",id:"vector-database-setup",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Model Quantization",id:"model-quantization",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:2},{value:"System Monitoring",id:"system-monitoring",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"API Security",id:"api-security",level:3},{value:"Input Sanitization",id:"input-sanitization",level:3},{value:"Deployment Scripts",id:"deployment-scripts",level:2},{value:"Automated Setup Script",id:"automated-setup-script",level:3},{value:"Systemd Service",id:"systemd-service",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Deploying AI agents locally offers numerous advantages including data privacy, reduced latency, cost control, and independence from cloud services. This comprehensive guide covers multiple approaches to setting up AI agents on your local infrastructure, from simple chatbots to complex multi-modal systems."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Minimum Configuration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CPU: 8-core processor (Intel i7/AMD Ryzen 7 or equivalent)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 16GB DDR4"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 100GB available SSD space"}),"\n",(0,i.jsx)(n.li,{children:"GPU: Optional but recommended (NVIDIA GTX 1060 or better)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recommended Configuration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CPU: 12+ core processor (Intel i9/AMD Ryzen 9 or equivalent)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 32GB+ DDR4/DDR5"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 500GB+ NVMe SSD"}),"\n",(0,i.jsx)(n.li,{children:"GPU: NVIDIA RTX 3080/4070 or better with 12GB+ VRAM"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"software-prerequisites",children:"Software Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Operating System: Ubuntu 20.04+, macOS 12+, or Windows 10/11"}),"\n",(0,i.jsx)(n.li,{children:"Docker and Docker Compose"}),"\n",(0,i.jsx)(n.li,{children:"Python 3.8+ with pip"}),"\n",(0,i.jsx)(n.li,{children:"Git"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA drivers (for GPU acceleration)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"method-1-ollama---the-simplest-approach",children:"Method 1: Ollama - The Simplest Approach"}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Linux/macOS:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.ai/install.sh | sh\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Windows:"}),"\nDownload and install from ",(0,i.jsx)(n.a,{href:"https://ollama.ai/download",children:"https://ollama.ai/download"})]}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Pull a model\nollama pull llama2\n\n# Run interactive chat\nollama run llama2\n\n# Start as service\nollama serve\n"})}),"\n",(0,i.jsx)(n.h3,{id:"api-integration",children:"API Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\n\ndef chat_with_ollama(message, model="llama2"):\n    url = "http://localhost:11434/api/generate"\n    payload = {\n        "model": model,\n        "prompt": message,\n        "stream": False\n    }\n    \n    response = requests.post(url, json=payload)\n    return response.json()["response"]\n\n# Example usage\nresponse = chat_with_ollama("Explain quantum computing")\nprint(response)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"available-models",children:"Available Models"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"llama2"}),": General purpose conversational AI"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"codellama"}),": Code generation and analysis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"mistral"}),": Efficient multilingual model"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"neural-chat"}),": Optimized for dialogue"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"llava"}),": Vision-language model"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"method-2-docker-based-deployment",children:"Method 2: Docker-based Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"create-docker-environment",children:"Create Docker Environment"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dockerfile:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'FROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\nEXPOSE 8000\n\nCMD ["python", "app.py"]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"requirements.txt:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"fastapi==0.104.1\nuvicorn==0.24.0\ntransformers==4.35.0\ntorch==2.1.0\naccelerate==0.24.1\nlangchain==0.0.335\nchromadb==0.4.15\nsentence-transformers==2.2.2\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"docker-compose.yml:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  ai-agent:\n    build: .\n    ports:\n      - "8000:8000"\n    volumes:\n      - ./models:/app/models\n      - ./data:/app/data\n    environment:\n      - CUDA_VISIBLE_DEVICES=0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  vector-db:\n    image: chromadb/chroma:latest\n    ports:\n      - "8001:8000"\n    volumes:\n      - ./chroma_data:/chroma/chroma\n'})}),"\n",(0,i.jsx)(n.h3,{id:"fastapi-application",children:"FastAPI Application"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"app.py:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport uvicorn\n\napp = FastAPI(title="Local AI Agent API")\n\nclass ChatRequest(BaseModel):\n    message: str\n    max_length: int = 512\n    temperature: float = 0.7\n\nclass AIAgent:\n    def __init__(self, model_name="microsoft/DialoGPT-medium"):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.model.to(self.device)\n        \n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def generate_response(self, message, max_length=512, temperature=0.7):\n        inputs = self.tokenizer.encode(message, return_tensors="pt").to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_length=max_length,\n                temperature=temperature,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response[len(message):].strip()\n\n# Initialize agent\nagent = AIAgent()\n\n@app.post("/chat")\nasync def chat(request: ChatRequest):\n    try:\n        response = agent.generate_response(\n            request.message,\n            request.max_length,\n            request.temperature\n        )\n        return {"response": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get("/health")\nasync def health_check():\n    return {"status": "healthy", "device": str(agent.device)}\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-3-langchain-with-local-models",children:"Method 3: LangChain with Local Models"}),"\n",(0,i.jsx)(n.h3,{id:"setup-langchain-environment",children:"Setup LangChain Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\n\nclass LocalAIAgent:\n    def __init__(self, model_path):\n        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n        \n        self.llm = LlamaCpp(\n            model_path=model_path,\n            temperature=0.7,\n            max_tokens=512,\n            top_p=1,\n            callback_manager=callback_manager,\n            verbose=True,\n            n_ctx=2048,\n            n_gpu_layers=35  # Adjust based on your GPU\n        )\n        \n        self.memory = ConversationBufferMemory()\n        \n        template = """\n        You are a helpful AI assistant. Have a conversation with the human.\n        \n        Current conversation:\n        {history}\n        Human: {input}\n        AI Assistant:"""\n        \n        prompt = PromptTemplate(\n            input_variables=["history", "input"],\n            template=template\n        )\n        \n        self.conversation = ConversationChain(\n            llm=self.llm,\n            memory=self.memory,\n            prompt=prompt,\n            verbose=True\n        )\n    \n    def chat(self, message):\n        return self.conversation.predict(input=message)\n\n# Usage\nagent = LocalAIAgent("./models/llama-2-7b-chat.gguf")\nresponse = agent.chat("What is machine learning?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-4-multi-modal-ai-agent",children:"Method 4: Multi-Modal AI Agent"}),"\n",(0,i.jsx)(n.h3,{id:"vision-language-model-setup",children:"Vision-Language Model Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nclass MultiModalAgent:\n    def __init__(self):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        # Load vision-language model\n        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\n        self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n        self.model.to(self.device)\n    \n    def analyze_image(self, image_path_or_url, question=None):\n        # Load image\n        if image_path_or_url.startswith(\'http\'):\n            response = requests.get(image_path_or_url)\n            image = Image.open(BytesIO(response.content))\n        else:\n            image = Image.open(image_path_or_url)\n        \n        if question:\n            # Visual question answering\n            inputs = self.processor(image, question, return_tensors="pt").to(self.device)\n            out = self.model.generate(**inputs, max_length=50)\n            answer = self.processor.decode(out[0], skip_special_tokens=True)\n            return answer\n        else:\n            # Image captioning\n            inputs = self.processor(image, return_tensors="pt").to(self.device)\n            out = self.model.generate(**inputs, max_length=50)\n            caption = self.processor.decode(out[0], skip_special_tokens=True)\n            return caption\n\n# Usage\nagent = MultiModalAgent()\ncaption = agent.analyze_image("path/to/image.jpg")\nanswer = agent.analyze_image("path/to/image.jpg", "What color is the car?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-5-rag-retrieval-augmented-generation-system",children:"Method 5: RAG (Retrieval-Augmented Generation) System"}),"\n",(0,i.jsx)(n.h3,{id:"vector-database-setup",children:"Vector Database Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\n\nclass RAGAgent:\n    def __init__(self, documents_path, persist_directory="./chroma_db"):\n        # Initialize embeddings\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name="sentence-transformers/all-MiniLM-L6-v2"\n        )\n        \n        # Load and process documents\n        loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader)\n        documents = loader.load()\n        \n        # Split documents\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        texts = text_splitter.split_documents(documents)\n        \n        # Create vector store\n        self.vectorstore = Chroma.from_documents(\n            documents=texts,\n            embedding=self.embeddings,\n            persist_directory=persist_directory\n        )\n        \n        # Initialize LLM (using Ollama)\n        from langchain.llms import Ollama\n        self.llm = Ollama(model="llama2")\n    \n    def query(self, question, k=3):\n        # Retrieve relevant documents\n        docs = self.vectorstore.similarity_search(question, k=k)\n        \n        # Create context from retrieved documents\n        context = "\\n\\n".join([doc.page_content for doc in docs])\n        \n        # Generate response\n        prompt = f"""\n        Based on the following context, answer the question:\n        \n        Context:\n        {context}\n        \n        Question: {question}\n        \n        Answer:"""\n        \n        response = self.llm(prompt)\n        return response, docs\n\n# Usage\nrag_agent = RAGAgent("./documents")\nanswer, sources = rag_agent.query("What is the main topic discussed?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Check GPU availability\nimport torch\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU count: {torch.cuda.device_count()}")\nif torch.cuda.is_available():\n    print(f"GPU name: {torch.cuda.get_device_name(0)}")\n\n# Optimize memory usage\ntorch.cuda.empty_cache()\n\n# Use mixed precision\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    # Your model inference here\n    pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type="nf4"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "model_name",\n    quantization_config=quantization_config,\n    device_map="auto"\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,i.jsx)(n.h3,{id:"system-monitoring",children:"System Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import psutil\nimport GPUtil\nimport logging\nfrom datetime import datetime\n\nclass SystemMonitor:\n    def __init__(self):\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('ai_agent.log'),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n    \n    def log_system_stats(self):\n        # CPU usage\n        cpu_percent = psutil.cpu_percent(interval=1)\n        \n        # Memory usage\n        memory = psutil.virtual_memory()\n        memory_percent = memory.percent\n        \n        # GPU usage\n        gpus = GPUtil.getGPUs()\n        gpu_stats = []\n        for gpu in gpus:\n            gpu_stats.append({\n                'id': gpu.id,\n                'name': gpu.name,\n                'load': gpu.load * 100,\n                'memory_used': gpu.memoryUsed,\n                'memory_total': gpu.memoryTotal,\n                'temperature': gpu.temperature\n            })\n        \n        self.logger.info(f\"CPU: {cpu_percent}%, Memory: {memory_percent}%\")\n        for gpu_stat in gpu_stats:\n            self.logger.info(f\"GPU {gpu_stat['id']}: {gpu_stat['load']:.1f}% load, \"\n                           f\"{gpu_stat['memory_used']}/{gpu_stat['memory_total']}MB memory\")\n\nmonitor = SystemMonitor()\nmonitor.log_system_stats()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"api-security",children:"API Security"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nimport jwt\nimport hashlib\nimport os\n\napp = FastAPI()\nsecurity = HTTPBearer()\n\nSECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    try:\n        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=["HS256"])\n        return payload\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail="Invalid authentication credentials"\n        )\n\n@app.post("/secure-chat")\nasync def secure_chat(request: ChatRequest, user=Depends(verify_token)):\n    # Your secure chat logic here\n    pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"input-sanitization",children:"Input Sanitization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import str\n\ndef sanitize_input(text: str) -> str:\n    # Remove potentially harmful characters\n    text = re.sub(r'[<>\"\\']', '', text)\n    \n    # Limit length\n    text = text[:1000]\n    \n    # Remove excessive whitespace\n    text = ' '.join(text.split())\n    \n    return text\n\ndef validate_input(text: str) -> bool:\n    # Check for common injection patterns\n    dangerous_patterns = [\n        r'<script',\n        r'javascript:',\n        r'eval\\(',\n        r'exec\\(',\n        r'import\\s+os',\n        r'__import__'\n    ]\n    \n    for pattern in dangerous_patterns:\n        if re.search(pattern, text, re.IGNORECASE):\n            return False\n    \n    return True\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment-scripts",children:"Deployment Scripts"}),"\n",(0,i.jsx)(n.h3,{id:"automated-setup-script",children:"Automated Setup Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n\n# setup_ai_agent.sh\n\nset -e\n\necho "Setting up Local AI Agent Environment..."\n\n# Update system\nsudo apt update && sudo apt upgrade -y\n\n# Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\nsudo usermod -aG docker $USER\n\n# Install Docker Compose\nsudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\n# Install NVIDIA Container Toolkit (if GPU present)\nif lspci | grep -i nvidia; then\n    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n    sudo apt-get update && sudo apt-get install -y nvidia-docker2\n    sudo systemctl restart docker\nfi\n\n# Install Python dependencies\npip3 install --upgrade pip\npip3 install -r requirements.txt\n\n# Download models\nmkdir -p models\ncd models\n\n# Download Llama 2 model (example)\nwget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf\n\necho "Setup complete! Run \'docker-compose up\' to start the AI agent."\n'})}),"\n",(0,i.jsx)(n.h3,{id:"systemd-service",children:"Systemd Service"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ini",children:"# /etc/systemd/system/ai-agent.service\n\n[Unit]\nDescription=Local AI Agent Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=aiagent\nWorkingDirectory=/opt/ai-agent\nExecStart=/usr/local/bin/docker-compose up\nExecStop=/usr/local/bin/docker-compose down\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Out of Memory Errors:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Reduce batch size\nbatch_size = 1\n\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Clear cache regularly\ntorch.cuda.empty_cache()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Slow Inference:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Use torch.no_grad() for inference\nwith torch.no_grad():\n    output = model(input_ids)\n\n# Optimize for inference\nmodel.eval()\ntorch.backends.cudnn.benchmark = True\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Model Loading Issues:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Check available disk space\nimport shutil\nfree_space = shutil.disk_usage(\'.\').free / (1024**3)  # GB\nprint(f"Free space: {free_space:.2f} GB")\n\n# Use model caching\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained("model_name", cache_dir="./model_cache")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Management"}),": Monitor CPU, GPU, and memory usage continuously"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Selection"}),": Choose models appropriate for your hardware capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caching"}),": Implement proper caching for models and embeddings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging"}),": Maintain comprehensive logs for debugging and monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security"}),": Implement proper authentication and input validation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Backup"}),": Regular backup of models and configuration files"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Updates"}),": Keep dependencies and models updated"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing"}),": Implement comprehensive testing for all components"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Local AI agent deployment offers significant advantages in terms of privacy, control, and cost-effectiveness. The methods outlined in this guide provide various approaches depending on your specific requirements, from simple chatbots using Ollama to complex multi-modal RAG systems."}),"\n",(0,i.jsx)(n.p,{children:"Choose the approach that best fits your hardware capabilities, technical requirements, and use case. Start with simpler methods like Ollama for proof-of-concept, then scale up to more complex deployments as needed."}),"\n",(0,i.jsx)(n.p,{children:"Remember to continuously monitor performance, implement proper security measures, and maintain your deployment for optimal results."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Last updated: September 2025"})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);