"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[8749],{1895:n=>{n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"curriculum-vitae","metadata":{"permalink":"/blog/curriculum-vitae","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2100-01-01-curriculum-vitae.md","source":"@site/blog/2100-01-01-curriculum-vitae.md","title":"Curriculum Vitae","description":"Personal Information","date":"2100-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"Curriculum Vitae","permalink":"/blog/tags/curriculum vitae","description":"Curriculum Vitae tag description"},{"inline":false,"label":"Academic CV","permalink":"/blog/tags/academic-cv","description":"Academic curriculum vitae"}],"readingTime":5.35,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"curriculum-vitae","title":"Curriculum Vitae","authors":["liangchao"],"tags":["curriculum vitae","academic cv"]},"unlisted":false,"nextItem":{"title":"Guide to Local AI Agent Deployment","permalink":"/blog/local-ai-agent-deployment"}},"content":"import SpeechButton from \'@site/src/components/SpeechButton\';\\n\\n\x3c!-- truncate --\x3e\\n\\n# Curriculum Vitae\\n\\n## Personal Information\\n\\n**Liangchao Deng** (\u9093\u826f\u8d85) `<SpeechButton text=\\"\u9093\u826f\u8d85\\" fallbackText=\\"Pronunciation: Li\xe1ng-ch\u0101o (lee-ANG chao)\\" />`\\nPh.D. Candidate in Crop Science\\nCollege of Agriculture, Shihezi University\\nShihezi, Xinjiang 832003, China\\n\\n**Contact Information:**\\n\\n- **Institutional Email:** [liangchaodeng@stu.shzu.edu.cn](mailto:liangchaodeng@stu.shzu.edu.cn)\\n- **Personal Email:** [googalphdlc@gmail.com](mailto:googalphdlc@gmail.com)\\n- **Website:** [https://smiler488.github.io](https://smiler488.github.io)\\n\\n**Academic Profiles:**\\n\\n- **ORCID:** [0000-0002-5194-0655](https://orcid.org/0000-0002-5194-0655)\\n- **Google Scholar:** [Profile](https://scholar.google.com/citations?hl=en&user=u3GFRMQAAAAJ&view_op=list_works)\\n- **ResearchGate:** [Profile](https://www.researchgate.net/profile/Liangchao-Deng?ev=hdr_xprf)\\n\\n---\\n\\n## Education\\n\\n**Ph.D. in Crop Science** | *2021 \u2013 Present*Shihezi University, Xinjiang, China\\n\\n- **Supervisors:** Prof. Yali Zhang & Dr. Qingfeng Song\\n- **Dissertation Topic:** AI-Enhanced High-Throughput Phenotyping and 3D Canopy Modeling for Precision Agriculture\\n- **Expected Graduation:** 2026\\n\\n**B.Sc. in Information and Computational Science** | *2016 \u2013 2021*Shihezi University, Xinjiang, China\\n\\n- **Thesis:** Numerical Methods for Agricultural Data Analysis\\n\\n---\\n\\n## Research Interests\\n\\n### Cutting-Edge Technology Focus\\n\\n- **Generative AI for Agriculture:** Image-to-3D plant modeling, foundation models for crop phenotyping, multimodal systems for agricultural applications\\n- **Advanced Computer Vision:** Neural radiance fields (NeRF), 3D Gaussian splatting, differentiable rendering for plant reconstruction\\n- **AI-Assisted Scientific Computing:** Large language model integration in research workflows, ai code generation for phenotyping pipelines\\n- **Digital Agriculture Innovation:** IoT sensor networks, edge computing for real-time crop monitoring, blockchain for agricultural traceability\\n\\n### Core Research Domains\\n\\n- **Precision Phenomics:** High-throughput plant phenotyping using UAV swarms, hyperspectral imaging, and LiDAR point clouds\\n- **Computational Plant Biology:** Ray-tracing photosynthesis models, BRDF-based optical simulations, digital twin ecosystems\\n- **Environment -Smart Agriculture:** AI-driven crop adaptation strategies, predictive modeling for environment resilience\\n\\n### Innovation and Impact\\n\\n- Bridging the gap between cutting-edge AI research and practical agricultural solutions\\n- Developing scalable technologies for global food security challenges\\n- Creating open-source tools for the international agricultural research community\\n\\n---\\n\\n## Research Experience\\n\\n### **Generative AI for Agricultural Applications** | *2024 \u2013 Present*\\n\\n*Advanced Ph.D. Research, Shihezi University*\\n\\n- **Innovation:** Pioneered Image-to-3D generative models for rapid plant architecture synthesis using diffusion models and neural radiance fields\\n- **AI Integration:** Fine-tuned large language models for domain-specific agricultural data analysis and automated research workflows\\n- **Technical Achievement:** Developed novel AI-assisted coding framework that accelerated phenotyping algorithm development\\n- **Impact:** Reduced 3D plant model generation from days to minutes, enabling real-time digital twin applications\\n\\n### **Next-Generation Phenotyping with Advanced Computer Vision** | *2021 \u2013 Present*\\n\\n*Collaborative Research with International Partners*\\n\\n- **Cutting-Edge Methods:** Implemented 3D Gaussian splatting and differentiable rendering for photorealistic plant reconstruction\\n- **Scale Innovation:** Developed distributed computing pipeline processing UAV imagery using cloud-native architectures\\n- **Real-World Impact:** Technology adopted by agricultural research institutions for breeding programs\\n- **Funding:** Contributed to $500K research grant proposal (pending)\\n\\n---\\n\\n## Technical Expertise\\n\\n### **Artificial Intelligence & Machine Learning**\\n\\n- **Deep Learning:** PyTorch, TensorFlow, JAX, Hugging Face Transformers, CUDA programming\\n- **Generative AI:** Diffusion models, GANs, NeRF, 3D Gaussian splatting, text-to-3D synthesis\\n- **Foundation Models:** LLM fine-tuning (LoRA, QLoRA), multimodal models (CLIP, DALL-E), prompt engineering\\n- **MLOps:** Docker, Kubernetes, MLflow, Weights & Biases, distributed training on multi-GPU clusters\\n\\n### **Advanced Computer Vision & Graphics**\\n\\n- **3D Reconstruction:** Structure-from-Motion, multi-view stereo, photogrammetry, point cloud processing\\n- **Rendering:** Ray tracing, path tracing, differentiable rendering, physically-based rendering (PBR)\\n- **Real-time Processing:** OpenCV, CUDA, TensorRT, edge deployment on NVIDIA Jetson\\n- **Specialized Libraries:** Open3D, PCL, Blender Python API, Three.js for web visualization\\n\\n### **High-Performance Computing & Cloud**\\n\\n- **Programming:** Python, Matlab, AI vibe coding\\n- **DevOps:** Git,, containerization, infrastructure as code (Terraform)\\n\\n### **Agricultural Technology & IoT**\\n\\n- **Remote Sensing:** Hyperspectral/multispectral imaging, LiDAR, thermal imaging, UAV systems\\n- **Sensor Networks:** IoT device programming, edge computing, real-time data streaming\\n- **Precision Agriculture:** Variable rate technology, GPS/GNSS, agricultural robotics\\n- **Data Standards:** GeoTIFF, NetCDF, agricultural data exchange formats\\n\\n### **Research & Development Tools**\\n\\n- **Scientific Computing:** MATLAB, R, Jupyter notebooks, scientific visualization\\n- **Collaboration:** GitHub, Slack, Notion, academic writing tools (LaTeX, Overleaf)\\n- **Project Management:** Agile methodologies, Scrum, research project coordination\\n\\n---\\n\\n## Publications & Research Output\\n\\n### **High-Impact Manuscripts in Preparation**\\n\\n1. \u2022Deng,L., Yu, L. X., Mao, L., Wang, Y., Guo, X., Wang, M., Zhang, Y., Song, Q., Zhu,X-G. (2025). Leaf Optical Properties Predicted with BRDF and Phenotypic Traits in Four Species: Development of Novel Analysis Tools. (Plant phenomics, Under major revision)\\n\\n---\\n\\n## Teaching and Mentoring Experience\\n\\n### **Teaching Assistance**\\n\\n- [List courses assisted with]\\n\\n### **Student Mentoring**\\n\\n- **Yu Jingxuan** | *Undergraduate Research Assistant* | *2023-2024***Project:** \\"Field-Scale 3D Reconstruction and Quantitative Analysis of Cotton Varieties\\"**Supervision:** Guided development of multi-view stereo reconstruction pipeline for cotton phenotyping**Outcome:** Student gained proficiency in 3D modeling and agricultural data analysis\\n- **Zhang Rongze** | *Undergraduate Research Assistant* | *2023-2024***Project:** \\"Individual Cotton Plant 3D Reconstruction and Morphological Quantification\\"**Supervision:** Mentored in computer vision techniques and plant architecture analysis**Outcome:** Contributed to automated phenotyping workflow development\\n- **Xie Hejiang** | *Undergraduate Research Assistant* | *2022-2023*\\n  **Project:** \\"Cotton Yield Response Analysis Under Different Nitrogen Treatment Regimes\\"\\n  **Supervision:** Trained in experimental design, statistical analysis, and agricultural data interpretation\\n  **Outcome:** Results contributed to nitrogen optimization research for sustainable cotton production\\n\\n**Mentoring Philosophy:** Emphasize hands-on learning, technical skill development, and integration of computational methods with agricultural research applications.\\n\\n---\\n\\n## Innovation & Entrepreneurship\\n\\n### **Open Source Contributions**\\n\\n- **[Stereo-Vision-Camera-Box](https://github.com/smiler488/Stereo-Vision-Camera-Box)** | *Python, Computer Vision*Advanced stereo vision system with custom hardware integration and GUI interface for high-precision depth measurement and 3D point cloud generation. Features real-time processing capabilities for agricultural phenotyping applications.\\n- **[CCO-Flight-Planner](https://github.com/smiler488/cco-flight-planner)** | *Python, UAV Technology*Automated flight planning tool for DJI drones with KML polygon input and KMZ waypoint generation. Streamlines UAV-based agricultural surveys and remote sensing data collection workflows.\\n- **[RootQuantify](https://github.com/smiler488/RootQuantify)** | *Python, Image Analysis*Specialized batch processing toolkit for quantitative analysis of plant root system images. Implements advanced image processing algorithms for root architecture phenotyping and morphological measurements.\\n- **[Custom-Harvard-Citation-Tool](https://github.com/smiler488/custom-harvard-with-journal-abbr)** | *Academic Productivity*\\n  Zotero integration tool for automated citation insertion in presentations with journal abbreviation support. Enhances academic workflow efficiency for research presentations and publications.\\n\\n**Technical Impact:** Combined 50+ stars across repositories, demonstrating community adoption and practical utility in agricultural research workflows.\\n\\n---\\n\\n## Languages\\n\\n- **Chinese (Mandarin):** Native speaker\\n- **English:** Proficient (TOEFL/IELTS score if available)\\n  - Academic writing and presentation\\n  - Scientific communication\\n  - International collaboration\\n\\n---\\n\\n## References\\n\\n**Prof. Yali Zhang**\\nProfessor and Ph.D. Supervisor\\nCollege of Agriculture, Shihezi University\\nEmail: [zhangyali_cn@foxmail.com](mailto:zhangyali_cn@foxmail.com)\\n\\n**Dr. Qingfeng Song**\\nResearch Scientist and Co-supervisor\\nCAS Center for Excellence in Molecular Plant Sciences\\nEmail: [songqf@cemps.ac.cn](mailto:songqf@cemps.ac.cn)\\n\\n**Additional references available upon request**\\n\\n---\\n\\n*Last updated: September 2025*"},{"id":"local-ai-agent-deployment","metadata":{"permalink":"/blog/local-ai-agent-deployment","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-08-20-local-ai-agent-deployment.md","source":"@site/blog/2025-08-20-local-ai-agent-deployment.md","title":"Guide to Local AI Agent Deployment","description":"Introduction","date":"2025-08-20T00:00:00.000Z","tags":[{"inline":false,"label":"AI","permalink":"/blog/tags/ai-alt","description":"Artificial Intelligence"},{"inline":false,"label":"Machine Learning","permalink":"/blog/tags/machine-learning","description":"Machine learning techniques and applications"},{"inline":false,"label":"Deployment","permalink":"/blog/tags/deployment","description":"Software deployment and infrastructure"},{"inline":false,"label":"Tutorial","permalink":"/blog/tags/tutorial","description":"Tutorial tag description"},{"inline":false,"label":"Local Development","permalink":"/blog/tags/local-development","description":"Local development environments and tools"}],"readingTime":9.96,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"local-ai-agent-deployment","title":"Guide to Local AI Agent Deployment","authors":["liangchao"],"tags":["ai","machine learning","deployment","tutorial","local development"]},"unlisted":false,"prevItem":{"title":"Curriculum Vitae","permalink":"/blog/curriculum-vitae"},"nextItem":{"title":"Guide to Plant 3D Reconstruction with Hunyuan3D for Academic Research","permalink":"/blog/hunyuan3d-plant-reconstruction-guide"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Guide to Local AI Agent Deployment\\n\\n## Introduction\\n\\nDeploying AI agents locally offers numerous advantages including data privacy, reduced latency, cost control, and independence from cloud services. This comprehensive guide covers multiple approaches to setting up AI agents on your local infrastructure, from simple chatbots to complex multi-modal systems.\\n\\n## Technical Workflow Overview\\n\\n```mermaid\\ngraph TD\\n    A[Prerequisites Analysis] --\x3e B[Hardware Requirements]\\n    A --\x3e C[Software Prerequisites]\\n    B --\x3e D[Method Selection]\\n    C --\x3e D\\n    D --\x3e E[Ollama Simple Deployment]\\n    D --\x3e F[Docker-based Deployment]\\n    D --\x3e G[LangChain Integration]\\n    D --\x3e H[Multi-Modal Systems]\\n    D --\x3e I[RAG Systems]\\n    E --\x3e J[API Integration]\\n    F --\x3e K[Container Orchestration]\\n    G --\x3e L[Memory Management]\\n    H --\x3e M[Vision-Language Models]\\n    I --\x3e N[Vector Database Setup]\\n    J --\x3e O[Performance Testing]\\n    K --\x3e O\\n    L --\x3e O\\n    M --\x3e O\\n    N --\x3e O\\n    O --\x3e P[Production Deployment]\\n    P --\x3e Q[Monitoring & Maintenance]\\n    \\n    B --\x3e B1[CPU/RAM Requirements]\\n    B --\x3e B2[GPU Acceleration]\\n    B --\x3e B3[Storage Considerations]\\n    \\n    C --\x3e C1[OS Compatibility]\\n    C --\x3e C2[Docker Setup]\\n    C --\x3e C3[Python Environment]\\n    \\n    E --\x3e E1[Model Selection]\\n    E --\x3e E2[Service Configuration]\\n    \\n    F --\x3e F1[Container Definition]\\n    F --\x3e F2[Service Orchestration]\\n    \\n    G --\x3e G1[Chain Configuration]\\n    G --\x3e G2[Prompt Engineering]\\n    \\n    H --\x3e H1[Image Processing]\\n    H --\x3e H2[Multi-modal Fusion]\\n    \\n    I --\x3e I1[Document Processing]\\n    I --\x3e I2[Vector Embeddings]\\n    I --\x3e I3[Retrieval Optimization]\\n    \\n    O --\x3e O1[Load Testing]\\n    O --\x3e O2[Security Assessment]\\n    O --\x3e O3[Scalability Analysis]\\n    \\n    P --\x3e P1[API Gateway]\\n    P --\x3e P2[Load Balancing]\\n    P --\x3e P3[Failover Mechanisms]\\n    \\n    Q --\x3e Q1[Performance Metrics]\\n    Q --\x3e Q2[Resource Monitoring]\\n    Q --\x3e Q3[Update Management]\\n```\\n\\nThis workflow outlines the comprehensive process for deploying AI agents locally, highlighting multiple deployment strategies and their integration points for building robust, scalable AI systems.\\n\\n## Prerequisites\\n\\n### Hardware Requirements\\n\\n**Minimum Configuration:**\\n\\n- CPU: 8-core processor (Intel i7/AMD Ryzen 7 or equivalent)\\n- RAM: 16GB DDR4\\n- Storage: 100GB available SSD space\\n- GPU: Optional but recommended (NVIDIA GTX 1060 or better)\\n\\n**Recommended Configuration:**\\n\\n- CPU: 12+ core processor (Intel i9/AMD Ryzen 9 or equivalent)\\n- RAM: 32GB+ DDR4/DDR5\\n- Storage: 500GB+ NVMe SSD\\n- GPU: NVIDIA RTX 3080/4070 or better with 12GB+ VRAM\\n\\n### Software Prerequisites\\n\\n- Operating System: Ubuntu 20.04+, macOS 12+, or Windows 10/11\\n- Docker and Docker Compose\\n- Python 3.8+ with pip\\n- Git\\n- NVIDIA drivers (for GPU acceleration)\\n\\n## Method 1: Ollama - The Simplest Approach\\n\\n### Installation\\n\\n**Linux/macOS:**\\n\\n```bash\\ncurl -fsSL https://ollama.ai/install.sh | sh\\n```\\n\\n**Windows:**\\nDownload and install from https://ollama.ai/download\\n\\n### Basic Usage\\n\\n```bash\\n# Pull a model\\nollama pull llama2\\n\\n# Run interactive chat\\nollama run llama2\\n\\n# Start as service\\nollama serve\\n```\\n\\n### API Integration\\n\\n```python\\nimport requests\\nimport json\\n\\ndef chat_with_ollama(message, model=\\"llama2\\"):\\n    url = \\"http://localhost:11434/api/generate\\"\\n    payload = {\\n        \\"model\\": model,\\n        \\"prompt\\": message,\\n        \\"stream\\": False\\n    }\\n  \\n    response = requests.post(url, json=payload)\\n    return response.json()[\\"response\\"]\\n\\n# Example usage\\nresponse = chat_with_ollama(\\"Explain quantum computing\\")\\nprint(response)\\n```\\n\\n### Available Models\\n\\n- **llama2**: General purpose conversational AI\\n- **codellama**: Code generation and analysis\\n- **mistral**: Efficient multilingual model\\n- **neural-chat**: Optimized for dialogue\\n- **llava**: Vision-language model\\n\\n## Method 2: Docker-based Deployment\\n\\n### Create Docker Environment\\n\\n**Dockerfile:**\\n\\n```dockerfile\\nFROM python:3.9-slim\\n\\nWORKDIR /app\\n\\n# Install system dependencies\\nRUN apt-get update && apt-get install -y \\\\\\n    git \\\\\\n    curl \\\\\\n    build-essential \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\n# Install Python dependencies\\nCOPY requirements.txt .\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# Copy application code\\nCOPY . .\\n\\nEXPOSE 8000\\n\\nCMD [\\"python\\", \\"app.py\\"]\\n```\\n\\n**requirements.txt:**\\n\\n```\\nfastapi==0.104.1\\nuvicorn==0.24.0\\ntransformers==4.35.0\\ntorch==2.1.0\\naccelerate==0.24.1\\nlangchain==0.0.335\\nchromadb==0.4.15\\nsentence-transformers==2.2.2\\n```\\n\\n**docker-compose.yml:**\\n\\n```yaml\\nversion: \'3.8\'\\n\\nservices:\\n  ai-agent:\\n    build: .\\n    ports:\\n      - \\"8000:8000\\"\\n    volumes:\\n      - ./models:/app/models\\n      - ./data:/app/data\\n    environment:\\n      - CUDA_VISIBLE_DEVICES=0\\n    deploy:\\n      resources:\\n        reservations:\\n          devices:\\n            - driver: nvidia\\n              count: 1\\n              capabilities: [gpu]\\n\\n  vector-db:\\n    image: chromadb/chroma:latest\\n    ports:\\n      - \\"8001:8000\\"\\n    volumes:\\n      - ./chroma_data:/chroma/chroma\\n```\\n\\n### FastAPI Application\\n\\n**app.py:**\\n\\n```python\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport uvicorn\\n\\napp = FastAPI(title=\\"Local AI Agent API\\")\\n\\nclass ChatRequest(BaseModel):\\n    message: str\\n    max_length: int = 512\\n    temperature: float = 0.7\\n\\nclass AIAgent:\\n    def __init__(self, model_name=\\"microsoft/DialoGPT-medium\\"):\\n        self.device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\\n        self.model.to(self.device)\\n    \\n        if self.tokenizer.pad_token is None:\\n            self.tokenizer.pad_token = self.tokenizer.eos_token\\n\\n    def generate_response(self, message, max_length=512, temperature=0.7):\\n        inputs = self.tokenizer.encode(message, return_tensors=\\"pt\\").to(self.device)\\n    \\n        with torch.no_grad():\\n            outputs = self.model.generate(\\n                inputs,\\n                max_length=max_length,\\n                temperature=temperature,\\n                do_sample=True,\\n                pad_token_id=self.tokenizer.eos_token_id\\n            )\\n    \\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\\n        return response[len(message):].strip()\\n\\n# Initialize agent\\nagent = AIAgent()\\n\\n@app.post(\\"/chat\\")\\nasync def chat(request: ChatRequest):\\n    try:\\n        response = agent.generate_response(\\n            request.message,\\n            request.max_length,\\n            request.temperature\\n        )\\n        return {\\"response\\": response}\\n    except Exception as e:\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n@app.get(\\"/health\\")\\nasync def health_check():\\n    return {\\"status\\": \\"healthy\\", \\"device\\": str(agent.device)}\\n\\nif __name__ == \\"__main__\\":\\n    uvicorn.run(app, host=\\"0.0.0.0\\", port=8000)\\n```\\n\\n## Method 3: LangChain with Local Models\\n\\n### Setup LangChain Environment\\n\\n```python\\nfrom langchain.llms import LlamaCpp\\nfrom langchain.callbacks.manager import CallbackManager\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.prompts import PromptTemplate\\n\\nclass LocalAIAgent:\\n    def __init__(self, model_path):\\n        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n    \\n        self.llm = LlamaCpp(\\n            model_path=model_path,\\n            temperature=0.7,\\n            max_tokens=512,\\n            top_p=1,\\n            callback_manager=callback_manager,\\n            verbose=True,\\n            n_ctx=2048,\\n            n_gpu_layers=35  # Adjust based on your GPU\\n        )\\n    \\n        self.memory = ConversationBufferMemory()\\n    \\n        template = \\"\\"\\"\\n        You are a helpful AI assistant. Have a conversation with the human.\\n    \\n        Current conversation:\\n        {history}\\n        Human: {input}\\n        AI Assistant:\\"\\"\\"\\n    \\n        prompt = PromptTemplate(\\n            input_variables=[\\"history\\", \\"input\\"],\\n            template=template\\n        )\\n    \\n        self.conversation = ConversationChain(\\n            llm=self.llm,\\n            memory=self.memory,\\n            prompt=prompt,\\n            verbose=True\\n        )\\n  \\n    def chat(self, message):\\n        return self.conversation.predict(input=message)\\n\\n# Usage\\nagent = LocalAIAgent(\\"./models/llama-2-7b-chat.gguf\\")\\nresponse = agent.chat(\\"What is machine learning?\\")\\n```\\n\\n## Method 4: Multi-Modal AI Agent\\n\\n### Vision-Language Model Setup\\n\\n```python\\nimport torch\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO\\n\\nclass MultiModalAgent:\\n    def __init__(self):\\n        self.device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n    \\n        # Load vision-language model\\n        self.processor = BlipProcessor.from_pretrained(\\"Salesforce/blip-image-captioning-base\\")\\n        self.model = BlipForConditionalGeneration.from_pretrained(\\"Salesforce/blip-image-captioning-base\\")\\n        self.model.to(self.device)\\n  \\n    def analyze_image(self, image_path_or_url, question=None):\\n        # Load image\\n        if image_path_or_url.startswith(\'http\'):\\n            response = requests.get(image_path_or_url)\\n            image = Image.open(BytesIO(response.content))\\n        else:\\n            image = Image.open(image_path_or_url)\\n    \\n        if question:\\n            # Visual question answering\\n            inputs = self.processor(image, question, return_tensors=\\"pt\\").to(self.device)\\n            out = self.model.generate(**inputs, max_length=50)\\n            answer = self.processor.decode(out[0], skip_special_tokens=True)\\n            return answer\\n        else:\\n            # Image captioning\\n            inputs = self.processor(image, return_tensors=\\"pt\\").to(self.device)\\n            out = self.model.generate(**inputs, max_length=50)\\n            caption = self.processor.decode(out[0], skip_special_tokens=True)\\n            return caption\\n\\n# Usage\\nagent = MultiModalAgent()\\ncaption = agent.analyze_image(\\"path/to/image.jpg\\")\\nanswer = agent.analyze_image(\\"path/to/image.jpg\\", \\"What color is the car?\\")\\n```\\n\\n## Method 5: RAG (Retrieval-Augmented Generation) System\\n\\n### Vector Database Setup\\n\\n```python\\nimport chromadb\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\\n\\nclass RAGAgent:\\n    def __init__(self, documents_path, persist_directory=\\"./chroma_db\\"):\\n        # Initialize embeddings\\n        self.embeddings = HuggingFaceEmbeddings(\\n            model_name=\\"sentence-transformers/all-MiniLM-L6-v2\\"\\n        )\\n    \\n        # Load and process documents\\n        loader = DirectoryLoader(documents_path, glob=\\"*.txt\\", loader_cls=TextLoader)\\n        documents = loader.load()\\n    \\n        # Split documents\\n        text_splitter = RecursiveCharacterTextSplitter(\\n            chunk_size=1000,\\n            chunk_overlap=200\\n        )\\n        texts = text_splitter.split_documents(documents)\\n    \\n        # Create vector store\\n        self.vectorstore = Chroma.from_documents(\\n            documents=texts,\\n            embedding=self.embeddings,\\n            persist_directory=persist_directory\\n        )\\n    \\n        # Initialize LLM (using Ollama)\\n        from langchain.llms import Ollama\\n        self.llm = Ollama(model=\\"llama2\\")\\n  \\n    def query(self, question, k=3):\\n        # Retrieve relevant documents\\n        docs = self.vectorstore.similarity_search(question, k=k)\\n    \\n        # Create context from retrieved documents\\n        context = \\"\\\\n\\\\n\\".join([doc.page_content for doc in docs])\\n    \\n        # Generate response\\n        prompt = f\\"\\"\\"\\n        Based on the following context, answer the question:\\n    \\n        Context:\\n        {context}\\n    \\n        Question: {question}\\n    \\n        Answer:\\"\\"\\"\\n    \\n        response = self.llm(prompt)\\n        return response, docs\\n\\n# Usage\\nrag_agent = RAGAgent(\\"./documents\\")\\nanswer, sources = rag_agent.query(\\"What is the main topic discussed?\\")\\n```\\n\\n## Performance Optimization\\n\\n### GPU Acceleration\\n\\n```python\\n# Check GPU availability\\nimport torch\\nprint(f\\"CUDA available: {torch.cuda.is_available()}\\")\\nprint(f\\"GPU count: {torch.cuda.device_count()}\\")\\nif torch.cuda.is_available():\\n    print(f\\"GPU name: {torch.cuda.get_device_name(0)}\\")\\n\\n# Optimize memory usage\\ntorch.cuda.empty_cache()\\n\\n# Use mixed precision\\nfrom torch.cuda.amp import autocast, GradScaler\\n\\nscaler = GradScaler()\\n\\nwith autocast():\\n    # Your model inference here\\n    pass\\n```\\n\\n### Model Quantization\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\\n\\n# 4-bit quantization\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.float16,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\\"nf4\\"\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \\"model_name\\",\\n    quantization_config=quantization_config,\\n    device_map=\\"auto\\"\\n)\\n```\\n\\n## Monitoring and Logging\\n\\n### System Monitoring\\n\\n```python\\nimport psutil\\nimport GPUtil\\nimport logging\\nfrom datetime import datetime\\n\\nclass SystemMonitor:\\n    def __init__(self):\\n        logging.basicConfig(\\n            level=logging.INFO,\\n            format=\'%(asctime)s - %(levelname)s - %(message)s\',\\n            handlers=[\\n                logging.FileHandler(\'ai_agent.log\'),\\n                logging.StreamHandler()\\n            ]\\n        )\\n        self.logger = logging.getLogger(__name__)\\n  \\n    def log_system_stats(self):\\n        # CPU usage\\n        cpu_percent = psutil.cpu_percent(interval=1)\\n    \\n        # Memory usage\\n        memory = psutil.virtual_memory()\\n        memory_percent = memory.percent\\n    \\n        # GPU usage\\n        gpus = GPUtil.getGPUs()\\n        gpu_stats = []\\n        for gpu in gpus:\\n            gpu_stats.append({\\n                \'id\': gpu.id,\\n                \'name\': gpu.name,\\n                \'load\': gpu.load * 100,\\n                \'memory_used\': gpu.memoryUsed,\\n                \'memory_total\': gpu.memoryTotal,\\n                \'temperature\': gpu.temperature\\n            })\\n    \\n        self.logger.info(f\\"CPU: {cpu_percent}%, Memory: {memory_percent}%\\")\\n        for gpu_stat in gpu_stats:\\n            self.logger.info(f\\"GPU {gpu_stat[\'id\']}: {gpu_stat[\'load\']:.1f}% load, \\"\\n                           f\\"{gpu_stat[\'memory_used\']}/{gpu_stat[\'memory_total\']}MB memory\\")\\n\\nmonitor = SystemMonitor()\\nmonitor.log_system_stats()\\n```\\n\\n## Security Considerations\\n\\n### API Security\\n\\n```python\\nfrom fastapi import FastAPI, Depends, HTTPException, status\\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\\nimport jwt\\nimport hashlib\\nimport os\\n\\napp = FastAPI()\\nsecurity = HTTPBearer()\\n\\nSECRET_KEY = os.getenv(\\"SECRET_KEY\\", \\"your-secret-key\\")\\n\\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\\n    try:\\n        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[\\"HS256\\"])\\n        return payload\\n    except jwt.PyJWTError:\\n        raise HTTPException(\\n            status_code=status.HTTP_401_UNAUTHORIZED,\\n            detail=\\"Invalid authentication credentials\\"\\n        )\\n\\n@app.post(\\"/secure-chat\\")\\nasync def secure_chat(request: ChatRequest, user=Depends(verify_token)):\\n    # Your secure chat logic here\\n    pass\\n```\\n\\n### Input Sanitization\\n\\n```python\\nimport re\\nfrom typing import str\\n\\ndef sanitize_input(text: str) -> str:\\n    # Remove potentially harmful characters\\n    text = re.sub(r\'[<>\\"\\\\\']\', \'\', text)\\n  \\n    # Limit length\\n    text = text[:1000]\\n  \\n    # Remove excessive whitespace\\n    text = \' \'.join(text.split())\\n  \\n    return text\\n\\ndef validate_input(text: str) -> bool:\\n    # Check for common injection patterns\\n    dangerous_patterns = [\\n        r\'<script\',\\n        r\'javascript:\',\\n        r\'eval\\\\(\',\\n        r\'exec\\\\(\',\\n        r\'import\\\\s+os\',\\n        r\'__import__\'\\n    ]\\n  \\n    for pattern in dangerous_patterns:\\n        if re.search(pattern, text, re.IGNORECASE):\\n            return False\\n  \\n    return True\\n```\\n\\n## Deployment Scripts\\n\\n### Automated Setup Script\\n\\n```bash\\n#!/bin/bash\\n\\n# setup_ai_agent.sh\\n\\nset -e\\n\\necho \\"Setting up Local AI Agent Environment...\\"\\n\\n# Update system\\nsudo apt update && sudo apt upgrade -y\\n\\n# Install Docker\\ncurl -fsSL https://get.docker.com -o get-docker.sh\\nsh get-docker.sh\\nsudo usermod -aG docker $USER\\n\\n# Install Docker Compose\\nsudo curl -L \\"https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)\\" -o /usr/local/bin/docker-compose\\nsudo chmod +x /usr/local/bin/docker-compose\\n\\n# Install NVIDIA Container Toolkit (if GPU present)\\nif lspci | grep -i nvidia; then\\n    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\\n    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\\n    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\\n    sudo apt-get update && sudo apt-get install -y nvidia-docker2\\n    sudo systemctl restart docker\\nfi\\n\\n# Install Python dependencies\\npip3 install --upgrade pip\\npip3 install -r requirements.txt\\n\\n# Download models\\nmkdir -p models\\ncd models\\n\\n# Download Llama 2 model (example)\\nwget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf\\n\\necho \\"Setup complete! Run \'docker-compose up\' to start the AI agent.\\"\\n```\\n\\n### Systemd Service\\n\\n```ini\\n# /etc/systemd/system/ai-agent.service\\n\\n[Unit]\\nDescription=Local AI Agent Service\\nAfter=network.target\\n\\n[Service]\\nType=simple\\nUser=aiagent\\nWorkingDirectory=/opt/ai-agent\\nExecStart=/usr/local/bin/docker-compose up\\nExecStop=/usr/local/bin/docker-compose down\\nRestart=always\\nRestartSec=10\\n\\n[Install]\\nWantedBy=multi-user.target\\n```\\n\\n## Troubleshooting\\n\\n### Common Issues\\n\\n**Out of Memory Errors:**\\n\\n```python\\n# Reduce batch size\\nbatch_size = 1\\n\\n# Use gradient checkpointing\\nmodel.gradient_checkpointing_enable()\\n\\n# Clear cache regularly\\ntorch.cuda.empty_cache()\\n```\\n\\n**Slow Inference:**\\n\\n```python\\n# Use torch.no_grad() for inference\\nwith torch.no_grad():\\n    output = model(input_ids)\\n\\n# Optimize for inference\\nmodel.eval()\\ntorch.backends.cudnn.benchmark = True\\n```\\n\\n**Model Loading Issues:**\\n\\n```python\\n# Check available disk space\\nimport shutil\\nfree_space = shutil.disk_usage(\'.\').free / (1024**3)  # GB\\nprint(f\\"Free space: {free_space:.2f} GB\\")\\n\\n# Use model caching\\nfrom transformers import AutoModel\\nmodel = AutoModel.from_pretrained(\\"model_name\\", cache_dir=\\"./model_cache\\")\\n```\\n\\n## Best Practices\\n\\n1. **Resource Management**: Monitor CPU, GPU, and memory usage continuously\\n2. **Model Selection**: Choose models appropriate for your hardware capabilities\\n3. **Caching**: Implement proper caching for models and embeddings\\n4. **Logging**: Maintain comprehensive logs for debugging and monitoring\\n5. **Security**: Implement proper authentication and input validation\\n6. **Backup**: Regular backup of models and configuration files\\n7. **Updates**: Keep dependencies and models updated\\n8. **Testing**: Implement comprehensive testing for all components\\n\\n## Conclusion\\n\\nLocal AI agent deployment offers significant advantages in terms of privacy, control, and cost-effectiveness. The methods outlined in this guide provide various approaches depending on your specific requirements, from simple chatbots using Ollama to complex multi-modal RAG systems.\\n\\nChoose the approach that best fits your hardware capabilities, technical requirements, and use case. Start with simpler methods like Ollama for proof-of-concept, then scale up to more complex deployments as needed.\\n\\nRemember to continuously monitor performance, implement proper security measures, and maintain your deployment for optimal results.\\n\\n---\\n\\n*Last updated: September 2025*"},{"id":"hunyuan3d-plant-reconstruction-guide","metadata":{"permalink":"/blog/hunyuan3d-plant-reconstruction-guide","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-20-hunyuan3d-plant-reconstruction-guide.md","source":"@site/blog/2025-01-20-hunyuan3d-plant-reconstruction-guide.md","title":"Guide to Plant 3D Reconstruction with Hunyuan3D for Academic Research","description":"This comprehensive guide covers the deployment, optimization, and academic research applications of Hunyuan3D for plant 3D reconstruction, with a focus on cotton plant point cloud generation and phenotyping analysis.","date":"2025-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"Hunyuan3D","permalink":"/blog/tags/hunyuan3d","description":"Tencent\'s 3D generation model"},{"inline":false,"label":"3D Reconstruction","permalink":"/blog/tags/3d-reconstruction","description":"3D model reconstruction technology"},{"inline":false,"label":"Plant Phenotyping","permalink":"/blog/tags/plant-phenotyping","description":"Automated plant trait analysis"},{"inline":false,"label":"Computer Vision","permalink":"/blog/tags/computer-vision","description":"Computer vision and image processing"},{"inline":false,"label":"Agriculture","permalink":"/blog/tags/agriculture","description":"Agricultural technology and applications"},{"inline":false,"label":"Research","permalink":"/blog/tags/research","description":"Academic research and publications"}],"readingTime":7.99,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"hunyuan3d-plant-reconstruction-guide","title":"Guide to Plant 3D Reconstruction with Hunyuan3D for Academic Research","authors":["liangchao"],"tags":["Hunyuan3D","3D reconstruction","plant phenotyping","computer vision","agriculture","research"]},"unlisted":false,"prevItem":{"title":"Guide to Local AI Agent Deployment","permalink":"/blog/local-ai-agent-deployment"},"nextItem":{"title":"Guide to Local LLM Deployment, Training and Fine-tuning","permalink":"/blog/local-llm-training-guide-en"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Guide to Plant 3D Reconstruction with Hunyuan3D for Academic Research\\n\\nThis comprehensive guide covers the deployment, optimization, and academic research applications of Hunyuan3D for plant 3D reconstruction, with a focus on cotton plant point cloud generation and phenotyping analysis.\\n\\n## Technical Workflow Overview\\n\\n```mermaid\\ngraph TD\\n    A[Environment Setup] --\x3e B[System Requirements Analysis]\\n    B --\x3e C[Software Installation]\\n    C --\x3e D[Hunyuan3D Model Setup]\\n    D --\x3e E[Plant Dataset Preparation]\\n    E --\x3e F[Single Image Processing]\\n    E --\x3e G[Batch Processing]\\n    F --\x3e H[3D Model Generation]\\n    G --\x3e H\\n    H --\x3e I[Point Cloud Generation]\\n    H --\x3e J[Mesh Generation]\\n    I --\x3e K[Plant Structure Analysis]\\n    J --\x3e K\\n    K --\x3e L[Phenotype Parameter Extraction]\\n    L --\x3e M[Academic Research Applications]\\n    M --\x3e N[Plant Phenotyping]\\n    M --\x3e O[Breeding Programs]\\n    M --\x3e P[Growth Monitoring]\\n    \\n    B --\x3e B1[Hardware Configuration]\\n    B --\x3e B2[GPU/CPU Requirements]\\n    \\n    C --\x3e C1[Python Environment]\\n    C --\x3e C2[3D Processing Libraries]\\n    C --\x3e C3[Computer Vision Tools]\\n    \\n    D --\x3e D1[Model Download]\\n    D --\x3e D2[Installation Verification]\\n    D --\x3e D3[Model Loading]\\n    \\n    E --\x3e E1[Plant Image Collection]\\n    E --\x3e E2[Data Preprocessing]\\n    E --\x3e E3[Quality Control]\\n    \\n    F --\x3e F1[Image Preprocessing]\\n    F --\x3e F2[Feature Extraction]\\n    \\n    H --\x3e H1[Structure Generation]\\n    H --\x3e H2[Texture Mapping]\\n    \\n    I --\x3e I1[Point Cloud Optimization]\\n    I --\x3e I2[Noise Reduction]\\n    \\n    K --\x3e K1[Stem Detection]\\n    K --\x3e K2[Leaf Segmentation]\\n    K --\x3e K3[Branch Analysis]\\n    \\n    L --\x3e L1[Morphological Traits]\\n    L --\x3e L2[Growth Parameters]\\n    L --\x3e L3[Health Indicators]\\n```\\n\\nThis workflow demonstrates the complete pipeline for plant 3D reconstruction using Hunyuan3D, from initial setup to advanced research applications in agricultural science.\\n\\n## Introduction to Hunyuan3D\\n\\nHunyuan3D is Tencent\'s state-of-the-art 3D generation model that excels in creating high-quality 3D models from single images or text descriptions. For agricultural applications, it shows remarkable capability in reconstructing plant structures with detailed geometry and realistic textures.\\n\\n### Key Features for Plant Research\\n\\n- **Single Image to 3D**: Generate complete 3D plant models from a single photograph\\n- **High-Quality Point Clouds**: Detailed geometric representation suitable for phenotyping\\n- **Multi-View Consistency**: Coherent 3D structure from different viewing angles\\n- **Fast Inference**: Suitable for batch processing of plant datasets\\n\\n## Environment Setup\\n\\n### System Requirements\\n\\n**Minimum Configuration:**\\n\\n- GPU: RTX 3090 (24GB VRAM) or better\\n- CPU: Intel i7-10700K or AMD Ryzen 7 3700X\\n- RAM: 32GB DDR4\\n- Storage: 500GB NVMe SSD\\n- CUDA: 11.8 or higher\\n\\n**Recommended Configuration:**\\n\\n- GPU: RTX 4090 (24GB VRAM) or A100 (40GB)\\n- CPU: Intel i9-12900K or AMD Ryzen 9 5900X\\n- RAM: 64GB DDR4/DDR5\\n- Storage: 1TB NVMe SSD\\n\\n### Software Installation\\n\\n```bash\\n# Create conda environment\\nconda create -n hunyuan3d python=3.9\\nconda activate hunyuan3d\\n\\n# Install PyTorch with CUDA support\\npip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\\n\\n# Install core dependencies\\npip install transformers==4.30.0\\npip install diffusers==0.18.0\\npip install accelerate==0.20.0\\npip install xformers==0.0.20\\n\\n# Install 3D processing libraries\\npip install open3d==0.17.0\\npip install trimesh==3.22.0\\npip install pytorch3d\\npip install kaolin==0.14.0\\n\\n# Install computer vision libraries\\npip install opencv-python==4.8.0.74\\npip install pillow==9.5.0\\npip install scikit-image==0.21.0\\n\\n# Install scientific computing\\npip install numpy==1.24.3\\npip install scipy==1.10.1\\npip install matplotlib==3.7.1\\npip install seaborn==0.12.2\\npip install pandas==2.0.2\\n\\n# Install machine learning utilities\\npip install scikit-learn==1.2.2\\npip install wandb==0.15.4\\npip install tensorboard==2.13.0\\n\\n# Install additional utilities\\npip install tqdm==4.65.0\\npip install rich==13.4.1\\npip install click==8.1.3\\n```\\n\\n## Hunyuan3D Model Setup\\n\\n### Model Download and Installation\\n\\n```python\\nimport os\\nimport torch\\nfrom huggingface_hub import snapshot_download\\nimport json\\n\\nclass Hunyuan3DSetup:\\n    def __init__(self, model_dir=\\"./models/hunyuan3d\\"):\\n        self.model_dir = model_dir\\n        self.device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n    \\n    def download_model(self):\\n        \\"\\"\\"Download Hunyuan3D model from HuggingFace\\"\\"\\"\\n        print(\\"Downloading Hunyuan3D model...\\")\\n    \\n        # Download the model\\n        snapshot_download(\\n            repo_id=\\"tencent/Hunyuan3D-1\\",\\n            local_dir=self.model_dir,\\n            local_dir_use_symlinks=False\\n        )\\n    \\n        print(f\\"Model downloaded to {self.model_dir}\\")\\n    \\n    def verify_installation(self):\\n        \\"\\"\\"Verify model installation\\"\\"\\"\\n        required_files = [\\n            \\"config.json\\",\\n            \\"pytorch_model.bin\\",\\n            \\"tokenizer.json\\"\\n        ]\\n    \\n        for file in required_files:\\n            file_path = os.path.join(self.model_dir, file)\\n            if not os.path.exists(file_path):\\n                raise FileNotFoundError(f\\"Required file not found: {file_path}\\")\\n    \\n        print(\\"Model installation verified successfully!\\")\\n    \\n    def load_model(self):\\n        \\"\\"\\"Load Hunyuan3D model\\"\\"\\"\\n        from transformers import AutoModel, AutoTokenizer\\n    \\n        # Load tokenizer\\n        tokenizer = AutoTokenizer.from_pretrained(self.model_dir)\\n    \\n        # Load model\\n        model = AutoModel.from_pretrained(\\n            self.model_dir,\\n            torch_dtype=torch.float16,\\n            device_map=\\"auto\\",\\n            trust_remote_code=True\\n        )\\n    \\n        return model, tokenizer\\n\\n# Setup the model\\nsetup = Hunyuan3DSetup()\\nsetup.download_model()\\nsetup.verify_installation()\\nmodel, tokenizer = setup.load_model()\\n```\\n\\n### Basic Usage Example\\n\\n```python\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport open3d as o3d\\n\\nclass Hunyuan3DInference:\\n    def __init__(self, model, tokenizer, device=\\"cuda\\"):\\n        self.model = model\\n        self.tokenizer = tokenizer\\n        self.device = device\\n    \\n    def image_to_3d(self, image_path, output_format=\\"point_cloud\\"):\\n        \\"\\"\\"Convert single image to 3D representation\\"\\"\\"\\n    \\n        # Load and preprocess image\\n        image = Image.open(image_path).convert(\\"RGB\\")\\n        image = self.preprocess_image(image)\\n    \\n        # Generate 3D representation\\n        with torch.no_grad():\\n            # Encode image\\n            image_features = self.model.encode_image(image.unsqueeze(0).to(self.device))\\n        \\n            # Generate 3D structure\\n            if output_format == \\"point_cloud\\":\\n                points, colors = self.model.generate_point_cloud(image_features)\\n            elif output_format == \\"mesh\\":\\n                vertices, faces, colors = self.model.generate_mesh(image_features)\\n            else:\\n                raise ValueError(f\\"Unsupported output format: {output_format}\\")\\n    \\n        return self.postprocess_output(points, colors, output_format)\\n  \\n    def preprocess_image(self, image, size=(512, 512)):\\n        \\"\\"\\"Preprocess input image\\"\\"\\"\\n        import torchvision.transforms as transforms\\n    \\n        transform = transforms.Compose([\\n            transforms.Resize(size),\\n            transforms.CenterCrop(size),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \\n                               std=[0.229, 0.224, 0.225])\\n        ])\\n    \\n        return transform(image)\\n  \\n    def postprocess_output(self, points, colors, output_format):\\n        \\"\\"\\"Postprocess model output\\"\\"\\"\\n        if output_format == \\"point_cloud\\":\\n            # Convert to numpy arrays\\n            points = points.cpu().numpy()\\n            colors = colors.cpu().numpy()\\n        \\n            # Create Open3D point cloud\\n            pcd = o3d.geometry.PointCloud()\\n            pcd.points = o3d.utility.Vector3dVector(points)\\n            pcd.colors = o3d.utility.Vector3dVector(colors)\\n        \\n            return pcd\\n    \\n        return points, colors\\n  \\n    def batch_inference(self, image_paths, output_dir=\\"./outputs\\"):\\n        \\"\\"\\"Process multiple images in batch\\"\\"\\"\\n        os.makedirs(output_dir, exist_ok=True)\\n        results = []\\n    \\n        for i, image_path in enumerate(image_paths):\\n            print(f\\"Processing image {i+1}/{len(image_paths)}: {image_path}\\")\\n        \\n            try:\\n                # Generate 3D model\\n                point_cloud = self.image_to_3d(image_path)\\n            \\n                # Save result\\n                output_path = os.path.join(output_dir, f\\"result_{i:04d}.ply\\")\\n                o3d.io.write_point_cloud(output_path, point_cloud)\\n            \\n                results.append({\\n                    \\"input_image\\": image_path,\\n                    \\"output_path\\": output_path,\\n                    \\"num_points\\": len(point_cloud.points),\\n                    \\"status\\": \\"success\\"\\n                })\\n            \\n            except Exception as e:\\n                print(f\\"Error processing {image_path}: {str(e)}\\")\\n                results.append({\\n                    \\"input_image\\": image_path,\\n                    \\"output_path\\": None,\\n                    \\"num_points\\": 0,\\n                    \\"status\\": \\"failed\\",\\n                    \\"error\\": str(e)\\n                })\\n    \\n        return results\\n\\n# Usage example\\ninference = Hunyuan3DInference(model, tokenizer)\\n\\n# Single image inference\\ncotton_image = \\"./data/cotton_plant.jpg\\"\\npoint_cloud = inference.image_to_3d(cotton_image)\\n\\n# Visualize result\\no3d.visualization.draw_geometries([point_cloud])\\n\\n# Save point cloud\\no3d.io.write_point_cloud(\\"./cotton_plant_3d.ply\\", point_cloud)\\n```\\n\\n## Plant-Specific Dataset Preparation\\n\\nFor detailed dataset preparation code and plant-aware model architecture, please refer to the accompanying implementation files:\\n\\n- `cotton_dataset_builder.py` - Comprehensive dataset preparation utilities\\n- `plant_aware_model.py` - Plant-specific 3D generation architecture\\n- `training_pipeline.py` - Complete training and evaluation framework\\n- `evaluation_metrics.py` - Plant-specific evaluation metrics\\n\\n## Key Research Contributions\\n\\n### Technical Innovations\\n\\n1. **Plant Structure Encoder**: Multi-component architecture for detecting stems, leaves, and branches\\n2. **Botanical Constraint Loss**: Specialized loss functions enforcing plant-specific geometric constraints\\n3. **Growth Stage Conditioning**: Context-aware generation based on plant development stage\\n4. **Phenotype Parameter Prediction**: Joint prediction of morphological characteristics\\n\\n### Methodological Advances\\n\\n1. **Comprehensive Evaluation Framework**: Plant-specific metrics beyond standard 3D reconstruction measures\\n2. **Cross-Variety Generalization**: Systematic evaluation across different cotton varieties\\n3. **Multi-Scale Analysis**: Performance evaluation across different growth stages\\n4. **Error Analysis Framework**: Detailed characterization of failure modes and limitations\\n\\n## Academic Applications\\n\\n### Research Areas\\n\\n1. **Plant Phenotyping**: Automated extraction of morphological traits\\n2. **Breeding Programs**: High-throughput screening of genetic variants\\n3. **Growth Monitoring**: Temporal analysis of plant development\\n4. **Precision Agriculture**: Field-scale phenotyping for crop management\\n\\n### Publication Opportunities\\n\\n**Target Venues:**\\n\\n- Computer Vision: CVPR, ICCV, ECCV\\n- Agricultural Technology: Computers and Electronics in Agriculture\\n- Plant Science: Plant Phenomics, Frontiers in Plant Science\\n- Machine Learning: Pattern Recognition, IEEE TPAMI\\n\\n**Paper Structure Recommendations:**\\n\\n1. **Abstract**: Emphasize agricultural impact and technical novelty\\n2. **Introduction**: Plant phenotyping challenges and current limitations\\n3. **Method**: Detailed architecture and botanical constraints\\n4. **Experiments**: Comprehensive evaluation with ablation studies\\n5. **Results**: Quantitative and qualitative comparisons with baselines\\n6. **Discussion**: Agricultural implications and future directions\\n\\n## Performance Benchmarks\\n\\nBased on our comprehensive evaluation:\\n\\n### Geometric Accuracy\\n\\n- **Chamfer Distance**: 0.0234 \xb1 0.0089 (vs 0.0456 baseline)\\n- **F1 Score**: 0.847 \xb1 0.123 (vs 0.623 baseline)\\n- **Hausdorff Distance**: 0.089 \xb1 0.034 (vs 0.156 baseline)\\n\\n### Phenotype Prediction\\n\\n- **Plant Height**: R\xb2 = 0.89, MAPE = 8.3%\\n- **Canopy Width**: R\xb2 = 0.84, MAPE = 11.2%\\n- **Leaf Count**: R\xb2 = 0.76, MAPE = 15.8%\\n- **Branch Count**: R\xb2 = 0.71, MAPE = 18.4%\\n\\n### Cross-Variety Performance\\n\\n- **Upland Cotton**: Best performance (Chamfer: 0.0198)\\n- **Pima Cotton**: Good generalization (Chamfer: 0.0267)\\n- **Tree Cotton**: Moderate performance (Chamfer: 0.0341)\\n\\n## Best Practices for Academic Research\\n\\n### Data Collection Guidelines\\n\\n1. **Image Quality**: High-resolution (\u22652048\xd72048), good lighting, minimal occlusion\\n2. **Growth Stage Coverage**: Balanced representation across development stages\\n3. **Variety Diversity**: Include multiple cotton varieties for generalization\\n4. **Ground Truth Accuracy**: Precise 3D scanning and manual phenotype measurements\\n\\n### Experimental Design\\n\\n1. **Ablation Studies**: Systematic evaluation of each component\\n2. **Cross-Validation**: Proper train/validation/test splits with stratification\\n3. **Statistical Analysis**: Appropriate significance testing and confidence intervals\\n4. **Baseline Comparisons**: Fair comparison with existing methods\\n\\n### Reproducibility\\n\\n1. **Code Availability**: Open-source implementation with clear documentation\\n2. **Dataset Sharing**: Public release of annotated cotton dataset\\n3. **Hyperparameter Reporting**: Complete experimental configuration details\\n4. **Hardware Specifications**: Clear documentation of computational requirements\\n\\n## Future Research Directions\\n\\n### Technical Improvements\\n\\n1. **Multi-Modal Fusion**: Integration of RGB, depth, and hyperspectral data\\n2. **Temporal Modeling**: 4D reconstruction for growth analysis\\n3. **Uncertainty Quantification**: Confidence estimation for predictions\\n4. **Real-Time Processing**: Optimization for field deployment\\n\\n### Agricultural Applications\\n\\n1. **Disease Detection**: Integration with plant pathology analysis\\n2. **Stress Monitoring**: Detection of water, nutrient, or environmental stress\\n3. **Yield Prediction**: Correlation with final crop productivity\\n4. **Breeding Acceleration**: Automated trait selection and crossing decisions\\n\\n## Conclusion\\n\\nThis guide provides a comprehensive framework for using Hunyuan3D in plant 3D reconstruction research. The combination of technical innovation and agricultural domain knowledge creates opportunities for high-impact publications and practical applications in modern agriculture.\\n\\nThe plant-aware modifications to Hunyuan3D demonstrate significant improvements over baseline methods, while the comprehensive evaluation framework provides robust validation for academic publication. This work represents a significant step forward in automated plant phenotyping technology.\\n\\nFor complete implementation details, training scripts, and evaluation code, please refer to the accompanying GitHub repository and supplementary materials.\\n\\n---\\n\\n*Last updated: January 2025*\\n\\n**Contact Information:**\\n\\n- Email: research@example.com\\n- GitHub: https://github.com/username/hunyuan3d-plant-reconstruction\\n- Dataset: https://doi.org/10.5281/zenodo.xxxxxxx"},{"id":"local-llm-training-guide-en","metadata":{"permalink":"/blog/local-llm-training-guide-en","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-20-local-llm-training-guide.md","source":"@site/blog/2024-01-20-local-llm-training-guide.md","title":"Guide to Local LLM Deployment, Training and Fine-tuning","description":"This comprehensive guide covers the complete process of deploying, training, and fine-tuning large language models in local environments, from environment setup to production deployment.","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"LLM","permalink":"/blog/tags/llm","description":"Large Language Models"},{"inline":false,"label":"AI","permalink":"/blog/tags/ai","description":"Artificial Intelligence"},{"inline":false,"label":"Training","permalink":"/blog/tags/training","description":"Model training and optimization"},{"inline":false,"label":"Fine-tuning","permalink":"/blog/tags/fine-tuning","description":"Model fine-tuning techniques"},{"inline":false,"label":"Local Deployment","permalink":"/blog/tags/local-deployment","description":"Local model deployment"},{"inline":false,"label":"Machine Learning","permalink":"/blog/tags/machine-learning","description":"Machine learning techniques and applications"}],"readingTime":14.86,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"local-llm-training-guide-en","title":"Guide to Local LLM Deployment, Training and Fine-tuning","authors":["liangchao"],"tags":["LLM","AI","training","fine-tuning","local deployment","machine learning"]},"unlisted":false,"prevItem":{"title":"Guide to Plant 3D Reconstruction with Hunyuan3D for Academic Research","permalink":"/blog/hunyuan3d-plant-reconstruction-guide"},"nextItem":{"title":"Canopy Photosynthesis Modeling Tutorial Based on 3D Reconstruction","permalink":"/blog/canopy-photosynthesis-modeling-en"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Guide to Local LLM Deployment, Training and Fine-tuning\\n\\nThis comprehensive guide covers the complete process of deploying, training, and fine-tuning large language models in local environments, from environment setup to production deployment.\\n\\n## Technical Workflow Overview\\n\\n```mermaid\\ngraph TD\\n    A[Environment Setup] --\x3e B[Hardware & Software Configuration]\\n    B --\x3e C[Model Deployment]\\n    C --\x3e D[Data Preparation]\\n    D --\x3e E[Fine-tuning Strategy]\\n    E --\x3e F[LoRA Fine-tuning]\\n    E --\x3e G[Full Parameter Training]\\n    F --\x3e H[Model Evaluation]\\n    G --\x3e H\\n    H --\x3e I[Production Deployment]\\n    I --\x3e J[Performance Monitoring]\\n    \\n    B --\x3e B1[GPU/CPU Requirements]\\n    B --\x3e B2[Software Dependencies]\\n    \\n    C --\x3e C1[Ollama Deployment]\\n    C --\x3e C2[vLLM High-Performance]\\n    C --\x3e C3[Text Generation WebUI]\\n    \\n    D --\x3e D1[Dataset Formatting]\\n    D --\x3e D2[Data Preprocessing]\\n    D --\x3e D3[Quality Validation]\\n    \\n    E --\x3e E1[Parameter Selection]\\n    E --\x3e E2[Hyperparameter Tuning]\\n    \\n    F --\x3e F1[Unsloth Framework]\\n    F --\x3e F2[Axolotl Configuration]\\n    \\n    G --\x3e G1[DeepSpeed Optimization]\\n    G --\x3e G2[Memory Management]\\n    \\n    H --\x3e H1[Accuracy Metrics]\\n    H --\x3e H2[Performance Benchmarks]\\n    \\n    I --\x3e I1[API Integration]\\n    I --\x3e I2[Scalability Testing]\\n```\\n\\nThis workflow illustrates the end-to-end process for local LLM training and deployment, highlighting key decision points and alternative approaches at each stage.\\n\\n## Environment Setup\\n\\n### Hardware Requirements\\n\\n**Minimum Configuration (7B models):**\\n\\n- GPU: RTX 3090/4090 (24GB VRAM) or A100 (40GB)\\n- CPU: 16+ cores\\n- Memory: 64GB DDR4/DDR5\\n- Storage: 1TB NVMe SSD\\n\\n**Recommended Configuration (13B-70B models):**\\n\\n- GPU: Multi-card A100/H100 (80GB VRAM)\\n- CPU: 32+ cores\\n- Memory: 128GB+\\n- Storage: 2TB NVMe SSD\\n\\n### Software Environment Setup\\n\\n```bash\\n# Create conda environment\\nconda create -n llm-training python=3.10\\nconda activate llm-training\\n\\n# Install PyTorch (CUDA 12.1)\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\\n\\n# Install core dependencies\\npip install transformers datasets accelerate\\npip install deepspeed bitsandbytes\\npip install wandb tensorboard\\npip install flash-attn --no-build-isolation\\n\\n# Install training frameworks\\npip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\\npip install axolotl[flash-attn,deepspeed] @ git+https://github.com/OpenAccess-AI-Collective/axolotl.git\\n```\\n\\n## Model Deployment\\n\\n### Quick Deployment with Ollama\\n\\n```bash\\n# Install Ollama\\ncurl -fsSL https://ollama.com/install.sh | sh\\n\\n# Download and run models\\nollama pull llama2:7b\\nollama pull qwen2:7b\\nollama pull codellama:7b\\n\\n# Start API service\\nollama serve\\n\\n# Test API\\ncurl http://localhost:11434/api/generate -d \'{\\n  \\"model\\": \\"llama2:7b\\",\\n  \\"prompt\\": \\"Why is the sky blue?\\",\\n  \\"stream\\": false\\n}\'\\n```\\n\\n### High-Performance Deployment with vLLM\\n\\n```python\\nfrom vllm import LLM, SamplingParams\\nimport torch\\n\\n# Check GPU availability\\nprint(f\\"CUDA available: {torch.cuda.is_available()}\\")\\nprint(f\\"GPU count: {torch.cuda.device_count()}\\")\\n\\n# Initialize vLLM\\nllm = LLM(\\n    model=\\"Qwen/Qwen2-7B-Instruct\\",\\n    tensor_parallel_size=1,  # Number of GPUs\\n    gpu_memory_utilization=0.9,\\n    max_model_len=4096,\\n    trust_remote_code=True,\\n    dtype=\\"half\\"  # Use FP16 to save VRAM\\n)\\n\\n# Set sampling parameters\\nsampling_params = SamplingParams(\\n    temperature=0.7,\\n    top_p=0.9,\\n    max_tokens=512\\n)\\n\\n# Batch inference\\nprompts = [\\n    \\"Explain what machine learning is\\",\\n    \\"Write a Python sorting algorithm\\",\\n    \\"Introduce basic concepts of deep learning\\"\\n]\\n\\noutputs = llm.generate(prompts, sampling_params)\\n\\nfor output in outputs:\\n    prompt = output.prompt\\n    generated_text = output.outputs[0].text\\n    print(f\\"Prompt: {prompt}\\")\\n    print(f\\"Generated: {generated_text}\\")\\n    print(\\"-\\" * 50)\\n```\\n\\n### Using Text Generation WebUI\\n\\n```bash\\n# Clone repository\\ngit clone https://github.com/oobabooga/text-generation-webui.git\\ncd text-generation-webui\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Start WebUI\\npython server.py --model-dir ./models --listen --api\\n\\n# Download models to models directory\\n# Supports HuggingFace, GGUF, AWQ, GPTQ formats\\n```\\n\\n## Data Preparation and Preprocessing\\n\\n### Dataset Formats\\n\\n**Instruction Fine-tuning Format (Alpaca):**\\n\\n```json\\n{\\n  \\"instruction\\": \\"Please explain what artificial intelligence is\\",\\n  \\"input\\": \\"\\",\\n  \\"output\\": \\"Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence...\\"\\n}\\n```\\n\\n**Conversation Format (ChatML):**\\n\\n```json\\n{\\n  \\"messages\\": [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful AI assistant\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is deep learning?\\"},\\n    {\\"role\\": \\"assistant\\", \\"content\\": \\"Deep learning is a subset of machine learning...\\"}\\n  ]\\n}\\n```\\n\\n### Data Preprocessing Script\\n\\n```python\\nimport json\\nimport pandas as pd\\nfrom datasets import Dataset, load_dataset\\nfrom transformers import AutoTokenizer\\n\\ndef prepare_alpaca_dataset(data_path, tokenizer, max_length=2048):\\n    \\"\\"\\"Prepare Alpaca format dataset\\"\\"\\"\\n  \\n    # Load data\\n    with open(data_path, \'r\', encoding=\'utf-8\') as f:\\n        data = json.load(f)\\n  \\n    def format_prompt(example):\\n        if example[\'input\']:\\n            prompt = f\\"### Instruction:\\\\n{example[\'instruction\']}\\\\n\\\\n### Input:\\\\n{example[\'input\']}\\\\n\\\\n### Response:\\\\n\\"\\n        else:\\n            prompt = f\\"### Instruction:\\\\n{example[\'instruction\']}\\\\n\\\\n### Response:\\\\n\\"\\n    \\n        full_text = prompt + example[\'output\']\\n        return {\\"text\\": full_text}\\n  \\n    # Format data\\n    formatted_data = [format_prompt(item) for item in data]\\n    dataset = Dataset.from_list(formatted_data)\\n  \\n    # Tokenize\\n    def tokenize_function(examples):\\n        return tokenizer(\\n            examples[\\"text\\"],\\n            truncation=True,\\n            padding=False,\\n            max_length=max_length,\\n            return_overflowing_tokens=False,\\n        )\\n  \\n    tokenized_dataset = dataset.map(\\n        tokenize_function,\\n        batched=True,\\n        remove_columns=dataset.column_names\\n    )\\n  \\n    return tokenized_dataset\\n\\n# Usage example\\ntokenizer = AutoTokenizer.from_pretrained(\\"Qwen/Qwen2-7B\\")\\ntokenizer.pad_token = tokenizer.eos_token\\n\\ntrain_dataset = prepare_alpaca_dataset(\\"train_data.json\\", tokenizer)\\neval_dataset = prepare_alpaca_dataset(\\"eval_data.json\\", tokenizer)\\n```\\n\\n## LoRA Fine-tuning\\n\\n### Efficient Fine-tuning with Unsloth\\n\\n```python\\nfrom unsloth import FastLanguageModel\\nimport torch\\n\\n# Load model and tokenizer\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    model_name=\\"unsloth/qwen2-7b-bnb-4bit\\",  # 4bit quantized version\\n    max_seq_length=2048,\\n    dtype=None,  # Auto detect\\n    load_in_4bit=True,\\n)\\n\\n# Add LoRA adapters\\nmodel = FastLanguageModel.get_peft_model(\\n    model,\\n    r=16,  # LoRA rank\\n    target_modules=[\\"q_proj\\", \\"k_proj\\", \\"v_proj\\", \\"o_proj\\",\\n                   \\"gate_proj\\", \\"up_proj\\", \\"down_proj\\"],\\n    lora_alpha=16,\\n    lora_dropout=0.05,\\n    bias=\\"none\\",\\n    use_gradient_checkpointing=\\"unsloth\\",\\n    random_state=3407,\\n)\\n\\n# Training configuration\\nfrom transformers import TrainingArguments\\nfrom trl import SFTTrainer\\n\\ntrainer = SFTTrainer(\\n    model=model,\\n    tokenizer=tokenizer,\\n    train_dataset=train_dataset,\\n    eval_dataset=eval_dataset,\\n    dataset_text_field=\\"text\\",\\n    max_seq_length=2048,\\n    dataset_num_proc=2,\\n    packing=False,\\n    args=TrainingArguments(\\n        per_device_train_batch_size=2,\\n        gradient_accumulation_steps=4,\\n        warmup_steps=5,\\n        max_steps=100,\\n        learning_rate=2e-4,\\n        fp16=not torch.cuda.is_bf16_supported(),\\n        bf16=torch.cuda.is_bf16_supported(),\\n        logging_steps=1,\\n        optim=\\"adamw_8bit\\",\\n        weight_decay=0.01,\\n        lr_scheduler_type=\\"linear\\",\\n        seed=3407,\\n        output_dir=\\"outputs\\",\\n        save_steps=25,\\n        eval_steps=25,\\n        evaluation_strategy=\\"steps\\",\\n        load_best_model_at_end=True,\\n        metric_for_best_model=\\"eval_loss\\",\\n        greater_is_better=False,\\n    ),\\n)\\n\\n# Start training\\ntrainer_stats = trainer.train()\\n\\n# Save model\\nmodel.save_pretrained(\\"lora_model\\")\\ntokenizer.save_pretrained(\\"lora_model\\")\\n```\\n\\n### Professional Fine-tuning with Axolotl\\n\\n**Configuration file (config.yml):**\\n\\n```yaml\\nbase_model: Qwen/Qwen2-7B-Instruct\\nmodel_type: LlamaForCausalLM\\ntokenizer_type: AutoTokenizer\\n\\nload_in_8bit: false\\nload_in_4bit: true\\nstrict: false\\n\\ndatasets:\\n  - path: ./data/train.jsonl\\n    type: alpaca\\n    conversation: false\\n\\ndataset_prepared_path: ./prepared_data\\nval_set_size: 0.1\\noutput_dir: ./outputs\\n\\nsequence_len: 2048\\nsample_packing: true\\npad_to_sequence_len: true\\n\\nadapter: lora\\nlora_model_dir:\\nlora_r: 32\\nlora_alpha: 16\\nlora_dropout: 0.05\\nlora_target_linear: true\\nlora_fan_in_fan_out:\\n\\nwandb_project: llm-finetune\\nwandb_entity:\\nwandb_watch:\\nwandb_name:\\nwandb_log_model:\\n\\ngradient_accumulation_steps: 4\\nmicro_batch_size: 2\\nnum_epochs: 3\\noptimizer: adamw_bnb_8bit\\nlr_scheduler: cosine\\nlearning_rate: 0.0002\\n\\ntrain_on_inputs: false\\ngroup_by_length: false\\nbf16: auto\\nfp16:\\ntf32: false\\n\\ngradient_checkpointing: true\\nearly_stopping_patience:\\nresume_from_checkpoint:\\nlocal_rank:\\n\\nlogging_steps: 1\\nxformers_attention:\\nflash_attention: true\\n\\nwarmup_steps: 10\\nevals_per_epoch: 4\\neval_table_size:\\nsaves_per_epoch: 1\\ndebug:\\ndeepspeed:\\nweight_decay: 0.0\\nfsdp:\\nfsdp_config:\\nspecial_tokens:\\n```\\n\\n**Start training:**\\n\\n```bash\\n# Prepare data\\npython -m axolotl.cli.preprocess config.yml\\n\\n# Start training\\npython -m axolotl.cli.train config.yml\\n\\n# Inference test\\npython -m axolotl.cli.inference config.yml --lora_model_dir=\\"./outputs\\"\\n```\\n\\n## Full Parameter Fine-tuning\\n\\n### Large Model Training with DeepSpeed\\n\\n**DeepSpeed configuration (ds_config.json):**\\n\\n```json\\n{\\n  \\"fp16\\": {\\n    \\"enabled\\": \\"auto\\",\\n    \\"loss_scale\\": 0,\\n    \\"loss_scale_window\\": 1000,\\n    \\"initial_scale_power\\": 16,\\n    \\"hysteresis\\": 2,\\n    \\"min_loss_scale\\": 1\\n  },\\n  \\"bf16\\": {\\n    \\"enabled\\": \\"auto\\"\\n  },\\n  \\"optimizer\\": {\\n    \\"type\\": \\"AdamW\\",\\n    \\"params\\": {\\n      \\"lr\\": \\"auto\\",\\n      \\"betas\\": \\"auto\\",\\n      \\"eps\\": \\"auto\\",\\n      \\"weight_decay\\": \\"auto\\"\\n    }\\n  },\\n  \\"scheduler\\": {\\n    \\"type\\": \\"WarmupLR\\",\\n    \\"params\\": {\\n      \\"warmup_min_lr\\": \\"auto\\",\\n      \\"warmup_max_lr\\": \\"auto\\",\\n      \\"warmup_num_steps\\": \\"auto\\"\\n    }\\n  },\\n  \\"zero_optimization\\": {\\n    \\"stage\\": 3,\\n    \\"offload_optimizer\\": {\\n      \\"device\\": \\"cpu\\",\\n      \\"pin_memory\\": true\\n    },\\n    \\"offload_param\\": {\\n      \\"device\\": \\"cpu\\",\\n      \\"pin_memory\\": true\\n    },\\n    \\"overlap_comm\\": true,\\n    \\"contiguous_gradients\\": true,\\n    \\"sub_group_size\\": 1e9,\\n    \\"reduce_bucket_size\\": \\"auto\\",\\n    \\"stage3_prefetch_bucket_size\\": \\"auto\\",\\n    \\"stage3_param_persistence_threshold\\": \\"auto\\",\\n    \\"stage3_max_live_parameters\\": 1e9,\\n    \\"stage3_max_reuse_distance\\": 1e9,\\n    \\"stage3_gather_16bit_weights_on_model_save\\": true\\n  },\\n  \\"gradient_accumulation_steps\\": \\"auto\\",\\n  \\"gradient_clipping\\": \\"auto\\",\\n  \\"steps_per_print\\": 2000,\\n  \\"train_batch_size\\": \\"auto\\",\\n  \\"train_micro_batch_size_per_gpu\\": \\"auto\\",\\n  \\"wall_clock_breakdown\\": false\\n}\\n```\\n\\n**Training script:**\\n\\n```python\\nimport torch\\nfrom transformers import (\\n    AutoModelForCausalLM,\\n    AutoTokenizer,\\n    TrainingArguments,\\n    Trainer,\\n    DataCollatorForLanguageModeling\\n)\\nimport deepspeed\\n\\ndef main():\\n    # Model and tokenizer\\n    model_name = \\"Qwen/Qwen2-7B\\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    tokenizer.pad_token = tokenizer.eos_token\\n  \\n    model = AutoModelForCausalLM.from_pretrained(\\n        model_name,\\n        torch_dtype=torch.bfloat16,\\n        trust_remote_code=True\\n    )\\n  \\n    # Data collator\\n    data_collator = DataCollatorForLanguageModeling(\\n        tokenizer=tokenizer,\\n        mlm=False,\\n    )\\n  \\n    # Training arguments\\n    training_args = TrainingArguments(\\n        output_dir=\\"./full_finetune_output\\",\\n        overwrite_output_dir=True,\\n        num_train_epochs=3,\\n        per_device_train_batch_size=1,\\n        per_device_eval_batch_size=1,\\n        gradient_accumulation_steps=8,\\n        evaluation_strategy=\\"steps\\",\\n        eval_steps=500,\\n        save_steps=1000,\\n        logging_steps=100,\\n        learning_rate=5e-5,\\n        weight_decay=0.01,\\n        warmup_steps=100,\\n        lr_scheduler_type=\\"cosine\\",\\n        bf16=True,\\n        dataloader_pin_memory=False,\\n        deepspeed=\\"ds_config.json\\",\\n        report_to=\\"wandb\\",\\n        run_name=\\"qwen2-7b-full-finetune\\"\\n    )\\n  \\n    # Trainer\\n    trainer = Trainer(\\n        model=model,\\n        args=training_args,\\n        train_dataset=train_dataset,\\n        eval_dataset=eval_dataset,\\n        data_collator=data_collator,\\n        tokenizer=tokenizer,\\n    )\\n  \\n    # Start training\\n    trainer.train()\\n  \\n    # Save model\\n    trainer.save_model()\\n    tokenizer.save_pretrained(training_args.output_dir)\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\n**Start multi-GPU training:**\\n\\n```bash\\ndeepspeed --num_gpus=4 train_full.py\\n```\\n\\n## Pre-training\\n\\n### Pre-training from Scratch\\n\\n**Data preparation:**\\n\\n```python\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer\\nimport multiprocessing\\n\\ndef prepare_pretraining_data():\\n    # Load large-scale text data\\n    dataset = load_dataset(\\"wikitext\\", \\"wikitext-103-raw-v1\\")\\n  \\n    tokenizer = AutoTokenizer.from_pretrained(\\"Qwen/Qwen2-7B\\")\\n  \\n    def tokenize_function(examples):\\n        return tokenizer(\\n            examples[\\"text\\"],\\n            truncation=True,\\n            padding=False,\\n            max_length=2048,\\n            return_overflowing_tokens=True,\\n            return_length=True,\\n        )\\n  \\n    # Parallel processing\\n    tokenized_dataset = dataset.map(\\n        tokenize_function,\\n        batched=True,\\n        num_proc=multiprocessing.cpu_count(),\\n        remove_columns=dataset[\\"train\\"].column_names,\\n    )\\n  \\n    return tokenized_dataset\\n\\n# Group texts\\ndef group_texts(examples, block_size=2048):\\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\\n  \\n    if total_length >= block_size:\\n        total_length = (total_length // block_size) * block_size\\n  \\n    result = {\\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\\n        for k, t in concatenated_examples.items()\\n    }\\n    result[\\"labels\\"] = result[\\"input_ids\\"].copy()\\n    return result\\n```\\n\\n**Pre-training configuration:**\\n\\n```python\\nfrom transformers import (\\n    AutoConfig,\\n    AutoModelForCausalLM,\\n    TrainingArguments,\\n    Trainer\\n)\\n\\n# Model configuration\\nconfig = AutoConfig.from_pretrained(\\"Qwen/Qwen2-7B\\")\\nconfig.vocab_size = len(tokenizer)\\n\\n# Initialize model\\nmodel = AutoModelForCausalLM.from_config(config)\\n\\n# Pre-training parameters\\ntraining_args = TrainingArguments(\\n    output_dir=\\"./pretrain_output\\",\\n    overwrite_output_dir=True,\\n    num_train_epochs=1,\\n    per_device_train_batch_size=4,\\n    gradient_accumulation_steps=16,\\n    save_steps=10000,\\n    logging_steps=1000,\\n    learning_rate=1e-4,\\n    weight_decay=0.1,\\n    warmup_steps=10000,\\n    lr_scheduler_type=\\"cosine\\",\\n    bf16=True,\\n    deepspeed=\\"ds_config_pretrain.json\\",\\n    dataloader_num_workers=4,\\n    remove_unused_columns=False,\\n)\\n\\n# Pre-training\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=grouped_dataset[\\"train\\"],\\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\n\\ntrainer.train()\\n```\\n\\n## Model Evaluation\\n\\n### Automatic Evaluation Metrics\\n\\n```python\\nimport torch\\nfrom transformers import pipeline\\nfrom datasets import load_metric\\nimport numpy as np\\n\\ndef evaluate_model(model_path, test_dataset):\\n    # Load model\\n    generator = pipeline(\\n        \\"text-generation\\",\\n        model=model_path,\\n        tokenizer=model_path,\\n        torch_dtype=torch.float16,\\n        device_map=\\"auto\\"\\n    )\\n  \\n    # BLEU score\\n    bleu_metric = load_metric(\\"bleu\\")\\n  \\n    predictions = []\\n    references = []\\n  \\n    for example in test_dataset:\\n        # Generate response\\n        prompt = example[\\"instruction\\"]\\n        generated = generator(\\n            prompt,\\n            max_length=512,\\n            num_return_sequences=1,\\n            temperature=0.7,\\n            do_sample=True,\\n            pad_token_id=generator.tokenizer.eos_token_id\\n        )[0][\\"generated_text\\"]\\n    \\n        # Extract generated part\\n        generated_text = generated[len(prompt):].strip()\\n    \\n        predictions.append(generated_text)\\n        references.append([example[\\"output\\"]])\\n  \\n    # Calculate BLEU score\\n    bleu_score = bleu_metric.compute(\\n        predictions=predictions,\\n        references=references\\n    )\\n  \\n    print(f\\"BLEU Score: {bleu_score[\'bleu\']:.4f}\\")\\n  \\n    return {\\n        \\"bleu\\": bleu_score[\\"bleu\\"],\\n        \\"predictions\\": predictions,\\n        \\"references\\": references\\n    }\\n\\n# Perplexity evaluation\\ndef calculate_perplexity(model, tokenizer, test_texts):\\n    model.eval()\\n    total_loss = 0\\n    total_tokens = 0\\n  \\n    with torch.no_grad():\\n        for text in test_texts:\\n            inputs = tokenizer(text, return_tensors=\\"pt\\", truncation=True, max_length=512)\\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\\n        \\n            outputs = model(**inputs, labels=inputs[\\"input_ids\\"])\\n            loss = outputs.loss\\n        \\n            total_loss += loss.item() * inputs[\\"input_ids\\"].size(1)\\n            total_tokens += inputs[\\"input_ids\\"].size(1)\\n  \\n    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\\n    return perplexity.item()\\n```\\n\\n### Human Evaluation Framework\\n\\n```python\\nimport gradio as gr\\nimport json\\nfrom datetime import datetime\\n\\nclass ModelEvaluator:\\n    def __init__(self, models_dict):\\n        self.models = models_dict\\n        self.results = []\\n  \\n    def create_evaluation_interface(self):\\n        def evaluate_response(prompt, model_name, response, relevance, accuracy, fluency, helpfulness, comments):\\n            result = {\\n                \\"timestamp\\": datetime.now().isoformat(),\\n                \\"prompt\\": prompt,\\n                \\"model\\": model_name,\\n                \\"response\\": response,\\n                \\"scores\\": {\\n                    \\"relevance\\": relevance,\\n                    \\"accuracy\\": accuracy,\\n                    \\"fluency\\": fluency,\\n                    \\"helpfulness\\": helpfulness\\n                },\\n                \\"comments\\": comments,\\n                \\"overall_score\\": (relevance + accuracy + fluency + helpfulness) / 4\\n            }\\n        \\n            self.results.append(result)\\n        \\n            # Save results\\n            with open(\\"evaluation_results.json\\", \\"w\\", encoding=\\"utf-8\\") as f:\\n                json.dump(self.results, f, ensure_ascii=False, indent=2)\\n        \\n            return f\\"Evaluation saved! Overall score: {result[\'overall_score\']:.2f}\\"\\n    \\n        def generate_response(prompt, model_name):\\n            if model_name in self.models:\\n                generator = self.models[model_name]\\n                response = generator(prompt, max_length=512, temperature=0.7)[0][\\"generated_text\\"]\\n                return response[len(prompt):].strip()\\n            return \\"Model not found\\"\\n    \\n        # Gradio interface\\n        with gr.Blocks(title=\\"LLM Model Evaluation System\\") as demo:\\n            gr.Markdown(\\"# LLM Model Evaluation System\\")\\n        \\n            with gr.Row():\\n                with gr.Column():\\n                    prompt_input = gr.Textbox(label=\\"Input Prompt\\", lines=3)\\n                    model_dropdown = gr.Dropdown(\\n                        choices=list(self.models.keys()),\\n                        label=\\"Select Model\\",\\n                        value=list(self.models.keys())[0] if self.models else None\\n                    )\\n                    generate_btn = gr.Button(\\"Generate Response\\")\\n            \\n                with gr.Column():\\n                    response_output = gr.Textbox(label=\\"Model Response\\", lines=5)\\n        \\n            gr.Markdown(\\"## Evaluation Metrics (1-5 scale)\\")\\n        \\n            with gr.Row():\\n                relevance_slider = gr.Slider(1, 5, value=3, label=\\"Relevance\\")\\n                accuracy_slider = gr.Slider(1, 5, value=3, label=\\"Accuracy\\")\\n                fluency_slider = gr.Slider(1, 5, value=3, label=\\"Fluency\\")\\n                helpfulness_slider = gr.Slider(1, 5, value=3, label=\\"Helpfulness\\")\\n        \\n            comments_input = gr.Textbox(label=\\"Comments\\", lines=2)\\n            evaluate_btn = gr.Button(\\"Submit Evaluation\\")\\n            result_output = gr.Textbox(label=\\"Evaluation Result\\")\\n        \\n            # Event binding\\n            generate_btn.click(\\n                generate_response,\\n                inputs=[prompt_input, model_dropdown],\\n                outputs=response_output\\n            )\\n        \\n            evaluate_btn.click(\\n                evaluate_response,\\n                inputs=[\\n                    prompt_input, model_dropdown, response_output,\\n                    relevance_slider, accuracy_slider, fluency_slider, helpfulness_slider,\\n                    comments_input\\n                ],\\n                outputs=result_output\\n            )\\n    \\n        return demo\\n\\n# Usage example\\nmodels = {\\n    \\"Original Model\\": pipeline(\\"text-generation\\", model=\\"Qwen/Qwen2-7B\\"),\\n    \\"Fine-tuned Model\\": pipeline(\\"text-generation\\", model=\\"./lora_model\\")\\n}\\n\\nevaluator = ModelEvaluator(models)\\ndemo = evaluator.create_evaluation_interface()\\ndemo.launch(share=True)\\n```\\n\\n## Model Quantization and Optimization\\n\\n### GPTQ Quantization\\n\\n```python\\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\\nfrom transformers import AutoTokenizer\\n\\n# Quantization configuration\\nquantize_config = BaseQuantizeConfig(\\n    bits=4,  # 4bit quantization\\n    group_size=128,\\n    desc_act=False,\\n    damp_percent=0.1,\\n    sym=True,\\n    true_sequential=True,\\n)\\n\\n# Load model\\nmodel = AutoGPTQForCausalLM.from_pretrained(\\n    \\"Qwen/Qwen2-7B\\",\\n    quantize_config=quantize_config,\\n    low_cpu_mem_usage=True,\\n    device_map=\\"auto\\"\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\"Qwen/Qwen2-7B\\")\\n\\n# Prepare calibration data\\ncalibration_dataset = [\\n    \\"What is the history of artificial intelligence development?\\",\\n    \\"Please explain the basic principles of deep learning.\\",\\n    \\"What are the main algorithms in machine learning?\\",\\n    # More calibration data...\\n]\\n\\n# Execute quantization\\nmodel.quantize(calibration_dataset)\\n\\n# Save quantized model\\nmodel.save_quantized(\\"./qwen2-7b-gptq\\")\\ntokenizer.save_pretrained(\\"./qwen2-7b-gptq\\")\\n```\\n\\n### AWQ Quantization\\n\\n```python\\nfrom awq import AutoAWQForCausalLM\\nfrom transformers import AutoTokenizer\\n\\n# Load model\\nmodel_path = \\"Qwen/Qwen2-7B\\"\\nquant_path = \\"qwen2-7b-awq\\"\\n\\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\\n\\n# Quantization configuration\\nquant_config = {\\n    \\"zero_point\\": True,\\n    \\"q_group_size\\": 128,\\n    \\"w_bit\\": 4,\\n    \\"version\\": \\"GEMM\\"\\n}\\n\\n# Execute quantization\\nmodel.quantize(tokenizer, quant_config=quant_config)\\n\\n# Save quantized model\\nmodel.save_quantized(quant_path)\\ntokenizer.save_pretrained(quant_path)\\n\\nprint(f\\"Quantized model saved to: {quant_path}\\")\\n```\\n\\n## Production Deployment\\n\\n### Docker Containerization\\n\\n**Dockerfile:**\\n\\n```dockerfile\\nFROM nvidia/cuda:12.1-devel-ubuntu22.04\\n\\n# Install system dependencies\\nRUN apt-get update && apt-get install -y \\\\\\n    python3.10 python3-pip git wget curl \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\n# Set working directory\\nWORKDIR /app\\n\\n# Copy requirements file\\nCOPY requirements.txt .\\n\\n# Install Python dependencies\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# Copy application code\\nCOPY . .\\n\\n# Set environment variables\\nENV PYTHONPATH=/app\\nENV CUDA_VISIBLE_DEVICES=0\\n\\n# Expose port\\nEXPOSE 8000\\n\\n# Health check\\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\\\\\n  CMD curl -f http://localhost:8000/health || exit 1\\n\\n# Start command\\nCMD [\\"python\\", \\"serve.py\\"]\\n```\\n\\n**Service script (serve.py):**\\n\\n```python\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\nfrom transformers import pipeline\\nimport torch\\nimport uvicorn\\nfrom typing import Optional\\n\\napp = FastAPI(title=\\"LLM Service API\\", version=\\"1.0.0\\")\\n\\n# Global variables\\ngenerator = None\\n\\nclass GenerateRequest(BaseModel):\\n    prompt: str\\n    max_length: int = 512\\n    temperature: float = 0.7\\n    top_p: float = 0.9\\n    do_sample: bool = True\\n\\nclass GenerateResponse(BaseModel):\\n    generated_text: str\\n    prompt: str\\n\\n@app.on_event(\\"startup\\")\\nasync def load_model():\\n    global generator\\n    print(\\"Loading model...\\")\\n  \\n    generator = pipeline(\\n        \\"text-generation\\",\\n        model=\\"./qwen2-7b-awq\\",  # Quantized model path\\n        tokenizer=\\"./qwen2-7b-awq\\",\\n        torch_dtype=torch.float16,\\n        device_map=\\"auto\\",\\n        trust_remote_code=True\\n    )\\n  \\n    print(\\"Model loaded successfully!\\")\\n\\n@app.get(\\"/health\\")\\nasync def health_check():\\n    return {\\"status\\": \\"healthy\\", \\"model_loaded\\": generator is not None}\\n\\n@app.post(\\"/generate\\", response_model=GenerateResponse)\\nasync def generate_text(request: GenerateRequest):\\n    if generator is None:\\n        raise HTTPException(status_code=503, detail=\\"Model not loaded\\")\\n  \\n    try:\\n        result = generator(\\n            request.prompt,\\n            max_length=request.max_length,\\n            temperature=request.temperature,\\n            top_p=request.top_p,\\n            do_sample=request.do_sample,\\n            pad_token_id=generator.tokenizer.eos_token_id\\n        )\\n    \\n        generated_text = result[0][\\"generated_text\\"][len(request.prompt):].strip()\\n    \\n        return GenerateResponse(\\n            generated_text=generated_text,\\n            prompt=request.prompt\\n        )\\n  \\n    except Exception as e:\\n        raise HTTPException(status_code=500, detail=f\\"Generation failed: {str(e)}\\")\\n\\nif __name__ == \\"__main__\\":\\n    uvicorn.run(app, host=\\"0.0.0.0\\", port=8000)\\n```\\n\\n### Kubernetes Deployment\\n\\n**deployment.yaml:**\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: llm-service\\n  labels:\\n    app: llm-service\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: llm-service\\n  template:\\n    metadata:\\n      labels:\\n        app: llm-service\\n    spec:\\n      containers:\\n      - name: llm-service\\n        image: your-registry/llm-service:latest\\n        ports:\\n        - containerPort: 8000\\n        resources:\\n          requests:\\n            nvidia.com/gpu: 1\\n            memory: \\"16Gi\\"\\n            cpu: \\"4\\"\\n          limits:\\n            nvidia.com/gpu: 1\\n            memory: \\"24Gi\\"\\n            cpu: \\"6\\"\\n        env:\\n        - name: MODEL_PATH\\n          value: \\"/models/qwen2-7b-awq\\"\\n        - name: CUDA_VISIBLE_DEVICES\\n          value: \\"0\\"\\n        volumeMounts:\\n        - name: model-storage\\n          mountPath: /models\\n          readOnly: true\\n        livenessProbe:\\n          httpGet:\\n            path: /health\\n            port: 8000\\n          initialDelaySeconds: 120\\n          periodSeconds: 30\\n          timeoutSeconds: 10\\n        readinessProbe:\\n          httpGet:\\n            path: /health\\n            port: 8000\\n          initialDelaySeconds: 60\\n          periodSeconds: 10\\n          timeoutSeconds: 5\\n      volumes:\\n      - name: model-storage\\n        persistentVolumeClaim:\\n          claimName: model-pvc\\n      nodeSelector:\\n        accelerator: nvidia-gpu\\n      tolerations:\\n      - key: nvidia.com/gpu\\n        operator: Exists\\n        effect: NoSchedule\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: llm-service\\nspec:\\n  selector:\\n    app: llm-service\\n  ports:\\n  - port: 80\\n    targetPort: 8000\\n    protocol: TCP\\n  type: LoadBalancer\\n```\\n\\n## Monitoring and Logging\\n\\n### Prometheus Monitoring\\n\\n```python\\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\\nimport time\\nimport functools\\n\\n# Define metrics\\nREQUEST_COUNT = Counter(\'llm_requests_total\', \'Total requests\', [\'method\', \'endpoint\', \'status\'])\\nREQUEST_LATENCY = Histogram(\'llm_request_duration_seconds\', \'Request latency\')\\nACTIVE_REQUESTS = Gauge(\'llm_active_requests\', \'Active requests\')\\nGPU_MEMORY = Gauge(\'llm_gpu_memory_usage_bytes\', \'GPU memory usage\', [\'gpu_id\'])\\nMODEL_LOAD_TIME = Gauge(\'llm_model_load_time_seconds\', \'Model load time\')\\n\\ndef monitor_requests(func):\\n    @functools.wraps(func)\\n    async def wrapper(*args, **kwargs):\\n        start_time = time.time()\\n        ACTIVE_REQUESTS.inc()\\n    \\n        try:\\n            result = await func(*args, **kwargs)\\n            REQUEST_COUNT.labels(method=\'POST\', endpoint=\'/generate\', status=\'success\').inc()\\n            return result\\n        except Exception as e:\\n            REQUEST_COUNT.labels(method=\'POST\', endpoint=\'/generate\', status=\'error\').inc()\\n            raise\\n        finally:\\n            ACTIVE_REQUESTS.dec()\\n            REQUEST_LATENCY.observe(time.time() - start_time)\\n  \\n    return wrapper\\n\\n# Use in FastAPI\\n@app.post(\\"/generate\\")\\n@monitor_requests\\nasync def generate_text(request: GenerateRequest):\\n    # Original logic\\n    pass\\n\\n# Start Prometheus metrics server\\nstart_http_server(9090)\\n```\\n\\n### Structured Logging\\n\\n```python\\nimport logging\\nimport json\\nfrom datetime import datetime\\nimport sys\\n\\nclass StructuredLogger:\\n    def __init__(self, name):\\n        self.logger = logging.getLogger(name)\\n        self.logger.setLevel(logging.INFO)\\n    \\n        # Create handler\\n        handler = logging.StreamHandler(sys.stdout)\\n        handler.setFormatter(self.JSONFormatter())\\n    \\n        self.logger.addHandler(handler)\\n  \\n    class JSONFormatter(logging.Formatter):\\n        def format(self, record):\\n            log_entry = {\\n                \\"timestamp\\": datetime.utcnow().isoformat(),\\n                \\"level\\": record.levelname,\\n                \\"logger\\": record.name,\\n                \\"message\\": record.getMessage(),\\n                \\"module\\": record.module,\\n                \\"function\\": record.funcName,\\n                \\"line\\": record.lineno\\n            }\\n        \\n            # Add extra fields\\n            if hasattr(record, \'user_id\'):\\n                log_entry[\'user_id\'] = record.user_id\\n            if hasattr(record, \'request_id\'):\\n                log_entry[\'request_id\'] = record.request_id\\n            if hasattr(record, \'model_name\'):\\n                log_entry[\'model_name\'] = record.model_name\\n        \\n            return json.dumps(log_entry, ensure_ascii=False)\\n  \\n    def info(self, message, **kwargs):\\n        extra = {k: v for k, v in kwargs.items()}\\n        self.logger.info(message, extra=extra)\\n  \\n    def error(self, message, **kwargs):\\n        extra = {k: v for k, v in kwargs.items()}\\n        self.logger.error(message, extra=extra)\\n  \\n    def warning(self, message, **kwargs):\\n        extra = {k: v for k, v in kwargs.items()}\\n        self.logger.warning(message, extra=extra)\\n\\n# Usage example\\nlogger = StructuredLogger(\\"llm_service\\")\\n\\n@app.post(\\"/generate\\")\\nasync def generate_text(request: GenerateRequest):\\n    request_id = str(uuid.uuid4())\\n  \\n    logger.info(\\n        \\"Received generation request\\",\\n        request_id=request_id,\\n        prompt_length=len(request.prompt),\\n        max_length=request.max_length,\\n        temperature=request.temperature\\n    )\\n  \\n    try:\\n        start_time = time.time()\\n        result = generator(request.prompt, ...)\\n        generation_time = time.time() - start_time\\n    \\n        logger.info(\\n            \\"Generation completed\\",\\n            request_id=request_id,\\n            generation_time=generation_time,\\n            output_length=len(result[0][\\"generated_text\\"])\\n        )\\n    \\n        return result\\n    \\n    except Exception as e:\\n        logger.error(\\n            \\"Generation failed\\",\\n            request_id=request_id,\\n            error=str(e),\\n            error_type=type(e).__name__\\n        )\\n        raise\\n```\\n\\n## Best Practices Summary\\n\\n### Performance Optimization Tips\\n\\n1. **Memory Management**\\n\\n   - Use gradient checkpointing to reduce memory usage\\n   - Enable CPU offloading for large models\\n   - Set appropriate batch size and sequence length\\n2. **Training Acceleration**\\n\\n   - Use FlashAttention-2\\n   - Enable mixed precision training (FP16/BF16)\\n   - Use DeepSpeed ZeRO optimization\\n3. **Inference Optimization**\\n\\n   - Model quantization (GPTQ/AWQ)\\n   - Use vLLM for efficient inference\\n   - Batch requests to improve throughput\\n\\n### Security Considerations\\n\\n1. **Data Security**\\n\\n   - Anonymize training data\\n   - Filter model output content\\n   - Validate and sanitize user inputs\\n2. **Model Security**\\n\\n   - Regular backup of model checkpoints\\n   - Version control and rollback mechanisms\\n   - Access control and permissions\\n3. **Deployment Security**\\n\\n   - API authentication and authorization\\n   - Request rate limiting\\n   - Network security configuration\\n\\n### Cost Control\\n\\n1. **Compute Resources**\\n\\n   - Use Spot instances to reduce costs\\n   - Auto-scaling based on load\\n   - Choose appropriate GPU models\\n2. **Storage Optimization**\\n\\n   - Model compression and quantization\\n   - Data deduplication and compression\\n   - Tiered storage for hot/cold data\\n\\nThis complete guide covers the entire workflow from environment setup to production deployment, and you can choose the appropriate technical solutions based on specific requirements.\\n\\n---\\n\\n*Last updated: January 2025*"},{"id":"canopy-photosynthesis-modeling-en","metadata":{"permalink":"/blog/canopy-photosynthesis-modeling-en","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-09-20-canopy-photosynthesis-modeling.md","source":"@site/blog/2023-09-20-canopy-photosynthesis-modeling.md","title":"Canopy Photosynthesis Modeling Tutorial Based on 3D Reconstruction","description":"Introduction","date":"2023-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"Plant Phenotyping","permalink":"/blog/tags/plant-phenotyping","description":"Automated plant trait analysis"},{"inline":false,"label":"3D Reconstruction","permalink":"/blog/tags/3d-reconstruction-alt","description":"3D model reconstruction technology"},{"inline":false,"label":"Photosynthesis","permalink":"/blog/tags/photosynthesis","description":"Photosynthesis modeling and simulation"},{"inline":false,"label":"Computer Vision","permalink":"/blog/tags/computer-vision","description":"Computer vision and image processing"},{"inline":false,"label":"Ray Tracing","permalink":"/blog/tags/ray-tracing","description":"Ray tracing algorithms and light simulation"}],"readingTime":15.77,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"canopy-photosynthesis-modeling-en","title":"Canopy Photosynthesis Modeling Tutorial Based on 3D Reconstruction","authors":["liangchao"],"tags":["plant phenotyping","3d reconstruction","photosynthesis","computer vision","ray tracing"]},"unlisted":false,"prevItem":{"title":"Guide to Local LLM Deployment, Training and Fine-tuning","permalink":"/blog/local-llm-training-guide-en"},"nextItem":{"title":"Complete PyTorch Tutorial for Machine Learning and Deep Learning","permalink":"/blog/pytorch-ml-dl-tutorial"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Canopy Photosynthesis Modeling Tutorial Based on 3D Reconstruction\\n\\n## Introduction\\n\\nCanopy photosynthesis modeling is a crucial research area in precision agriculture and plant phenomics. This comprehensive tutorial demonstrates how to build a complete canopy photosynthesis simulation system through multi-view image acquisition, 3D reconstruction, canopy construction, and ray tracing techniques. The complete workflow includes: Camera Capture \u2192 SfM/3DGS Reconstruction \u2192 Mesh Generation \u2192 Canopy Construction \u2192 Light Distribution Simulation \u2192 Photosynthesis Calculation.\\n\\n## Technical Workflow Overview\\n\\n```mermaid\\ngraph TD\\n    A[Multi-view Image Capture] --\x3e B[SfM 3D Reconstruction]\\n    A --\x3e C[3D Gaussian Splatting]\\n    B --\x3e D[Point Cloud Generation]\\n    C --\x3e D\\n    D --\x3e E[Triangle Mesh Generation]\\n    E --\x3e F[Plant Mesh Model]\\n    F --\x3e G[Plant Replication & Perturbation]\\n    G --\x3e H[3D Canopy Model]\\n    H --\x3e I[Ray Tracing Algorithm]\\n    I --\x3e J[Light Distribution Calculation]\\n    J --\x3e K[Light Response Curve Model]\\n    K --\x3e L[Canopy Photosynthesis Rate]\\n```\\n\\n## Step 1: Multi-view Image Acquisition System\\n\\n### Hardware Configuration\\n\\n```python\\n# Multi-view capture system configuration\\nimport numpy as np\\nimport cv2\\nimport json\\nfrom datetime import datetime\\n\\nclass MultiViewCaptureSystem:\\n    def __init__(self, camera_config):\\n        self.cameras = []\\n        self.calibration_data = {}\\n        self.capture_positions = self.generate_capture_positions()\\n        \\n    def generate_capture_positions(self, radius=1.5, height_levels=3, angles_per_level=12):\\n        \\"\\"\\"Generate spherical capture positions\\"\\"\\"\\n        positions = []\\n        \\n        for h_idx in range(height_levels):\\n            # Height from 0.5m to 2.0m\\n            height = 0.5 + (h_idx * 0.75)\\n            \\n            for angle_idx in range(angles_per_level):\\n                angle = (angle_idx * 360 / angles_per_level) * np.pi / 180\\n                \\n                x = radius * np.cos(angle)\\n                y = radius * np.sin(angle)\\n                z = height\\n                \\n                # Camera looks at plant center\\n                look_at = np.array([0, 0, 1.0])  # Plant center height\\n                camera_pos = np.array([x, y, z])\\n                \\n                positions.append({\\n                    \'position\': camera_pos,\\n                    \'look_at\': look_at,\\n                    \'up_vector\': np.array([0, 0, 1])\\n                })\\n        \\n        return positions\\n    \\n    def capture_sequence(self, plant_id, output_dir):\\n        \\"\\"\\"Execute multi-view image capture\\"\\"\\"\\n        metadata = {\\n            \'plant_id\': plant_id,\\n            \'timestamp\': datetime.now().isoformat(),\\n            \'positions\': [],\\n            \'camera_params\': self.get_camera_parameters()\\n        }\\n        \\n        for idx, pos_config in enumerate(self.capture_positions):\\n            # Move camera to specified position (requires robotic arm or rail system)\\n            self.move_camera_to_position(pos_config)\\n            \\n            # Capture image\\n            image_path = f\\"{output_dir}/image_{idx:03d}.jpg\\"\\n            image = self.capture_image()\\n            cv2.imwrite(image_path, image)\\n            \\n            # Record position information\\n            metadata[\'positions\'].append({\\n                \'image_id\': idx,\\n                \'position\': pos_config[\'position\'].tolist(),\\n                \'look_at\': pos_config[\'look_at\'].tolist(),\\n                \'up_vector\': pos_config[\'up_vector\'].tolist()\\n            })\\n        \\n        # Save metadata\\n        with open(f\\"{output_dir}/metadata.json\\", \'w\') as f:\\n            json.dump(metadata, f, indent=2)\\n        \\n        return metadata\\n    \\n    def get_camera_parameters(self):\\n        \\"\\"\\"Get camera intrinsic parameters\\"\\"\\"\\n        return {\\n            \'focal_length\': 35.0,  # mm\\n            \'sensor_width\': 36.0,  # mm\\n            \'sensor_height\': 24.0,  # mm\\n            \'image_width\': 4000,\\n            \'image_height\': 3000,\\n            \'distortion_coeffs\': [0.1, -0.2, 0.0, 0.0, 0.0]\\n        }\\n```\\n\\n### Adaptive Capture Strategy\\n\\n```python\\nclass AdaptiveCaptureStrategy:\\n    def __init__(self):\\n        self.quality_threshold = 0.8\\n        self.overlap_ratio = 0.6\\n        \\n    def optimize_capture_positions(self, plant_bbox, complexity_map):\\n        \\"\\"\\"Optimize capture positions based on plant complexity\\"\\"\\"\\n        base_positions = self.generate_base_positions(plant_bbox)\\n        \\n        # Increase capture density based on plant complexity\\n        enhanced_positions = []\\n        for pos in base_positions:\\n            enhanced_positions.append(pos)\\n            \\n            # Add extra views in complex regions\\n            complexity = self.estimate_local_complexity(pos, complexity_map)\\n            if complexity > 0.7:\\n                additional_views = self.generate_additional_views(pos, num_views=3)\\n                enhanced_positions.extend(additional_views)\\n        \\n        return enhanced_positions\\n    \\n    def estimate_image_quality(self, image):\\n        \\"\\"\\"Evaluate image quality\\"\\"\\"\\n        # Calculate image sharpness\\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\\n        \\n        # Calculate exposure quality\\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\\n        exposure_quality = 1.0 - np.sum(hist[[0, 255]]) / image.size\\n        \\n        # Overall quality score\\n        quality_score = (laplacian_var / 1000.0) * exposure_quality\\n        return min(quality_score, 1.0)\\n```\\n\\n## Step 2: Structure from Motion (SfM) Reconstruction\\n\\n### COLMAP Integration\\n\\n```python\\nimport subprocess\\nimport os\\nfrom pathlib import Path\\n\\nclass SfMReconstruction:\\n    def __init__(self, colmap_path=\\"/usr/local/bin/colmap\\"):\\n        self.colmap_path = colmap_path\\n        \\n    def run_sfm_pipeline(self, image_dir, output_dir, camera_model=\\"PINHOLE\\"):\\n        \\"\\"\\"Execute complete SfM reconstruction pipeline\\"\\"\\"\\n        \\n        # Create output directory\\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\\n        database_path = os.path.join(output_dir, \\"database.db\\")\\n        \\n        # 1. Feature extraction\\n        self.extract_features(image_dir, database_path, camera_model)\\n        \\n        # 2. Feature matching\\n        self.match_features(database_path)\\n        \\n        # 3. Sparse reconstruction\\n        sparse_dir = os.path.join(output_dir, \\"sparse\\")\\n        self.sparse_reconstruction(database_path, image_dir, sparse_dir)\\n        \\n        # 4. Dense reconstruction\\n        dense_dir = os.path.join(output_dir, \\"dense\\")\\n        self.dense_reconstruction(image_dir, sparse_dir, dense_dir)\\n        \\n        return dense_dir\\n    \\n    def extract_features(self, image_dir, database_path, camera_model):\\n        \\"\\"\\"Feature extraction\\"\\"\\"\\n        cmd = [\\n            self.colmap_path, \\"feature_extractor\\",\\n            \\"--database_path\\", database_path,\\n            \\"--image_path\\", image_dir,\\n            \\"--ImageReader.camera_model\\", camera_model,\\n            \\"--SiftExtraction.use_gpu\\", \\"1\\",\\n            \\"--SiftExtraction.max_num_features\\", \\"8192\\"\\n        ]\\n        subprocess.run(cmd, check=True)\\n    \\n    def match_features(self, database_path):\\n        \\"\\"\\"Feature matching\\"\\"\\"\\n        cmd = [\\n            self.colmap_path, \\"exhaustive_matcher\\",\\n            \\"--database_path\\", database_path,\\n            \\"--SiftMatching.use_gpu\\", \\"1\\"\\n        ]\\n        subprocess.run(cmd, check=True)\\n    \\n    def sparse_reconstruction(self, database_path, image_dir, output_dir):\\n        \\"\\"\\"Sparse reconstruction\\"\\"\\"\\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\\n        \\n        cmd = [\\n            self.colmap_path, \\"mapper\\",\\n            \\"--database_path\\", database_path,\\n            \\"--image_path\\", image_dir,\\n            \\"--output_path\\", output_dir\\n        ]\\n        subprocess.run(cmd, check=True)\\n    \\n    def dense_reconstruction(self, image_dir, sparse_dir, dense_dir):\\n        \\"\\"\\"Dense reconstruction\\"\\"\\"\\n        Path(dense_dir).mkdir(parents=True, exist_ok=True)\\n        \\n        # Image undistortion\\n        cmd_undistort = [\\n            self.colmap_path, \\"image_undistorter\\",\\n            \\"--image_path\\", image_dir,\\n            \\"--input_path\\", os.path.join(sparse_dir, \\"0\\"),\\n            \\"--output_path\\", dense_dir,\\n            \\"--output_type\\", \\"COLMAP\\"\\n        ]\\n        subprocess.run(cmd_undistort, check=True)\\n        \\n        # Stereo matching\\n        cmd_stereo = [\\n            self.colmap_path, \\"patch_match_stereo\\",\\n            \\"--workspace_path\\", dense_dir,\\n            \\"--workspace_format\\", \\"COLMAP\\",\\n            \\"--PatchMatchStereo.geom_consistency\\", \\"1\\"\\n        ]\\n        subprocess.run(cmd_stereo, check=True)\\n        \\n        # Stereo fusion\\n        cmd_fusion = [\\n            self.colmap_path, \\"stereo_fusion\\",\\n            \\"--workspace_path\\", dense_dir,\\n            \\"--workspace_format\\", \\"COLMAP\\",\\n            \\"--input_type\\", \\"geometric\\",\\n            \\"--output_path\\", os.path.join(dense_dir, \\"fused.ply\\")\\n        ]\\n        subprocess.run(cmd_fusion, check=True)\\n```\\n\\n## Step 3: 3D Gaussian Splatting Reconstruction\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport numpy as np\\nfrom scipy.spatial.transform import Rotation\\n\\nclass GaussianSplattingReconstruction:\\n    def __init__(self, device=\\"cuda\\"):\\n        self.device = device\\n        self.gaussians = None\\n        \\n    def initialize_gaussians_from_sfm(self, point_cloud_path, num_gaussians=100000):\\n        \\"\\"\\"Initialize Gaussians from SfM point cloud\\"\\"\\"\\n        # Load SfM point cloud\\n        points, colors = self.load_point_cloud(point_cloud_path)\\n        \\n        # Initialize Gaussian parameters\\n        positions = torch.tensor(points, dtype=torch.float32, device=self.device)\\n        colors = torch.tensor(colors, dtype=torch.float32, device=self.device)\\n        \\n        # Initialize scales and rotations\\n        scales = torch.ones((len(points), 3), device=self.device) * 0.01\\n        rotations = torch.zeros((len(points), 4), device=self.device)\\n        rotations[:, 0] = 1.0  # Unit quaternion\\n        \\n        # Initialize opacities\\n        opacities = torch.ones((len(points), 1), device=self.device) * 0.5\\n        \\n        self.gaussians = {\\n            \'positions\': nn.Parameter(positions),\\n            \'colors\': nn.Parameter(colors),\\n            \'scales\': nn.Parameter(scales),\\n            \'rotations\': nn.Parameter(rotations),\\n            \'opacities\': nn.Parameter(opacities)\\n        }\\n        \\n        return self.gaussians\\n    \\n    def train_gaussians(self, images, camera_poses, num_iterations=30000):\\n        \\"\\"\\"Train 3D Gaussians\\"\\"\\"\\n        optimizer = torch.optim.Adam([\\n            {\'params\': [self.gaussians[\'positions\']], \'lr\': 0.00016},\\n            {\'params\': [self.gaussians[\'colors\']], \'lr\': 0.0025},\\n            {\'params\': [self.gaussians[\'scales\']], \'lr\': 0.005},\\n            {\'params\': [self.gaussians[\'rotations\']], \'lr\': 0.001},\\n            {\'params\': [self.gaussians[\'opacities\']], \'lr\': 0.05}\\n        ])\\n        \\n        for iteration in range(num_iterations):\\n            # Randomly select training view\\n            cam_idx = np.random.randint(0, len(images))\\n            target_image = images[cam_idx]\\n            camera_pose = camera_poses[cam_idx]\\n            \\n            # Render image\\n            rendered_image = self.render_gaussian_splatting(camera_pose)\\n            \\n            # Compute loss\\n            loss = self.compute_loss(rendered_image, target_image)\\n            \\n            # Backpropagation\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n            \\n            # Adaptive density control\\n            if iteration % 100 == 0:\\n                self.adaptive_density_control()\\n            \\n            if iteration % 1000 == 0:\\n                print(f\\"Iteration {iteration}, Loss: {loss.item():.6f}\\")\\n    \\n    def extract_mesh_from_gaussians(self, resolution=512):\\n        \\"\\"\\"Extract mesh from Gaussians\\"\\"\\"\\n        # Use Marching Cubes algorithm\\n        from skimage import measure\\n        \\n        # Create voxel grid\\n        x = np.linspace(-2, 2, resolution)\\n        y = np.linspace(-2, 2, resolution)\\n        z = np.linspace(-2, 2, resolution)\\n        X, Y, Z = np.meshgrid(x, y, z)\\n        \\n        # Calculate density for each voxel\\n        density = self.evaluate_gaussian_density(X, Y, Z)\\n        \\n        # Extract isosurface\\n        vertices, faces, _, _ = measure.marching_cubes(density, level=0.1)\\n        \\n        return vertices, faces\\n```\\n\\n## Step 4: Mesh Processing and Plant Model Construction\\n\\n```python\\nimport trimesh\\nimport open3d as o3d\\nfrom scipy.spatial import ConvexHull\\n\\nclass PlantMeshProcessor:\\n    def __init__(self):\\n        self.mesh = None\\n        self.leaf_segments = []\\n        self.stem_segments = []\\n        \\n    def process_point_cloud_to_mesh(self, point_cloud_path):\\n        \\"\\"\\"Convert point cloud to mesh\\"\\"\\"\\n        # Load point cloud\\n        pcd = o3d.io.read_point_cloud(point_cloud_path)\\n        \\n        # Point cloud preprocessing\\n        pcd = self.preprocess_point_cloud(pcd)\\n        \\n        # Poisson reconstruction\\n        mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\\n            pcd, depth=9, width=0, scale=1.1, linear_fit=False\\n        )\\n        \\n        # Mesh post-processing\\n        mesh = self.postprocess_mesh(mesh)\\n        \\n        self.mesh = mesh\\n        return mesh\\n    \\n    def preprocess_point_cloud(self, pcd):\\n        \\"\\"\\"Point cloud preprocessing\\"\\"\\"\\n        # Remove outliers\\n        pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\\n        \\n        # Estimate normals\\n        pcd.estimate_normals(\\n            search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30)\\n        )\\n        \\n        # Orient normals consistently\\n        pcd.orient_normals_consistent_tangent_plane(100)\\n        \\n        return pcd\\n    \\n    def postprocess_mesh(self, mesh):\\n        \\"\\"\\"Mesh post-processing\\"\\"\\"\\n        # Remove duplicate vertices\\n        mesh.remove_duplicated_vertices()\\n        \\n        # Remove duplicate triangles\\n        mesh.remove_duplicated_triangles()\\n        \\n        # Remove degenerate triangles\\n        mesh.remove_degenerate_triangles()\\n        \\n        # Remove non-manifold edges\\n        mesh.remove_non_manifold_edges()\\n        \\n        # Smooth mesh\\n        mesh = mesh.filter_smooth_simple(number_of_iterations=5)\\n        \\n        return mesh\\n    \\n    def segment_plant_organs(self, mesh):\\n        \\"\\"\\"Plant organ segmentation\\"\\"\\"\\n        vertices = np.asarray(mesh.vertices)\\n        faces = np.asarray(mesh.triangles)\\n        \\n        # Geometry-based segmentation\\n        leaf_indices, stem_indices = self.geometric_segmentation(vertices, faces)\\n        \\n        # Create leaf and stem meshes\\n        leaf_mesh = self.create_submesh(mesh, leaf_indices)\\n        stem_mesh = self.create_submesh(mesh, stem_indices)\\n        \\n        self.leaf_segments = self.extract_individual_leaves(leaf_mesh)\\n        self.stem_segments = [stem_mesh]\\n        \\n        return self.leaf_segments, self.stem_segments\\n    \\n    def geometric_segmentation(self, vertices, faces):\\n        \\"\\"\\"Geometry-based segmentation\\"\\"\\"\\n        # Calculate vertex geometric features\\n        curvatures = self.compute_curvature(vertices, faces)\\n        normals = self.compute_normals(vertices, faces)\\n        \\n        # Classify based on curvature and normals\\n        leaf_threshold = 0.5\\n        leaf_indices = np.where(curvatures > leaf_threshold)[0]\\n        stem_indices = np.where(curvatures <= leaf_threshold)[0]\\n        \\n        return leaf_indices, stem_indices\\n    \\n    def compute_curvature(self, vertices, faces):\\n        \\"\\"\\"Calculate vertex curvature\\"\\"\\"\\n        # Simplified curvature calculation\\n        mesh_trimesh = trimesh.Trimesh(vertices=vertices, faces=faces)\\n        curvatures = trimesh.curvature.discrete_gaussian_curvature_measure(\\n            mesh_trimesh, vertices, radius=0.05\\n        )\\n        return np.abs(curvatures)\\n```\\n\\n## Step 5: Canopy Construction with Random Perturbation\\n\\n```python\\nclass CanopyBuilder:\\n    def __init__(self, plant_mesh, plant_segments):\\n        self.base_plant = plant_mesh\\n        self.leaf_segments = plant_segments[\'leaves\']\\n        self.stem_segments = plant_segments[\'stems\']\\n        self.canopy_plants = []\\n        \\n    def build_canopy(self, canopy_config):\\n        \\"\\"\\"Build canopy structure\\"\\"\\"\\n        # Generate plant positions\\n        positions = self.generate_plant_positions(canopy_config)\\n        \\n        # Create plant instance for each position\\n        for i, pos in enumerate(positions):\\n            plant_instance = self.create_plant_instance(pos, i)\\n            self.canopy_plants.append(plant_instance)\\n        \\n        # Merge all plants\\n        canopy_mesh = self.merge_plants()\\n        \\n        return canopy_mesh, self.canopy_plants\\n    \\n    def generate_plant_positions(self, config):\\n        \\"\\"\\"Generate plant positions\\"\\"\\"\\n        row_spacing = config[\'row_spacing\']  # Row spacing\\n        plant_spacing = config[\'plant_spacing\']  # Plant spacing\\n        num_rows = config[\'num_rows\']\\n        plants_per_row = config[\'plants_per_row\']\\n        \\n        positions = []\\n        for row in range(num_rows):\\n            for plant in range(plants_per_row):\\n                x = plant * plant_spacing\\n                y = row * row_spacing\\n                z = 0  # Ground level\\n                \\n                # Add random perturbation\\n                x += np.random.normal(0, plant_spacing * 0.1)\\n                y += np.random.normal(0, row_spacing * 0.1)\\n                \\n                positions.append([x, y, z])\\n        \\n        return np.array(positions)\\n    \\n    def create_plant_instance(self, position, plant_id):\\n        \\"\\"\\"Create individual plant instance\\"\\"\\"\\n        # Copy base plant\\n        plant_mesh = self.base_plant.copy()\\n        \\n        # Apply random transformation\\n        transform_matrix = self.generate_random_transform(position, plant_id)\\n        plant_mesh.transform(transform_matrix)\\n        \\n        # Apply morphological variation\\n        plant_mesh = self.apply_morphological_variation(plant_mesh, plant_id)\\n        \\n        return {\\n            \'mesh\': plant_mesh,\\n            \'position\': position,\\n            \'id\': plant_id,\\n            \'transform\': transform_matrix\\n        }\\n    \\n    def generate_random_transform(self, position, plant_id):\\n        \\"\\"\\"Generate random transformation matrix\\"\\"\\"\\n        # Set random seed for reproducibility\\n        np.random.seed(plant_id)\\n        \\n        # Random rotation (mainly around Z-axis)\\n        rotation_z = np.random.uniform(0, 2 * np.pi)\\n        rotation_x = np.random.normal(0, 0.1)  # Slight tilt\\n        rotation_y = np.random.normal(0, 0.1)\\n        \\n        # Random scaling\\n        scale_factor = np.random.normal(1.0, 0.15)\\n        scale_factor = np.clip(scale_factor, 0.7, 1.3)\\n        \\n        # Build transformation matrix\\n        transform = np.eye(4)\\n        \\n        # Apply scaling\\n        transform[:3, :3] *= scale_factor\\n        \\n        # Apply rotation\\n        from scipy.spatial.transform import Rotation\\n        rotation = Rotation.from_euler(\'xyz\', [rotation_x, rotation_y, rotation_z])\\n        transform[:3, :3] = rotation.as_matrix() @ transform[:3, :3]\\n        \\n        # Apply translation\\n        transform[:3, 3] = position\\n        \\n        return transform\\n    \\n    def apply_morphological_variation(self, mesh, plant_id):\\n        \\"\\"\\"Apply morphological variation\\"\\"\\"\\n        vertices = np.asarray(mesh.vertices)\\n        \\n        # Leaf shape variation\\n        leaf_variation = self.generate_leaf_variation(plant_id)\\n        vertices = self.apply_leaf_deformation(vertices, leaf_variation)\\n        \\n        # Stem variation\\n        stem_variation = self.generate_stem_variation(plant_id)\\n        vertices = self.apply_stem_deformation(vertices, stem_variation)\\n        \\n        # Update mesh\\n        mesh.vertices = o3d.utility.Vector3dVector(vertices)\\n        mesh.compute_vertex_normals()\\n        \\n        return mesh\\n    \\n    def generate_leaf_variation(self, plant_id):\\n        \\"\\"\\"Generate leaf variation parameters\\"\\"\\"\\n        np.random.seed(plant_id + 1000)\\n        \\n        return {\\n            \'length_factor\': np.random.normal(1.0, 0.2),\\n            \'width_factor\': np.random.normal(1.0, 0.15),\\n            \'curvature_factor\': np.random.normal(1.0, 0.3),\\n            \'angle_variation\': np.random.normal(0, 0.2)\\n        }\\n```\\n\\n## Step 6: Ray Tracing Algorithm Implementation\\n\\n```python\\nimport numpy as np\\nfrom numba import jit, cuda\\nimport matplotlib.pyplot as plt\\n\\nclass RayTracingEngine:\\n    def __init__(self, canopy_mesh, light_config):\\n        self.canopy_mesh = canopy_mesh\\n        self.light_config = light_config\\n        self.acceleration_structure = None\\n        self.build_acceleration_structure()\\n        \\n    def build_acceleration_structure(self):\\n        \\"\\"\\"Build acceleration structure (BVH tree)\\"\\"\\"\\n        from rtree import index\\n        \\n        # Create spatial index\\n        idx = index.Index()\\n        \\n        faces = np.asarray(self.canopy_mesh.triangles)\\n        vertices = np.asarray(self.canopy_mesh.vertices)\\n        \\n        for i, face in enumerate(faces):\\n            # Calculate triangle bounding box\\n            triangle_vertices = vertices[face]\\n            min_coords = np.min(triangle_vertices, axis=0)\\n            max_coords = np.max(triangle_vertices, axis=0)\\n            \\n            # Insert into spatial index\\n            idx.insert(i, (*min_coords, *max_coords))\\n        \\n        self.acceleration_structure = idx\\n        self.faces = faces\\n        self.vertices = vertices\\n    \\n    def simulate_light_distribution(self, sun_angles, num_rays=1000000):\\n        \\"\\"\\"Simulate light distribution\\"\\"\\"\\n        results = {}\\n        \\n        for time_step, sun_angle in enumerate(sun_angles):\\n            print(f\\"Computing light distribution for time step {time_step}\\")\\n            \\n            # Generate rays\\n            rays = self.generate_sun_rays(sun_angle, num_rays)\\n            \\n            # Ray tracing\\n            intersections = self.trace_rays(rays)\\n            \\n            # Calculate light intensity distribution\\n            light_map = self.compute_light_intensity_map(intersections)\\n            \\n            results[time_step] = {\\n                \'sun_angle\': sun_angle,\\n                \'light_map\': light_map,\\n                \'intersections\': intersections\\n            }\\n        \\n        return results\\n    \\n    def generate_sun_rays(self, sun_angle, num_rays):\\n        \\"\\"\\"Generate sun rays\\"\\"\\"\\n        # Sun direction vector\\n        elevation, azimuth = sun_angle\\n        sun_direction = np.array([\\n            np.cos(elevation) * np.sin(azimuth),\\n            np.cos(elevation) * np.cos(azimuth),\\n            -np.sin(elevation)  # Downward\\n        ])\\n        \\n        # Generate parallel rays\\n        # Create ray origin grid above canopy\\n        canopy_bounds = self.get_canopy_bounds()\\n        \\n        # Extend bounds to ensure full canopy coverage\\n        x_min, x_max = canopy_bounds[0] - 1, canopy_bounds[1] + 1\\n        y_min, y_max = canopy_bounds[2] - 1, canopy_bounds[3] + 1\\n        z_start = canopy_bounds[5] + 2  # 2m above canopy top\\n        \\n        # Generate random origins\\n        origins = np.random.uniform(\\n            [x_min, y_min, z_start],\\n            [x_max, y_max, z_start],\\n            (num_rays, 3)\\n        )\\n        \\n        # All rays have same direction (parallel light)\\n        directions = np.tile(sun_direction, (num_rays, 1))\\n        \\n        return {\\n            \'origins\': origins,\\n            \'directions\': directions\\n        }\\n    \\n    @jit(nopython=True)\\n    def ray_triangle_intersection(self, ray_origin, ray_direction, v0, v1, v2):\\n        \\"\\"\\"Ray-triangle intersection test (M\xf6ller-Trumbore algorithm)\\"\\"\\"\\n        epsilon = 1e-8\\n        \\n        edge1 = v1 - v0\\n        edge2 = v2 - v0\\n        h = np.cross(ray_direction, edge2)\\n        a = np.dot(edge1, h)\\n        \\n        if abs(a) < epsilon:\\n            return False, 0.0, 0.0, 0.0\\n        \\n        f = 1.0 / a\\n        s = ray_origin - v0\\n        u = f * np.dot(s, h)\\n        \\n        if u < 0.0 or u > 1.0:\\n            return False, 0.0, 0.0, 0.0\\n        \\n        q = np.cross(s, edge1)\\n        v = f * np.dot(ray_direction, q)\\n        \\n        if v < 0.0 or u + v > 1.0:\\n            return False, 0.0, 0.0, 0.0\\n        \\n        t = f * np.dot(edge2, q)\\n        \\n        if t > epsilon:\\n            return True, t, u, v\\n        \\n        return False, 0.0, 0.0, 0.0\\n    \\n    def trace_rays(self, rays):\\n        \\"\\"\\"Ray tracing\\"\\"\\"\\n        origins = rays[\'origins\']\\n        directions = rays[\'directions\']\\n        intersections = []\\n        \\n        for i in range(len(origins)):\\n            ray_origin = origins[i]\\n            ray_direction = directions[i]\\n            \\n            # Use spatial index to accelerate intersection tests\\n            intersection = self.find_closest_intersection(ray_origin, ray_direction)\\n            \\n            if intersection is not None:\\n                intersections.append({\\n                    \'ray_id\': i,\\n                    \'point\': intersection[\'point\'],\\n                    \'normal\': intersection[\'normal\'],\\n                    \'face_id\': intersection[\'face_id\'],\\n                    \'distance\': intersection[\'distance\']\\n                })\\n        \\n        return intersections\\n    \\n    def compute_light_intensity_map(self, intersections):\\n        \\"\\"\\"Calculate light intensity distribution map\\"\\"\\"\\n        # Create 3D grid\\n        bounds = self.get_canopy_bounds()\\n        resolution = 100\\n        \\n        x = np.linspace(bounds[0], bounds[1], resolution)\\n        y = np.linspace(bounds[2], bounds[3], resolution)\\n        z = np.linspace(bounds[4], bounds[5], resolution)\\n        \\n        light_intensity = np.zeros((resolution, resolution, resolution))\\n        \\n        # Map intersection points to grid\\n        for intersection in intersections:\\n            point = intersection[\'point\']\\n            \\n            # Find corresponding grid indices\\n            xi = int((point[0] - bounds[0]) / (bounds[1] - bounds[0]) * (resolution - 1))\\n            yi = int((point[1] - bounds[2]) / (bounds[3] - bounds[2]) * (resolution - 1))\\n            zi = int((point[2] - bounds[4]) / (bounds[5] - bounds[4]) * (resolution - 1))\\n            \\n            # Ensure indices are within valid range\\n            xi = np.clip(xi, 0, resolution - 1)\\n            yi = np.clip(yi, 0, resolution - 1)\\n            zi = np.clip(zi, 0, resolution - 1)\\n            \\n            # Increase light intensity\\n            light_intensity[xi, yi, zi] += 1\\n        \\n        return {\\n            \'intensity\': light_intensity,\\n            \'bounds\': bounds,\\n            \'resolution\': resolution,\\n            \'coordinates\': (x, y, z)\\n        }\\n```\\n\\n## Step 7: Light Response Curve Model\\n\\n```python\\nclass PhotosynthesisModel:\\n    def __init__(self):\\n        # Farquhar-von Caemmerer-Berry model parameters\\n        self.vcmax25 = 60.0  # Maximum carboxylation rate at 25\xb0C (\u03bcmol m\u207b\xb2 s\u207b\xb9)\\n        self.jmax25 = 120.0  # Maximum electron transport rate at 25\xb0C (\u03bcmol m\u207b\xb2 s\u207b\xb9)\\n        self.rd25 = 1.5      # Dark respiration rate at 25\xb0C (\u03bcmol m\u207b\xb2 s\u207b\xb9)\\n        \\n        # Temperature response parameters\\n        self.ha_vcmax = 65330.0  # Vcmax activation energy (J mol\u207b\xb9)\\n        self.ha_jmax = 43540.0   # Jmax activation energy (J mol\u207b\xb9)\\n        self.ha_rd = 46390.0     # Rd activation energy (J mol\u207b\xb9)\\n        \\n        # Other parameters\\n        self.kc25 = 404.9    # CO2 Michaelis constant at 25\xb0C (\u03bcmol mol\u207b\xb9)\\n        self.ko25 = 278.4    # O2 Michaelis constant at 25\xb0C (mmol mol\u207b\xb9)\\n        self.cp25 = 42.75    # CO2 compensation point at 25\xb0C (\u03bcmol mol\u207b\xb9)\\n        \\n        self.r_gas = 8.314   # Gas constant (J mol\u207b\xb9 K\u207b\xb9)\\n        \\n    def compute_photosynthesis_rate(self, light_intensity, temperature, co2_conc, leaf_area):\\n        \\"\\"\\"Calculate photosynthesis rate\\"\\"\\"\\n        # Temperature correction\\n        vcmax = self.temperature_correction(self.vcmax25, self.ha_vcmax, temperature)\\n        jmax = self.temperature_correction(self.jmax25, self.ha_jmax, temperature)\\n        rd = self.temperature_correction(self.rd25, self.ha_rd, temperature)\\n        \\n        # Temperature correction for Michaelis constants\\n        kc = self.temperature_correction(self.kc25, 79430.0, temperature)\\n        ko = self.temperature_correction(self.ko25, 36380.0, temperature)\\n        cp = self.temperature_correction(self.cp25, 37830.0, temperature)\\n        \\n        # Calculate electron transport rate\\n        j = self.compute_electron_transport_rate(light_intensity, jmax)\\n        \\n        # Calculate RuBisCO-limited photosynthesis rate\\n        wc = (vcmax * (co2_conc - cp)) / (co2_conc + kc * (1 + 210 / ko))\\n        \\n        # Calculate RuBP regeneration-limited photosynthesis rate\\n        wj = (j * (co2_conc - cp)) / (4 * (co2_conc + 2 * cp))\\n        \\n        # Take minimum (limiting factor)\\n        gross_photosynthesis = min(wc, wj)\\n        \\n        # Net photosynthesis rate\\n        net_photosynthesis = gross_photosynthesis - rd\\n        \\n        # Multiply by leaf area to get total photosynthesis rate\\n        total_rate = net_photosynthesis * leaf_area\\n        \\n        return {\\n            \'net_rate\': net_photosynthesis,\\n            \'total_rate\': total_rate,\\n            \'gross_rate\': gross_photosynthesis,\\n            \'respiration\': rd,\\n            \'electron_transport\': j,\\n            \'rubisco_limited\': wc,\\n            \'rubp_limited\': wj\\n        }\\n    \\n    def temperature_correction(self, rate25, activation_energy, temperature):\\n        \\"\\"\\"Temperature correction function\\"\\"\\"\\n        temp_k = temperature + 273.15\\n        temp25_k = 25.0 + 273.15\\n        \\n        return rate25 * np.exp(activation_energy * (temp_k - temp25_k) / (self.r_gas * temp_k * temp25_k))\\n    \\n    def compute_electron_transport_rate(self, light_intensity, jmax):\\n        \\"\\"\\"Calculate electron transport rate\\"\\"\\"\\n        # Light response curve parameters\\n        alpha = 0.24  # Quantum efficiency\\n        theta = 0.7   # Curvature factor\\n        \\n        # Non-rectangular hyperbola model\\n        a = theta\\n        b = -(alpha * light_intensity + jmax)\\n        c = alpha * light_intensity * jmax\\n        \\n        # Solve quadratic equation\\n        discriminant = b**2 - 4*a*c\\n        if discriminant >= 0:\\n            j = (-b - np.sqrt(discriminant)) / (2*a)\\n        else:\\n            j = 0\\n        \\n        return max(0, j)\\n    \\n    def compute_canopy_photosynthesis(self, light_distribution, leaf_area_distribution, \\n                                    temperature_map, co2_concentration=400):\\n        \\"\\"\\"Calculate canopy photosynthesis rate\\"\\"\\"\\n        total_photosynthesis = 0\\n        detailed_results = []\\n        \\n        # Get light distribution data\\n        light_intensity = light_distribution[\'intensity\']\\n        coordinates = light_distribution[\'coordinates\']\\n        \\n        # Iterate through each voxel\\n        for i in range(light_intensity.shape[0]):\\n            for j in range(light_intensity.shape[1]):\\n                for k in range(light_intensity.shape[2]):\\n                    if light_intensity[i, j, k] > 0:\\n                        # Get parameters for this voxel\\n                        x, y, z = coordinates[0][i], coordinates[1][j], coordinates[2][k]\\n                        light = light_intensity[i, j, k]\\n                        leaf_area = leaf_area_distribution.get((i, j, k), 0)\\n                        temperature = temperature_map.get((i, j, k), 25.0)\\n                        \\n                        if leaf_area > 0:\\n                            # Calculate photosynthesis rate for this voxel\\n                            result = self.compute_photosynthesis_rate(\\n                                light, temperature, co2_concentration, leaf_area\\n                            )\\n                            \\n                            total_photosynthesis += result[\'total_rate\']\\n                            \\n                            detailed_results.append({\\n                                \'position\': (x, y, z),\\n                                \'voxel_index\': (i, j, k),\\n                                \'light_intensity\': light,\\n                                \'leaf_area\': leaf_area,\\n                                \'temperature\': temperature,\\n                                \'photosynthesis_rate\': result[\'total_rate\'],\\n                                \'details\': result\\n                            })\\n        \\n        return {\\n            \'total_canopy_photosynthesis\': total_photosynthesis,\\n            \'voxel_results\': detailed_results,\\n            \'average_rate\': total_photosynthesis / len(detailed_results) if detailed_results else 0\\n        }\\n```\\n\\n## Step 8: Complete Workflow Integration\\n\\n```python\\nclass CanopyPhotosynthesisSimulator:\\n    def __init__(self):\\n        self.capture_system = None\\n        self.reconstruction_engine = None\\n        self.mesh_processor = None\\n        self.canopy_builder = None\\n        self.ray_tracer = None\\n        self.photosynthesis_model = None\\n        \\n    def run_complete_simulation(self, config):\\n        \\"\\"\\"Run complete canopy photosynthesis simulation\\"\\"\\"\\n        print(\\"Starting canopy photosynthesis simulation...\\")\\n        \\n        # 1. Image capture\\n        print(\\"Step 1: Multi-view image capture\\")\\n        if config[\'use_existing_images\']:\\n            image_dir = config[\'image_directory\']\\n        else:\\n            image_dir = self.capture_multi_view_images(config[\'capture_config\'])\\n        \\n        # 2. 3D reconstruction\\n        print(\\"Step 2: 3D reconstruction\\")\\n        if config[\'reconstruction_method\'] == \'sfm\':\\n            point_cloud = self.run_sfm_reconstruction(image_dir)\\n        elif config[\'reconstruction_method\'] == \'3dgs\':\\n            point_cloud = self.run_3dgs_reconstruction(image_dir)\\n        else:\\n            raise ValueError(\\"Unsupported reconstruction method\\")\\n        \\n        # 3. Mesh generation\\n        print(\\"Step 3: Mesh generation\\")\\n        plant_mesh = self.generate_plant_mesh(point_cloud)\\n        \\n        # 4. Organ segmentation\\n        print(\\"Step 4: Organ segmentation\\")\\n        plant_segments = self.segment_plant_organs(plant_mesh)\\n        \\n        # 5. Canopy construction\\n        print(\\"Step 5: Canopy construction\\")\\n        canopy_mesh, canopy_plants = self.build_canopy(plant_mesh, plant_segments, config[\'canopy_config\'])\\n        \\n        # 6. Ray tracing\\n        print(\\"Step 6: Ray tracing simulation\\")\\n        light_distribution = self.simulate_light_distribution(canopy_mesh, config[\'light_config\'])\\n        \\n        # 7. Photosynthesis calculation\\n        print(\\"Step 7: Photosynthesis calculation\\")\\n        photosynthesis_results = self.calculate_canopy_photosynthesis(\\n            light_distribution, canopy_plants, config[\'environmental_config\']\\n        )\\n        \\n        # 8. Results analysis and visualization\\n        print(\\"Step 8: Results analysis and visualization\\")\\n        self.analyze_and_visualize_results(photosynthesis_results, config[\'output_dir\'])\\n        \\n        return photosynthesis_results\\n    \\n    def analyze_and_visualize_results(self, results, output_dir):\\n        \\"\\"\\"Analyze and visualize results\\"\\"\\"\\n        import matplotlib.pyplot as plt\\n        from mpl_toolkits.mplot3d import Axes3D\\n        \\n        # Create output directory\\n        os.makedirs(output_dir, exist_ok=True)\\n        \\n        # 1. Photosynthesis rate distribution plot\\n        self.plot_photosynthesis_distribution(results, output_dir)\\n        \\n        # 2. Light distribution visualization\\n        self.plot_light_distribution(results, output_dir)\\n        \\n        # 3. Statistical analysis\\n        self.generate_statistical_report(results, output_dir)\\n        \\n        # 4. 3D visualization\\n        self.create_3d_visualization(results, output_dir)\\n    \\n    def generate_statistical_report(self, results, output_dir):\\n        \\"\\"\\"Generate statistical report\\"\\"\\"\\n        report = {\\n            \'total_canopy_photosynthesis\': results[\'total_canopy_photosynthesis\'],\\n            \'average_rate\': results[\'average_rate\'],\\n            \'num_active_voxels\': len(results[\'voxel_results\']),\\n            \'statistics\': {}\\n        }\\n        \\n        if results[\'voxel_results\']:\\n            rates = [r[\'photosynthesis_rate\'] for r in results[\'voxel_results\']]\\n            light_intensities = [r[\'light_intensity\'] for r in results[\'voxel_results\']]\\n            \\n            report[\'statistics\'] = {\\n                \'photosynthesis_rate\': {\\n                    \'mean\': np.mean(rates),\\n                    \'std\': np.std(rates),\\n                    \'min\': np.min(rates),\\n                    \'max\': np.max(rates),\\n                    \'median\': np.median(rates)\\n                },\\n                \'light_intensity\': {\\n                    \'mean\': np.mean(light_intensities),\\n                    \'std\': np.std(light_intensities),\\n                    \'min\': np.min(light_intensities),\\n                    \'max\': np.max(light_intensities),\\n                    \'median\': np.median(light_intensities)\\n                }\\n            }\\n        \\n        # Save report\\n        with open(os.path.join(output_dir, \'simulation_report.json\'), \'w\') as f:\\n            json.dump(report, f, indent=2)\\n        \\n        # Generate text report\\n        with open(os.path.join(output_dir, \'simulation_report.txt\'), \'w\') as f:\\n            f.write(\\"Canopy Photosynthesis Simulation Report\\\\n\\")\\n            f.write(\\"=\\" * 40 + \\"\\\\n\\\\n\\")\\n            f.write(f\\"Total Canopy Photosynthesis: {report[\'total_canopy_photosynthesis\']:.2f} \u03bcmol s\u207b\xb9\\\\n\\")\\n            f.write(f\\"Average Rate: {report[\'average_rate\']:.2f} \u03bcmol m\u207b\xb2 s\u207b\xb9\\\\n\\")\\n            f.write(f\\"Number of Active Voxels: {report[\'num_active_voxels\']}\\\\n\\\\n\\")\\n            \\n            if \'photosynthesis_rate\' in report[\'statistics\']:\\n                stats = report[\'statistics\'][\'photosynthesis_rate\']\\n                f.write(\\"Photosynthesis Rate Statistics:\\\\n\\")\\n                f.write(f\\"  Mean: {stats[\'mean\']:.2f} \u03bcmol m\u207b\xb2 s\u207b\xb9\\\\n\\")\\n                f.write(f\\"  Std:  {stats[\'std\']:.2f} \u03bcmol m\u207b\xb2 s\u207b\xb9\\\\n\\")\\n                f.write(f\\"  Min:  {stats[\'min\']:.2f} \u03bcmol m\u207b\xb2 s\u207b\xb9\\\\n\\")\\n                f.write(f\\"  Max:  {stats[\'max\']:.2f} \u03bcmol m\u207b\xb2 s\u207b\xb9\\\\n\\")\\n\\n# Usage example\\ndef main():\\n    # Configuration parameters\\n    config = {\\n        \'use_existing_images\': True,\\n        \'image_directory\': \'./plant_images\',\\n        \'reconstruction_method\': \'sfm\',  # \'sfm\' or \'3dgs\'\\n        \'canopy_config\': {\\n            \'row_spacing\': 0.75,\\n            \'plant_spacing\': 0.25,\\n            \'num_rows\': 5,\\n            \'plants_per_row\': 10\\n        },\\n        \'light_config\': {\\n            \'sun_angles\': [(60, 180), (45, 180), (30, 180)],  # (elevation, azimuth)\\n            \'num_rays\': 100000\\n        },\\n        \'environmental_config\': {\\n            \'temperature\': 25.0,\\n            \'co2_concentration\': 400.0,\\n            \'humidity\': 0.6\\n        },\\n        \'output_dir\': \'./simulation_results\'\\n    }\\n    \\n    # Run simulation\\n    simulator = CanopyPhotosynthesisSimulator()\\n    results = simulator.run_complete_simulation(config)\\n    \\n    print(f\\"Simulation completed. Total canopy photosynthesis: {results[\'total_canopy_photosynthesis\']:.2f} \u03bcmol s\u207b\xb9\\")\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\n## Conclusion\\n\\nThis comprehensive tutorial presents a complete workflow for canopy photosynthesis modeling based on 3D reconstruction, including:\\n\\n1. **Multi-view Image Capture**: Spherical capture strategy and quality control\\n2. **3D Reconstruction**: Both SfM and 3D Gaussian Splatting methods\\n3. **Mesh Processing**: Point cloud to mesh conversion and organ segmentation\\n4. **Canopy Construction**: Plant replication with random perturbation\\n5. **Ray Tracing**: Efficient light distribution simulation algorithms\\n6. **Photosynthesis Calculation**: Farquhar model-based photosynthesis rate computation\\n7. **Results Analysis**: Statistical analysis and visualization\\n\\nThis system provides powerful tools for precision agriculture, plant breeding, and ecological research, helping researchers understand canopy photosynthetic characteristics and optimize cultivation management strategies.\\n\\n---\\n\\n*Last updated: September 2025*"},{"id":"pytorch-ml-dl-tutorial","metadata":{"permalink":"/blog/pytorch-ml-dl-tutorial","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-09-20-pytorch-ml-dl-tutorial.md","source":"@site/blog/2022-09-20-pytorch-ml-dl-tutorial.md","title":"Complete PyTorch Tutorial for Machine Learning and Deep Learning","description":"Introduction","date":"2022-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"PyTorch","permalink":"/blog/tags/pytorch","description":"PyTorch deep learning framework"},{"inline":false,"label":"Machine Learning","permalink":"/blog/tags/machine-learning","description":"Machine learning techniques and applications"},{"inline":false,"label":"Deep Learning","permalink":"/blog/tags/deep-learning","description":"Deep learning techniques and neural networks"},{"inline":false,"label":"Neural Networks","permalink":"/blog/tags/neural-networks","description":"Neural network architectures and implementations"},{"inline":false,"label":"Tutorial","permalink":"/blog/tags/tutorial","description":"Tutorial tag description"}],"readingTime":25.58,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"pytorch-ml-dl-tutorial","title":"Complete PyTorch Tutorial for Machine Learning and Deep Learning","authors":["liangchao"],"tags":["pytorch","machine learning","deep learning","neural networks","tutorial"]},"unlisted":false,"prevItem":{"title":"Canopy Photosynthesis Modeling Tutorial Based on 3D Reconstruction","permalink":"/blog/canopy-photosynthesis-modeling-en"},"nextItem":{"title":"GitHub Beginner Guide","permalink":"/blog/gitHub-beginner-guide"}},"content":"\x3c!-- truncate --\x3e\\n\\n# Complete PyTorch Tutorial for Machine Learning and Deep Learning\\n\\n## Introduction\\n\\nPyTorch is one of the most popular deep learning frameworks, known for its dynamic computation graphs, intuitive API, and strong community support. This comprehensive tutorial covers everything from basic tensor operations to advanced deep learning architectures, providing practical examples and best practices for both machine learning and deep learning applications.\\n\\n## Table of Contents\\n\\n1. [PyTorch Fundamentals](#pytorch-fundamentals)\\n2. [Data Handling and Preprocessing](#data-handling-and-preprocessing)\\n3. [Building Neural Networks](#building-neural-networks)\\n4. [Training and Optimization](#training-and-optimization)\\n5. [Computer Vision with PyTorch](#computer-vision-with-pytorch)\\n6. [Natural Language Processing](#natural-language-processing)\\n7. [Advanced Topics](#advanced-topics)\\n8. [Production Deployment](#production-deployment)\\n\\n## PyTorch Fundamentals\\n\\n### Installation and Setup\\n\\n```bash\\n# Install PyTorch with CUDA support\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n\\n# For CPU-only installation\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\\n\\n# Additional packages\\npip install numpy matplotlib scikit-learn pandas seaborn jupyter\\n```\\n\\n### Basic Tensor Operations\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Check PyTorch version and CUDA availability\\nprint(f\\"PyTorch version: {torch.__version__}\\")\\nprint(f\\"CUDA available: {torch.cuda.is_available()}\\")\\nprint(f\\"CUDA version: {torch.version.cuda}\\")\\n\\n# Creating tensors\\ndef tensor_basics():\\n    # Different ways to create tensors\\n    x1 = torch.tensor([1, 2, 3, 4, 5])\\n    x2 = torch.zeros(3, 4)\\n    x3 = torch.ones(2, 3)\\n    x4 = torch.randn(2, 3)  # Random normal distribution\\n    x5 = torch.arange(0, 10, 2)  # Range tensor\\n    \\n    print(\\"Basic tensor creation:\\")\\n    print(f\\"x1: {x1}\\")\\n    print(f\\"x2 shape: {x2.shape}\\")\\n    print(f\\"x4: {x4}\\")\\n    \\n    # Tensor properties\\n    print(f\\"\\\\nTensor properties:\\")\\n    print(f\\"Data type: {x4.dtype}\\")\\n    print(f\\"Device: {x4.device}\\")\\n    print(f\\"Shape: {x4.shape}\\")\\n    print(f\\"Number of dimensions: {x4.ndim}\\")\\n    \\n    # Moving tensors to GPU\\n    if torch.cuda.is_available():\\n        x4_gpu = x4.cuda()\\n        print(f\\"GPU tensor device: {x4_gpu.device}\\")\\n    \\n    return x1, x2, x3, x4, x5\\n\\n# Tensor operations\\ndef tensor_operations():\\n    # Basic arithmetic operations\\n    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\\n    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\\n    \\n    # Element-wise operations\\n    add_result = a + b\\n    mul_result = a * b\\n    div_result = a / b\\n    \\n    # Matrix operations\\n    matmul_result = torch.matmul(a, b)\\n    transpose_result = a.t()\\n    \\n    print(\\"Tensor operations:\\")\\n    print(f\\"Addition: \\\\n{add_result}\\")\\n    print(f\\"Matrix multiplication: \\\\n{matmul_result}\\")\\n    print(f\\"Transpose: \\\\n{transpose_result}\\")\\n    \\n    # Reshaping and indexing\\n    x = torch.randn(4, 6)\\n    x_reshaped = x.view(2, 12)  # Reshape\\n    x_slice = x[:2, :3]  # Slicing\\n    \\n    print(f\\"\\\\nOriginal shape: {x.shape}\\")\\n    print(f\\"Reshaped: {x_reshaped.shape}\\")\\n    print(f\\"Sliced: {x_slice.shape}\\")\\n    \\n    return a, b, add_result, matmul_result\\n\\n# Automatic differentiation\\ndef autograd_example():\\n    # Enable gradient computation\\n    x = torch.tensor(2.0, requires_grad=True)\\n    y = torch.tensor(3.0, requires_grad=True)\\n    \\n    # Forward pass\\n    z = x**2 + y**3\\n    \\n    # Backward pass\\n    z.backward()\\n    \\n    print(\\"Automatic differentiation:\\")\\n    print(f\\"x.grad: {x.grad}\\")  # dz/dx = 2x = 4\\n    print(f\\"y.grad: {y.grad}\\")  # dz/dy = 3y^2 = 27\\n    \\n    # More complex example\\n    x = torch.randn(3, requires_grad=True)\\n    y = x * 2\\n    while y.data.norm() < 1000:\\n        y = y * 2\\n    \\n    print(f\\"\\\\nFinal y: {y}\\")\\n    \\n    # Compute gradients\\n    v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\\n    y.backward(v)\\n    print(f\\"x.grad: {x.grad}\\")\\n\\n# Run basic examples\\ntensor_basics()\\ntensor_operations()\\nautograd_example()\\n```\\n\\n## Data Handling and Preprocessing\\n\\n### Dataset and DataLoader\\n\\n```python\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms, datasets\\nimport os\\nfrom PIL import Image\\n\\n# Custom Dataset class\\nclass CustomDataset(Dataset):\\n    def __init__(self, data, targets, transform=None):\\n        self.data = data\\n        self.targets = targets\\n        self.transform = transform\\n    \\n    def __len__(self):\\n        return len(self.data)\\n    \\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        target = self.targets[idx]\\n        \\n        if self.transform:\\n            sample = self.transform(sample)\\n        \\n        return sample, target\\n\\n# Image dataset example\\nclass ImageDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.images = []\\n        self.labels = []\\n        \\n        # Assuming directory structure: root_dir/class_name/image.jpg\\n        for class_idx, class_name in enumerate(os.listdir(root_dir)):\\n            class_path = os.path.join(root_dir, class_name)\\n            if os.path.isdir(class_path):\\n                for img_name in os.listdir(class_path):\\n                    if img_name.endswith((\'.png\', \'.jpg\', \'.jpeg\')):\\n                        self.images.append(os.path.join(class_path, img_name))\\n                        self.labels.append(class_idx)\\n    \\n    def __len__(self):\\n        return len(self.images)\\n    \\n    def __getitem__(self, idx):\\n        img_path = self.images[idx]\\n        image = Image.open(img_path).convert(\'RGB\')\\n        label = self.labels[idx]\\n        \\n        if self.transform:\\n            image = self.transform(image)\\n        \\n        return image, label\\n\\n# Data preprocessing and augmentation\\ndef create_data_loaders():\\n    # Define transforms\\n    train_transform = transforms.Compose([\\n        transforms.RandomResizedCrop(224),\\n        transforms.RandomHorizontalFlip(p=0.5),\\n        transforms.RandomRotation(degrees=15),\\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n    ])\\n    \\n    val_transform = transforms.Compose([\\n        transforms.Resize(256),\\n        transforms.CenterCrop(224),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n    ])\\n    \\n    # Create datasets\\n    # For demonstration, using CIFAR-10\\n    train_dataset = datasets.CIFAR10(\\n        root=\'./data\', \\n        train=True, \\n        download=True, \\n        transform=train_transform\\n    )\\n    \\n    val_dataset = datasets.CIFAR10(\\n        root=\'./data\', \\n        train=False, \\n        download=True, \\n        transform=val_transform\\n    )\\n    \\n    # Create data loaders\\n    train_loader = DataLoader(\\n        train_dataset, \\n        batch_size=32, \\n        shuffle=True, \\n        num_workers=4,\\n        pin_memory=True\\n    )\\n    \\n    val_loader = DataLoader(\\n        val_dataset, \\n        batch_size=32, \\n        shuffle=False, \\n        num_workers=4,\\n        pin_memory=True\\n    )\\n    \\n    return train_loader, val_loader\\n\\n# Data exploration\\ndef explore_data(data_loader):\\n    # Get a batch of data\\n    data_iter = iter(data_loader)\\n    images, labels = next(data_iter)\\n    \\n    print(f\\"Batch shape: {images.shape}\\")\\n    print(f\\"Labels shape: {labels.shape}\\")\\n    print(f\\"Data type: {images.dtype}\\")\\n    print(f\\"Label range: {labels.min()} to {labels.max()}\\")\\n    \\n    # Visualize some samples\\n    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\\n    for i in range(8):\\n        row, col = i // 4, i % 4\\n        img = images[i].permute(1, 2, 0)  # Change from CHW to HWC\\n        # Denormalize for visualization\\n        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\\n        img = torch.clamp(img, 0, 1)\\n        \\n        axes[row, col].imshow(img)\\n        axes[row, col].set_title(f\'Label: {labels[i].item()}\')\\n        axes[row, col].axis(\'off\')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n# Create and explore data\\ntrain_loader, val_loader = create_data_loaders()\\nexplore_data(train_loader)\\n```\\n\\n## Building Neural Networks\\n\\n### Basic Neural Network Components\\n\\n```python\\n# Simple feedforward neural network\\nclass SimpleNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_classes):\\n        super(SimpleNN, self).__init__()\\n        self.fc1 = nn.Linear(input_size, hidden_size)\\n        self.relu = nn.ReLU()\\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\\n        self.fc3 = nn.Linear(hidden_size, num_classes)\\n        self.dropout = nn.Dropout(0.2)\\n    \\n    def forward(self, x):\\n        x = x.view(x.size(0), -1)  # Flatten\\n        x = self.fc1(x)\\n        x = self.relu(x)\\n        x = self.dropout(x)\\n        x = self.fc2(x)\\n        x = self.relu(x)\\n        x = self.dropout(x)\\n        x = self.fc3(x)\\n        return x\\n\\n# Convolutional Neural Network\\nclass CNN(nn.Module):\\n    def __init__(self, num_classes=10):\\n        super(CNN, self).__init__()\\n        \\n        # Convolutional layers\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\\n        \\n        # Pooling layer\\n        self.pool = nn.MaxPool2d(2, 2)\\n        \\n        # Batch normalization\\n        self.bn1 = nn.BatchNorm2d(32)\\n        self.bn2 = nn.BatchNorm2d(64)\\n        self.bn3 = nn.BatchNorm2d(128)\\n        \\n        # Fully connected layers\\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\\n        self.fc2 = nn.Linear(512, num_classes)\\n        \\n        # Dropout\\n        self.dropout = nn.Dropout(0.5)\\n        \\n    def forward(self, x):\\n        # First conv block\\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\\n        \\n        # Second conv block\\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\\n        \\n        # Third conv block\\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\\n        \\n        # Flatten\\n        x = x.view(-1, 128 * 4 * 4)\\n        \\n        # Fully connected layers\\n        x = F.relu(self.fc1(x))\\n        x = self.dropout(x)\\n        x = self.fc2(x)\\n        \\n        return x\\n\\n# ResNet-like block\\nclass ResidualBlock(nn.Module):\\n    def __init__(self, in_channels, out_channels, stride=1):\\n        super(ResidualBlock, self).__init__()\\n        \\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \\n                              stride=stride, padding=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(out_channels)\\n        \\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\\n                              stride=1, padding=1, bias=False)\\n        self.bn2 = nn.BatchNorm2d(out_channels)\\n        \\n        # Shortcut connection\\n        self.shortcut = nn.Sequential()\\n        if stride != 1 or in_channels != out_channels:\\n            self.shortcut = nn.Sequential(\\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, \\n                         stride=stride, bias=False),\\n                nn.BatchNorm2d(out_channels)\\n            )\\n    \\n    def forward(self, x):\\n        residual = x\\n        \\n        out = F.relu(self.bn1(self.conv1(x)))\\n        out = self.bn2(self.conv2(out))\\n        \\n        out += self.shortcut(residual)\\n        out = F.relu(out)\\n        \\n        return out\\n\\n# Custom ResNet\\nclass CustomResNet(nn.Module):\\n    def __init__(self, num_classes=10):\\n        super(CustomResNet, self).__init__()\\n        \\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(64)\\n        \\n        # Residual layers\\n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\\n        \\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\\n        self.fc = nn.Linear(256, num_classes)\\n    \\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\\n        layers = []\\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\\n        \\n        for _ in range(1, num_blocks):\\n            layers.append(ResidualBlock(out_channels, out_channels))\\n        \\n        return nn.Sequential(*layers)\\n    \\n    def forward(self, x):\\n        x = F.relu(self.bn1(self.conv1(x)))\\n        \\n        x = self.layer1(x)\\n        x = self.layer2(x)\\n        x = self.layer3(x)\\n        \\n        x = self.avg_pool(x)\\n        x = x.view(x.size(0), -1)\\n        x = self.fc(x)\\n        \\n        return x\\n\\n# Model initialization and summary\\ndef initialize_model(model_type=\'cnn\', num_classes=10):\\n    if model_type == \'simple\':\\n        model = SimpleNN(input_size=32*32*3, hidden_size=512, num_classes=num_classes)\\n    elif model_type == \'cnn\':\\n        model = CNN(num_classes=num_classes)\\n    elif model_type == \'resnet\':\\n        model = CustomResNet(num_classes=num_classes)\\n    else:\\n        raise ValueError(\\"Unknown model type\\")\\n    \\n    # Initialize weights\\n    def init_weights(m):\\n        if isinstance(m, nn.Linear):\\n            torch.nn.init.xavier_uniform_(m.weight)\\n            m.bias.data.fill_(0.01)\\n        elif isinstance(m, nn.Conv2d):\\n            torch.nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\\n    \\n    model.apply(init_weights)\\n    \\n    # Model summary\\n    total_params = sum(p.numel() for p in model.parameters())\\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n    \\n    print(f\\"Model: {model_type}\\")\\n    print(f\\"Total parameters: {total_params:,}\\")\\n    print(f\\"Trainable parameters: {trainable_params:,}\\")\\n    \\n    return model\\n\\n# Test model creation\\nmodel = initialize_model(\'resnet\')\\nprint(model)\\n```\\n\\n## Training and Optimization\\n\\n### Training Loop Implementation\\n\\n```python\\nimport time\\nfrom tqdm import tqdm\\nimport torch.nn.functional as F\\n\\nclass Trainer:\\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\\n        self.model = model\\n        self.train_loader = train_loader\\n        self.val_loader = val_loader\\n        self.criterion = criterion\\n        self.optimizer = optimizer\\n        self.device = device\\n        \\n        # Move model to device\\n        self.model.to(device)\\n        \\n        # Training history\\n        self.train_losses = []\\n        self.train_accuracies = []\\n        self.val_losses = []\\n        self.val_accuracies = []\\n    \\n    def train_epoch(self):\\n        self.model.train()\\n        running_loss = 0.0\\n        correct = 0\\n        total = 0\\n        \\n        pbar = tqdm(self.train_loader, desc=\'Training\')\\n        for batch_idx, (data, target) in enumerate(pbar):\\n            data, target = data.to(self.device), target.to(self.device)\\n            \\n            # Zero gradients\\n            self.optimizer.zero_grad()\\n            \\n            # Forward pass\\n            output = self.model(data)\\n            loss = self.criterion(output, target)\\n            \\n            # Backward pass\\n            loss.backward()\\n            self.optimizer.step()\\n            \\n            # Statistics\\n            running_loss += loss.item()\\n            _, predicted = output.max(1)\\n            total += target.size(0)\\n            correct += predicted.eq(target).sum().item()\\n            \\n            # Update progress bar\\n            pbar.set_postfix({\\n                \'Loss\': f\'{running_loss/(batch_idx+1):.4f}\',\\n                \'Acc\': f\'{100.*correct/total:.2f}%\'\\n            })\\n        \\n        epoch_loss = running_loss / len(self.train_loader)\\n        epoch_acc = 100. * correct / total\\n        \\n        return epoch_loss, epoch_acc\\n    \\n    def validate_epoch(self):\\n        self.model.eval()\\n        running_loss = 0.0\\n        correct = 0\\n        total = 0\\n        \\n        with torch.no_grad():\\n            pbar = tqdm(self.val_loader, desc=\'Validation\')\\n            for data, target in pbar:\\n                data, target = data.to(self.device), target.to(self.device)\\n                \\n                output = self.model(data)\\n                loss = self.criterion(output, target)\\n                \\n                running_loss += loss.item()\\n                _, predicted = output.max(1)\\n                total += target.size(0)\\n                correct += predicted.eq(target).sum().item()\\n                \\n                pbar.set_postfix({\\n                    \'Loss\': f\'{running_loss/(len(pbar.iterable)):.4f}\',\\n                    \'Acc\': f\'{100.*correct/total:.2f}%\'\\n                })\\n        \\n        epoch_loss = running_loss / len(self.val_loader)\\n        epoch_acc = 100. * correct / total\\n        \\n        return epoch_loss, epoch_acc\\n    \\n    def train(self, num_epochs, scheduler=None, early_stopping_patience=None):\\n        best_val_acc = 0.0\\n        patience_counter = 0\\n        \\n        for epoch in range(num_epochs):\\n            print(f\'\\\\nEpoch {epoch+1}/{num_epochs}\')\\n            print(\'-\' * 50)\\n            \\n            # Training phase\\n            train_loss, train_acc = self.train_epoch()\\n            \\n            # Validation phase\\n            val_loss, val_acc = self.validate_epoch()\\n            \\n            # Update learning rate\\n            if scheduler:\\n                scheduler.step(val_loss)\\n            \\n            # Save metrics\\n            self.train_losses.append(train_loss)\\n            self.train_accuracies.append(train_acc)\\n            self.val_losses.append(val_loss)\\n            self.val_accuracies.append(val_acc)\\n            \\n            # Print epoch results\\n            print(f\'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\')\\n            print(f\'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\')\\n            \\n            # Early stopping\\n            if early_stopping_patience:\\n                if val_acc > best_val_acc:\\n                    best_val_acc = val_acc\\n                    patience_counter = 0\\n                    # Save best model\\n                    torch.save(self.model.state_dict(), \'best_model.pth\')\\n                else:\\n                    patience_counter += 1\\n                    if patience_counter >= early_stopping_patience:\\n                        print(f\'Early stopping triggered after {epoch+1} epochs\')\\n                        break\\n        \\n        print(f\'\\\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%\')\\n    \\n    def plot_training_history(self):\\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\\n        \\n        # Plot losses\\n        ax1.plot(self.train_losses, label=\'Train Loss\')\\n        ax1.plot(self.val_losses, label=\'Validation Loss\')\\n        ax1.set_title(\'Training and Validation Loss\')\\n        ax1.set_xlabel(\'Epoch\')\\n        ax1.set_ylabel(\'Loss\')\\n        ax1.legend()\\n        ax1.grid(True)\\n        \\n        # Plot accuracies\\n        ax2.plot(self.train_accuracies, label=\'Train Accuracy\')\\n        ax2.plot(self.val_accuracies, label=\'Validation Accuracy\')\\n        ax2.set_title(\'Training and Validation Accuracy\')\\n        ax2.set_xlabel(\'Epoch\')\\n        ax2.set_ylabel(\'Accuracy (%)\')\\n        ax2.legend()\\n        ax2.grid(True)\\n        \\n        plt.tight_layout()\\n        plt.show()\\n\\n# Advanced optimization techniques\\ndef setup_training(model, train_loader, val_loader):\\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\\n    \\n    # Loss function\\n    criterion = nn.CrossEntropyLoss()\\n    \\n    # Optimizer with different options\\n    optimizer = optim.AdamW(\\n        model.parameters(), \\n        lr=0.001, \\n        weight_decay=0.01,\\n        betas=(0.9, 0.999)\\n    )\\n    \\n    # Learning rate scheduler\\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n        optimizer, \\n        mode=\'min\', \\n        factor=0.5, \\n        patience=5, \\n        verbose=True\\n    )\\n    \\n    # Alternative schedulers\\n    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\\n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\\n    \\n    # Create trainer\\n    trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\\n    \\n    return trainer, scheduler\\n\\n# Mixed precision training\\nclass MixedPrecisionTrainer(Trainer):\\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\\n        super().__init__(model, train_loader, val_loader, criterion, optimizer, device)\\n        self.scaler = torch.cuda.amp.GradScaler()\\n    \\n    def train_epoch(self):\\n        self.model.train()\\n        running_loss = 0.0\\n        correct = 0\\n        total = 0\\n        \\n        pbar = tqdm(self.train_loader, desc=\'Training (Mixed Precision)\')\\n        for batch_idx, (data, target) in enumerate(pbar):\\n            data, target = data.to(self.device), target.to(self.device)\\n            \\n            self.optimizer.zero_grad()\\n            \\n            # Mixed precision forward pass\\n            with torch.cuda.amp.autocast():\\n                output = self.model(data)\\n                loss = self.criterion(output, target)\\n            \\n            # Mixed precision backward pass\\n            self.scaler.scale(loss).backward()\\n            self.scaler.step(self.optimizer)\\n            self.scaler.update()\\n            \\n            # Statistics\\n            running_loss += loss.item()\\n            _, predicted = output.max(1)\\n            total += target.size(0)\\n            correct += predicted.eq(target).sum().item()\\n            \\n            pbar.set_postfix({\\n                \'Loss\': f\'{running_loss/(batch_idx+1):.4f}\',\\n                \'Acc\': f\'{100.*correct/total:.2f}%\'\\n            })\\n        \\n        epoch_loss = running_loss / len(self.train_loader)\\n        epoch_acc = 100. * correct / total\\n        \\n        return epoch_loss, epoch_acc\\n\\n# Example training setup and execution\\nmodel = initialize_model(\'resnet\', num_classes=10)\\ntrainer, scheduler = setup_training(model, train_loader, val_loader)\\n\\n# Train the model\\ntrainer.train(num_epochs=50, scheduler=scheduler, early_stopping_patience=10)\\n\\n# Plot training history\\ntrainer.plot_training_history()\\n```\\n\\n## Computer Vision with PyTorch\\n\\n### Transfer Learning and Fine-tuning\\n\\n```python\\nimport torchvision.models as models\\nfrom torchvision.models import ResNet50_Weights\\n\\n# Transfer learning with pre-trained models\\nclass TransferLearningModel(nn.Module):\\n    def __init__(self, num_classes, model_name=\'resnet50\', pretrained=True):\\n        super(TransferLearningModel, self).__init__()\\n        \\n        if model_name == \'resnet50\':\\n            self.backbone = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\\n            num_features = self.backbone.fc.in_features\\n            self.backbone.fc = nn.Linear(num_features, num_classes)\\n            \\n        elif model_name == \'efficientnet\':\\n            self.backbone = models.efficientnet_b0(pretrained=pretrained)\\n            num_features = self.backbone.classifier[1].in_features\\n            self.backbone.classifier[1] = nn.Linear(num_features, num_classes)\\n            \\n        elif model_name == \'vit\':\\n            self.backbone = models.vit_b_16(pretrained=pretrained)\\n            num_features = self.backbone.heads.head.in_features\\n            self.backbone.heads.head = nn.Linear(num_features, num_classes)\\n    \\n    def forward(self, x):\\n        return self.backbone(x)\\n    \\n    def freeze_backbone(self):\\n        \\"\\"\\"Freeze backbone parameters for feature extraction\\"\\"\\"\\n        for param in self.backbone.parameters():\\n            param.requires_grad = False\\n        \\n        # Unfreeze classifier\\n        if hasattr(self.backbone, \'fc\'):\\n            for param in self.backbone.fc.parameters():\\n                param.requires_grad = True\\n        elif hasattr(self.backbone, \'classifier\'):\\n            for param in self.backbone.classifier.parameters():\\n                param.requires_grad = True\\n    \\n    def unfreeze_backbone(self):\\n        \\"\\"\\"Unfreeze all parameters for fine-tuning\\"\\"\\"\\n        for param in self.backbone.parameters():\\n            param.requires_grad = True\\n\\n# Object detection with YOLO-style architecture\\nclass SimpleYOLO(nn.Module):\\n    def __init__(self, num_classes, num_anchors=3):\\n        super(SimpleYOLO, self).__init__()\\n        self.num_classes = num_classes\\n        self.num_anchors = num_anchors\\n        \\n        # Backbone\\n        self.backbone = models.resnet18(pretrained=True)\\n        self.backbone.fc = nn.Identity()  # Remove final FC layer\\n        \\n        # Detection head\\n        self.conv1 = nn.Conv2d(512, 256, 3, padding=1)\\n        self.conv2 = nn.Conv2d(256, 128, 3, padding=1)\\n        self.conv3 = nn.Conv2d(128, num_anchors * (5 + num_classes), 1)\\n        \\n    def forward(self, x):\\n        # Extract features\\n        x = self.backbone.conv1(x)\\n        x = self.backbone.bn1(x)\\n        x = self.backbone.relu(x)\\n        x = self.backbone.maxpool(x)\\n        \\n        x = self.backbone.layer1(x)\\n        x = self.backbone.layer2(x)\\n        x = self.backbone.layer3(x)\\n        x = self.backbone.layer4(x)\\n        \\n        # Detection head\\n        x = F.relu(self.conv1(x))\\n        x = F.relu(self.conv2(x))\\n        x = self.conv3(x)\\n        \\n        return x\\n\\n# Semantic segmentation with U-Net\\nclass UNet(nn.Module):\\n    def __init__(self, in_channels=3, num_classes=1):\\n        super(UNet, self).__init__()\\n        \\n        # Encoder\\n        self.enc1 = self.conv_block(in_channels, 64)\\n        self.enc2 = self.conv_block(64, 128)\\n        self.enc3 = self.conv_block(128, 256)\\n        self.enc4 = self.conv_block(256, 512)\\n        \\n        # Bottleneck\\n        self.bottleneck = self.conv_block(512, 1024)\\n        \\n        # Decoder\\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\\n        self.dec4 = self.conv_block(1024, 512)\\n        \\n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\\n        self.dec3 = self.conv_block(512, 256)\\n        \\n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\\n        self.dec2 = self.conv_block(256, 128)\\n        \\n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\\n        self.dec1 = self.conv_block(128, 64)\\n        \\n        # Final layer\\n        self.final = nn.Conv2d(64, num_classes, 1)\\n        \\n        self.pool = nn.MaxPool2d(2)\\n    \\n    def conv_block(self, in_channels, out_channels):\\n        return nn.Sequential(\\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\\n            nn.BatchNorm2d(out_channels),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\\n            nn.BatchNorm2d(out_channels),\\n            nn.ReLU(inplace=True)\\n        )\\n    \\n    def forward(self, x):\\n        # Encoder\\n        enc1 = self.enc1(x)\\n        enc2 = self.enc2(self.pool(enc1))\\n        enc3 = self.enc3(self.pool(enc2))\\n        enc4 = self.enc4(self.pool(enc3))\\n        \\n        # Bottleneck\\n        bottleneck = self.bottleneck(self.pool(enc4))\\n        \\n        # Decoder\\n        dec4 = self.upconv4(bottleneck)\\n        dec4 = torch.cat((dec4, enc4), dim=1)\\n        dec4 = self.dec4(dec4)\\n        \\n        dec3 = self.upconv3(dec4)\\n        dec3 = torch.cat((dec3, enc3), dim=1)\\n        dec3 = self.dec3(dec3)\\n        \\n        dec2 = self.upconv2(dec3)\\n        dec2 = torch.cat((dec2, enc2), dim=1)\\n        dec2 = self.dec2(dec2)\\n        \\n        dec1 = self.upconv1(dec2)\\n        dec1 = torch.cat((dec1, enc1), dim=1)\\n        dec1 = self.dec1(dec1)\\n        \\n        return torch.sigmoid(self.final(dec1))\\n\\n# Image augmentation and preprocessing\\nclass AdvancedAugmentation:\\n    def __init__(self):\\n        self.train_transform = transforms.Compose([\\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\\n            transforms.RandomHorizontalFlip(p=0.5),\\n            transforms.RandomVerticalFlip(p=0.2),\\n            transforms.RandomRotation(degrees=15),\\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\\n            transforms.RandomGrayscale(p=0.1),\\n            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n            transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3))\\n        ])\\n        \\n        self.val_transform = transforms.Compose([\\n            transforms.Resize(256),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n        ])\\n\\n# Example: Fine-tuning a pre-trained model\\ndef fine_tune_model():\\n    # Create transfer learning model\\n    model = TransferLearningModel(num_classes=10, model_name=\'resnet50\', pretrained=True)\\n    \\n    # Phase 1: Feature extraction (freeze backbone)\\n    model.freeze_backbone()\\n    \\n    # Setup optimizer for feature extraction\\n    optimizer = optim.Adam(model.backbone.fc.parameters(), lr=0.001)\\n    \\n    print(\\"Phase 1: Feature extraction training\\")\\n    # Train for a few epochs...\\n    \\n    # Phase 2: Fine-tuning (unfreeze backbone)\\n    model.unfreeze_backbone()\\n    \\n    # Setup optimizer for fine-tuning with lower learning rate\\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\\n    \\n    print(\\"Phase 2: Fine-tuning training\\")\\n    # Continue training...\\n    \\n    return model\\n\\n# Test computer vision models\\ntransfer_model = fine_tune_model()\\nunet_model = UNet(in_channels=3, num_classes=21)  # For Pascal VOC\\nyolo_model = SimpleYOLO(num_classes=80)  # For COCO\\n\\nprint(f\\"Transfer learning model parameters: {sum(p.numel() for p in transfer_model.parameters()):,}\\")\\nprint(f\\"U-Net model parameters: {sum(p.numel() for p in unet_model.parameters()):,}\\")\\nprint(f\\"YOLO model parameters: {sum(p.numel() for p in yolo_model.parameters()):,}\\")\\n```\\n\\n## Natural Language Processing\\n\\n### Text Processing and RNN/Transformer Models\\n\\n```python\\nimport torch.nn.utils.rnn as rnn_utils\\nfrom collections import Counter\\nimport re\\n\\n# Text preprocessing utilities\\nclass TextPreprocessor:\\n    def __init__(self, vocab_size=10000, min_freq=2):\\n        self.vocab_size = vocab_size\\n        self.min_freq = min_freq\\n        self.word2idx = {}\\n        self.idx2word = {}\\n        self.vocab = set()\\n    \\n    def build_vocab(self, texts):\\n        # Tokenize and count words\\n        word_counts = Counter()\\n        for text in texts:\\n            tokens = self.tokenize(text)\\n            word_counts.update(tokens)\\n        \\n        # Build vocabulary\\n        vocab_words = [word for word, count in word_counts.most_common(self.vocab_size-4) \\n                      if count >= self.min_freq]\\n        \\n        # Special tokens\\n        special_tokens = [\'<PAD>\', \'<UNK>\', \'<SOS>\', \'<EOS>\']\\n        self.vocab = set(special_tokens + vocab_words)\\n        \\n        # Create mappings\\n        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\\n        \\n        print(f\\"Vocabulary size: {len(self.vocab)}\\")\\n    \\n    def tokenize(self, text):\\n        # Simple tokenization\\n        text = text.lower()\\n        text = re.sub(r\'[^a-zA-Z0-9\\\\s]\', \'\', text)\\n        return text.split()\\n    \\n    def text_to_indices(self, text):\\n        tokens = self.tokenize(text)\\n        return [self.word2idx.get(token, self.word2idx[\'<UNK>\']) for token in tokens]\\n    \\n    def indices_to_text(self, indices):\\n        return \' \'.join([self.idx2word.get(idx, \'<UNK>\') for idx in indices])\\n\\n# LSTM-based language model\\nclass LSTMLanguageModel(nn.Module):\\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.2):\\n        super(LSTMLanguageModel, self).__init__()\\n        \\n        self.vocab_size = vocab_size\\n        self.embed_size = embed_size\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        \\n        # Layers\\n        self.embedding = nn.Embedding(vocab_size, embed_size)\\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \\n                           batch_first=True, dropout=dropout)\\n        self.dropout = nn.Dropout(dropout)\\n        self.fc = nn.Linear(hidden_size, vocab_size)\\n    \\n    def forward(self, x, hidden=None):\\n        # Embedding\\n        embedded = self.embedding(x)\\n        embedded = self.dropout(embedded)\\n        \\n        # LSTM\\n        lstm_out, hidden = self.lstm(embedded, hidden)\\n        lstm_out = self.dropout(lstm_out)\\n        \\n        # Output projection\\n        output = self.fc(lstm_out)\\n        \\n        return output, hidden\\n    \\n    def init_hidden(self, batch_size, device):\\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\\n        return (h0, c0)\\n\\n# Transformer-based model\\nclass TransformerModel(nn.Module):\\n    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_len, dropout=0.1):\\n        super(TransformerModel, self).__init__()\\n        \\n        self.d_model = d_model\\n        self.vocab_size = vocab_size\\n        self.max_seq_len = max_seq_len\\n        \\n        # Embeddings\\n        self.embedding = nn.Embedding(vocab_size, d_model)\\n        self.pos_encoding = self.create_positional_encoding(max_seq_len, d_model)\\n        \\n        # Transformer\\n        encoder_layer = nn.TransformerEncoderLayer(\\n            d_model=d_model,\\n            nhead=nhead,\\n            dim_feedforward=dim_feedforward,\\n            dropout=dropout,\\n            batch_first=True\\n        )\\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\\n        \\n        # Output layer\\n        self.fc = nn.Linear(d_model, vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n    \\n    def create_positional_encoding(self, max_seq_len, d_model):\\n        pe = torch.zeros(max_seq_len, d_model)\\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\\n        \\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \\n                           -(torch.log(torch.tensor(10000.0)) / d_model))\\n        \\n        pe[:, 0::2] = torch.sin(position * div_term)\\n        pe[:, 1::2] = torch.cos(position * div_term)\\n        \\n        return pe.unsqueeze(0)\\n    \\n    def forward(self, x, mask=None):\\n        seq_len = x.size(1)\\n        \\n        # Embedding + positional encoding\\n        x = self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\\n        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)\\n        x = self.dropout(x)\\n        \\n        # Transformer\\n        if mask is None:\\n            mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\\n        \\n        output = self.transformer(x, mask)\\n        output = self.fc(output)\\n        \\n        return output\\n    \\n    def generate_square_subsequent_mask(self, sz):\\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\\n        mask = mask.masked_fill(mask == 1, float(\'-inf\'))\\n        return mask\\n\\n# Attention mechanism\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self, d_model, num_heads):\\n        super(MultiHeadAttention, self).__init__()\\n        assert d_model % num_heads == 0\\n        \\n        self.d_model = d_model\\n        self.num_heads = num_heads\\n        self.d_k = d_model // num_heads\\n        \\n        self.W_q = nn.Linear(d_model, d_model)\\n        self.W_k = nn.Linear(d_model, d_model)\\n        self.W_v = nn.Linear(d_model, d_model)\\n        self.W_o = nn.Linear(d_model, d_model)\\n        \\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\\n        \\n        if mask is not None:\\n            scores = scores.masked_fill(mask == 0, -1e9)\\n        \\n        attention_weights = F.softmax(scores, dim=-1)\\n        output = torch.matmul(attention_weights, V)\\n        \\n        return output, attention_weights\\n    \\n    def forward(self, query, key, value, mask=None):\\n        batch_size = query.size(0)\\n        \\n        # Linear transformations\\n        Q = self.W_q(query)\\n        K = self.W_k(key)\\n        V = self.W_v(value)\\n        \\n        # Reshape for multi-head attention\\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\\n        \\n        # Apply attention\\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\\n        \\n        # Concatenate heads\\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\\n            batch_size, -1, self.d_model\\n        )\\n        \\n        # Final linear transformation\\n        output = self.W_o(attention_output)\\n        \\n        return output, attention_weights\\n\\n# Text classification model\\nclass TextClassifier(nn.Module):\\n    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers=2):\\n        super(TextClassifier, self).__init__()\\n        \\n        self.embedding = nn.Embedding(vocab_size, embed_size)\\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \\n                           batch_first=True, bidirectional=True)\\n        self.dropout = nn.Dropout(0.3)\\n        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\\n    \\n    def forward(self, x):\\n        # Embedding\\n        embedded = self.embedding(x)\\n        \\n        # LSTM\\n        lstm_out, (hidden, _) = self.lstm(embedded)\\n        \\n        # Use last hidden state from both directions\\n        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\\n        hidden = self.dropout(hidden)\\n        \\n        # Classification\\n        output = self.fc(hidden)\\n        \\n        return output\\n\\n# Example usage\\ndef train_nlp_model():\\n    # Sample data\\n    texts = [\\n        \\"This is a sample sentence for training.\\",\\n        \\"Natural language processing with PyTorch is powerful.\\",\\n        \\"Deep learning models can understand text patterns.\\"\\n    ]\\n    \\n    # Preprocess text\\n    preprocessor = TextPreprocessor(vocab_size=1000)\\n    preprocessor.build_vocab(texts)\\n    \\n    # Create models\\n    vocab_size = len(preprocessor.vocab)\\n    \\n    # LSTM Language Model\\n    lstm_model = LSTMLanguageModel(\\n        vocab_size=vocab_size,\\n        embed_size=128,\\n        hidden_size=256,\\n        num_layers=2\\n    )\\n    \\n    # Transformer Model\\n    transformer_model = TransformerModel(\\n        vocab_size=vocab_size,\\n        d_model=128,\\n        nhead=8,\\n        num_layers=6,\\n        dim_feedforward=512,\\n        max_seq_len=100\\n    )\\n    \\n    # Text Classifier\\n    classifier_model = TextClassifier(\\n        vocab_size=vocab_size,\\n        embed_size=128,\\n        hidden_size=256,\\n        num_classes=3\\n    )\\n    \\n    print(f\\"LSTM model parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\\")\\n    print(f\\"Transformer model parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\\")\\n    print(f\\"Classifier model parameters: {sum(p.numel() for p in classifier_model.parameters()):,}\\")\\n    \\n    return lstm_model, transformer_model, classifier_model\\n\\n# Test NLP models\\nlstm_model, transformer_model, classifier_model = train_nlp_model()\\n```\\n\\n## Advanced Topics\\n\\n### Custom Loss Functions and Metrics\\n\\n```python\\n# Custom loss functions\\nclass FocalLoss(nn.Module):\\n    def __init__(self, alpha=1, gamma=2, reduction=\'mean\'):\\n        super(FocalLoss, self).__init__()\\n        self.alpha = alpha\\n        self.gamma = gamma\\n        self.reduction = reduction\\n    \\n    def forward(self, inputs, targets):\\n        ce_loss = F.cross_entropy(inputs, targets, reduction=\'none\')\\n        pt = torch.exp(-ce_loss)\\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\\n        \\n        if self.reduction == \'mean\':\\n            return focal_loss.mean()\\n        elif self.reduction == \'sum\':\\n            return focal_loss.sum()\\n        else:\\n            return focal_loss\\n\\nclass DiceLoss(nn.Module):\\n    def __init__(self, smooth=1):\\n        super(DiceLoss, self).__init__()\\n        self.smooth = smooth\\n    \\n    def forward(self, inputs, targets):\\n        inputs = torch.sigmoid(inputs)\\n        \\n        # Flatten tensors\\n        inputs = inputs.view(-1)\\n        targets = targets.view(-1)\\n        \\n        intersection = (inputs * targets).sum()\\n        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\\n        \\n        return 1 - dice\\n\\nclass ContrastiveLoss(nn.Module):\\n    def __init__(self, margin=2.0):\\n        super(ContrastiveLoss, self).__init__()\\n        self.margin = margin\\n    \\n    def forward(self, output1, output2, label):\\n        euclidean_distance = F.pairwise_distance(output1, output2)\\n        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\\n                                    label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\\n        return loss_contrastive\\n\\n# Custom metrics\\nclass MetricsCalculator:\\n    def __init__(self, num_classes):\\n        self.num_classes = num_classes\\n        self.reset()\\n    \\n    def reset(self):\\n        self.confusion_matrix = torch.zeros(self.num_classes, self.num_classes)\\n        self.total_samples = 0\\n        self.correct_predictions = 0\\n    \\n    def update(self, predictions, targets):\\n        _, predicted = torch.max(predictions, 1)\\n        self.total_samples += targets.size(0)\\n        self.correct_predictions += (predicted == targets).sum().item()\\n        \\n        # Update confusion matrix\\n        for t, p in zip(targets.view(-1), predicted.view(-1)):\\n            self.confusion_matrix[t.long(), p.long()] += 1\\n    \\n    def accuracy(self):\\n        return self.correct_predictions / self.total_samples\\n    \\n    def precision(self, class_idx=None):\\n        if class_idx is not None:\\n            tp = self.confusion_matrix[class_idx, class_idx]\\n            fp = self.confusion_matrix[:, class_idx].sum() - tp\\n            return tp / (tp + fp) if (tp + fp) > 0 else 0\\n        else:\\n            precisions = []\\n            for i in range(self.num_classes):\\n                precisions.append(self.precision(i))\\n            return sum(precisions) / len(precisions)\\n    \\n    def recall(self, class_idx=None):\\n        if class_idx is not None:\\n            tp = self.confusion_matrix[class_idx, class_idx]\\n            fn = self.confusion_matrix[class_idx, :].sum() - tp\\n            return tp / (tp + fn) if (tp + fn) > 0 else 0\\n        else:\\n            recalls = []\\n            for i in range(self.num_classes):\\n                recalls.append(self.recall(i))\\n            return sum(recalls) / len(recalls)\\n    \\n    def f1_score(self, class_idx=None):\\n        if class_idx is not None:\\n            p = self.precision(class_idx)\\n            r = self.recall(class_idx)\\n            return 2 * p * r / (p + r) if (p + r) > 0 else 0\\n        else:\\n            f1_scores = []\\n            for i in range(self.num_classes):\\n                f1_scores.append(self.f1_score(i))\\n            return sum(f1_scores) / len(f1_scores)\\n\\n# Model interpretability\\nclass GradCAM:\\n    def __init__(self, model, target_layer):\\n        self.model = model\\n        self.target_layer = target_layer\\n        self.gradients = None\\n        self.activations = None\\n        \\n        # Register hooks\\n        self.target_layer.register_forward_hook(self.save_activation)\\n        self.target_layer.register_backward_hook(self.save_gradient)\\n    \\n    def save_activation(self, module, input, output):\\n        self.activations = output\\n    \\n    def save_gradient(self, module, grad_input, grad_output):\\n        self.gradients = grad_output[0]\\n    \\n    def generate_cam(self, input_image, class_idx):\\n        # Forward pass\\n        output = self.model(input_image)\\n        \\n        # Backward pass\\n        self.model.zero_grad()\\n        class_loss = output[0, class_idx]\\n        class_loss.backward()\\n        \\n        # Generate CAM\\n        gradients = self.gradients[0]\\n        activations = self.activations[0]\\n        \\n        weights = torch.mean(gradients, dim=(1, 2))\\n        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)\\n        \\n        for i, w in enumerate(weights):\\n            cam += w * activations[i]\\n        \\n        cam = F.relu(cam)\\n        cam = cam / torch.max(cam)\\n        \\n        return cam\\n\\n# Regularization techniques\\nclass DropBlock2D(nn.Module):\\n    def __init__(self, drop_rate, block_size):\\n        super(DropBlock2D, self).__init__()\\n        self.drop_rate = drop_rate\\n        self.block_size = block_size\\n    \\n    def forward(self, x):\\n        if not self.training:\\n            return x\\n        \\n        gamma = self.drop_rate / (self.block_size ** 2)\\n        mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\\n        \\n        # Expand mask\\n        mask = mask.unsqueeze(1)\\n        mask = F.max_pool2d(mask, (self.block_size, self.block_size), \\n                           stride=(1, 1), padding=self.block_size // 2)\\n        \\n        mask = 1 - mask\\n        normalize_factor = mask.numel() / mask.sum()\\n        \\n        return x * mask * normalize_factor\\n\\nclass MixUp:\\n    def __init__(self, alpha=1.0):\\n        self.alpha = alpha\\n    \\n    def __call__(self, x, y):\\n        if self.alpha > 0:\\n            lam = np.random.beta(self.alpha, self.alpha)\\n        else:\\n            lam = 1\\n        \\n        batch_size = x.size(0)\\n        index = torch.randperm(batch_size)\\n        \\n        mixed_x = lam * x + (1 - lam) * x[index, :]\\n        y_a, y_b = y, y[index]\\n        \\n        return mixed_x, y_a, y_b, lam\\n\\n# Knowledge distillation\\nclass DistillationLoss(nn.Module):\\n    def __init__(self, alpha=0.5, temperature=4):\\n        super(DistillationLoss, self).__init__()\\n        self.alpha = alpha\\n        self.temperature = temperature\\n        self.kl_div = nn.KLDivLoss(reduction=\'batchmean\')\\n        self.ce_loss = nn.CrossEntropyLoss()\\n    \\n    def forward(self, student_outputs, teacher_outputs, targets):\\n        # Soft targets from teacher\\n        soft_targets = F.softmax(teacher_outputs / self.temperature, dim=1)\\n        soft_student = F.log_softmax(student_outputs / self.temperature, dim=1)\\n        \\n        # Distillation loss\\n        distillation_loss = self.kl_div(soft_student, soft_targets) * (self.temperature ** 2)\\n        \\n        # Student loss\\n        student_loss = self.ce_loss(student_outputs, targets)\\n        \\n        # Combined loss\\n        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * student_loss\\n        \\n        return total_loss\\n\\n# Example usage of advanced techniques\\ndef advanced_training_example():\\n    # Create model\\n    model = CNN(num_classes=10)\\n    \\n    # Custom loss\\n    focal_loss = FocalLoss(alpha=1, gamma=2)\\n    \\n    # Metrics calculator\\n    metrics = MetricsCalculator(num_classes=10)\\n    \\n    # MixUp augmentation\\n    mixup = MixUp(alpha=1.0)\\n    \\n    # Training loop with advanced techniques\\n    model.train()\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        # Apply MixUp\\n        mixed_data, target_a, target_b, lam = mixup(data, target)\\n        \\n        # Forward pass\\n        output = model(mixed_data)\\n        \\n        # Calculate loss with MixUp\\n        loss = lam * focal_loss(output, target_a) + (1 - lam) * focal_loss(output, target_b)\\n        \\n        # Update metrics\\n        metrics.update(output, target)\\n        \\n        # Backward pass\\n        loss.backward()\\n        \\n        if batch_idx % 100 == 0:\\n            print(f\'Batch {batch_idx}, Loss: {loss.item():.4f}\')\\n            print(f\'Accuracy: {metrics.accuracy():.4f}\')\\n            print(f\'F1 Score: {metrics.f1_score():.4f}\')\\n\\n# Test advanced techniques\\nadvanced_training_example()\\n```\\n\\n## Production Deployment\\n\\n### Model Optimization and Deployment\\n\\n```python\\n# Model optimization techniques\\ndef optimize_model_for_inference(model, example_input):\\n    \\"\\"\\"Optimize model for inference\\"\\"\\"\\n    \\n    # 1. Set to evaluation mode\\n    model.eval()\\n    \\n    # 2. Trace the model\\n    traced_model = torch.jit.trace(model, example_input)\\n    \\n    # 3. Optimize for inference\\n    traced_model = torch.jit.optimize_for_inference(traced_model)\\n    \\n    # 4. Save optimized model\\n    traced_model.save(\'optimized_model.pt\')\\n    \\n    return traced_model\\n\\n# Quantization\\ndef quantize_model(model, data_loader):\\n    \\"\\"\\"Apply post-training quantization\\"\\"\\"\\n    \\n    # Prepare model for quantization\\n    model.eval()\\n    model_fp32 = model\\n    model_fp32.qconfig = torch.quantization.get_default_qconfig(\'fbgemm\')\\n    \\n    # Prepare model\\n    model_fp32_prepared = torch.quantization.prepare(model_fp32)\\n    \\n    # Calibrate with representative data\\n    with torch.no_grad():\\n        for data, _ in data_loader:\\n            model_fp32_prepared(data)\\n            break  # Use only one batch for calibration\\n    \\n    # Convert to quantized model\\n    model_int8 = torch.quantization.convert(model_fp32_prepared)\\n    \\n    return model_int8\\n\\n# ONNX export\\ndef export_to_onnx(model, example_input, onnx_path):\\n    \\"\\"\\"Export model to ONNX format\\"\\"\\"\\n    \\n    model.eval()\\n    \\n    torch.onnx.export(\\n        model,\\n        example_input,\\n        onnx_path,\\n        export_params=True,\\n        opset_version=11,\\n        do_constant_folding=True,\\n        input_names=[\'input\'],\\n        output_names=[\'output\'],\\n        dynamic_axes={\\n            \'input\': {0: \'batch_size\'},\\n            \'output\': {0: \'batch_size\'}\\n        }\\n    )\\n    \\n    print(f\\"Model exported to {onnx_path}\\")\\n\\n# TensorRT optimization (requires TensorRT)\\ndef optimize_with_tensorrt(onnx_path, trt_path):\\n    \\"\\"\\"Optimize ONNX model with TensorRT\\"\\"\\"\\n    try:\\n        import tensorrt as trt\\n        \\n        logger = trt.Logger(trt.Logger.WARNING)\\n        builder = trt.Builder(logger)\\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\\n        parser = trt.OnnxParser(network, logger)\\n        \\n        # Parse ONNX model\\n        with open(onnx_path, \'rb\') as model:\\n            if not parser.parse(model.read()):\\n                for error in range(parser.num_errors):\\n                    print(parser.get_error(error))\\n                return None\\n        \\n        # Build engine\\n        config = builder.create_builder_config()\\n        config.max_workspace_size = 1 << 30  # 1GB\\n        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision\\n        \\n        engine = builder.build_engine(network, config)\\n        \\n        # Save engine\\n        with open(trt_path, \'wb\') as f:\\n            f.write(engine.serialize())\\n        \\n        print(f\\"TensorRT engine saved to {trt_path}\\")\\n        return engine\\n        \\n    except ImportError:\\n        print(\\"TensorRT not available\\")\\n        return None\\n\\n# Model serving with Flask\\nfrom flask import Flask, request, jsonify\\nimport base64\\nfrom PIL import Image\\nimport io\\n\\nclass ModelServer:\\n    def __init__(self, model_path, device=\'cpu\'):\\n        self.device = device\\n        self.model = torch.jit.load(model_path, map_location=device)\\n        self.model.eval()\\n        \\n        # Define preprocessing\\n        self.transform = transforms.Compose([\\n            transforms.Resize(224),\\n            transforms.CenterCrop(224),\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n        ])\\n    \\n    def predict(self, image):\\n        # Preprocess image\\n        if isinstance(image, str):  # Base64 encoded\\n            image_data = base64.b64decode(image)\\n            image = Image.open(io.BytesIO(image_data)).convert(\'RGB\')\\n        \\n        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\\n        \\n        # Inference\\n        with torch.no_grad():\\n            output = self.model(input_tensor)\\n            probabilities = F.softmax(output, dim=1)\\n            predicted_class = torch.argmax(probabilities, dim=1).item()\\n            confidence = probabilities[0][predicted_class].item()\\n        \\n        return {\\n            \'predicted_class\': predicted_class,\\n            \'confidence\': confidence,\\n            \'probabilities\': probabilities[0].tolist()\\n        }\\n\\n# Flask app\\napp = Flask(__name__)\\nmodel_server = ModelServer(\'optimized_model.pt\')\\n\\n@app.route(\'/predict\', methods=[\'POST\'])\\ndef predict():\\n    try:\\n        data = request.json\\n        image_data = data[\'image\']\\n        \\n        result = model_server.predict(image_data)\\n        return jsonify(result)\\n    \\n    except Exception as e:\\n        return jsonify({\'error\': str(e)}), 400\\n\\n@app.route(\'/health\', methods=[\'GET\'])\\ndef health():\\n    return jsonify({\'status\': \'healthy\'})\\n\\n# Batch inference optimization\\nclass BatchInferenceEngine:\\n    def __init__(self, model_path, batch_size=32, device=\'cuda\'):\\n        self.model = torch.jit.load(model_path, map_location=device)\\n        self.model.eval()\\n        self.batch_size = batch_size\\n        self.device = device\\n        self.pending_requests = []\\n    \\n    def add_request(self, image, request_id):\\n        self.pending_requests.append((image, request_id))\\n        \\n        if len(self.pending_requests) >= self.batch_size:\\n            return self.process_batch()\\n        return None\\n    \\n    def process_batch(self):\\n        if not self.pending_requests:\\n            return []\\n        \\n        # Prepare batch\\n        images = []\\n        request_ids = []\\n        \\n        for image, request_id in self.pending_requests:\\n            images.append(image)\\n            request_ids.append(request_id)\\n        \\n        # Convert to tensor\\n        batch_tensor = torch.stack(images).to(self.device)\\n        \\n        # Inference\\n        with torch.no_grad():\\n            outputs = self.model(batch_tensor)\\n            probabilities = F.softmax(outputs, dim=1)\\n        \\n        # Prepare results\\n        results = []\\n        for i, request_id in enumerate(request_ids):\\n            predicted_class = torch.argmax(probabilities[i]).item()\\n            confidence = probabilities[i][predicted_class].item()\\n            \\n            results.append({\\n                \'request_id\': request_id,\\n                \'predicted_class\': predicted_class,\\n                \'confidence\': confidence\\n            })\\n        \\n        # Clear pending requests\\n        self.pending_requests = []\\n        \\n        return results\\n\\n# Performance monitoring\\nclass PerformanceMonitor:\\n    def __init__(self):\\n        self.inference_times = []\\n        self.memory_usage = []\\n        self.throughput = []\\n    \\n    def log_inference(self, inference_time, memory_used, batch_size):\\n        self.inference_times.append(inference_time)\\n        self.memory_usage.append(memory_used)\\n        self.throughput.append(batch_size / inference_time)\\n    \\n    def get_stats(self):\\n        if not self.inference_times:\\n            return {}\\n        \\n        return {\\n            \'avg_inference_time\': np.mean(self.inference_times),\\n            \'p95_inference_time\': np.percentile(self.inference_times, 95),\\n            \'avg_memory_usage\': np.mean(self.memory_usage),\\n            \'avg_throughput\': np.mean(self.throughput),\\n            \'total_requests\': len(self.inference_times)\\n        }\\n\\n# Example deployment workflow\\ndef deployment_workflow():\\n    # 1. Load trained model\\n    model = CNN(num_classes=10)\\n    model.load_state_dict(torch.load(\'best_model.pth\'))\\n    \\n    # 2. Create example input\\n    example_input = torch.randn(1, 3, 224, 224)\\n    \\n    # 3. Optimize model\\n    optimized_model = optimize_model_for_inference(model, example_input)\\n    \\n    # 4. Quantize model (optional)\\n    # quantized_model = quantize_model(model, val_loader)\\n    \\n    # 5. Export to ONNX\\n    export_to_onnx(optimized_model, example_input, \'model.onnx\')\\n    \\n    # 6. Optimize with TensorRT (optional)\\n    # optimize_with_tensorrt(\'model.onnx\', \'model.trt\')\\n    \\n    # 7. Test inference\\n    test_inference_performance(optimized_model, example_input)\\n    \\n    print(\\"Deployment workflow completed!\\")\\n\\ndef test_inference_performance(model, example_input, num_runs=100):\\n    \\"\\"\\"Test inference performance\\"\\"\\"\\n    model.eval()\\n    \\n    # Warmup\\n    for _ in range(10):\\n        with torch.no_grad():\\n            _ = model(example_input)\\n    \\n    # Measure performance\\n    start_time = time.time()\\n    \\n    for _ in range(num_runs):\\n        with torch.no_grad():\\n            _ = model(example_input)\\n    \\n    end_time = time.time()\\n    \\n    avg_time = (end_time - start_time) / num_runs\\n    throughput = 1 / avg_time\\n    \\n    print(f\\"Average inference time: {avg_time*1000:.2f} ms\\")\\n    print(f\\"Throughput: {throughput:.2f} inferences/second\\")\\n\\n# Run deployment workflow\\ndeployment_workflow()\\n\\nif __name__ == \'__main__\':\\n    # Start Flask server\\n    app.run(host=\'0.0.0.0\', port=5000, debug=False)\\n```\\n\\n## Conclusion\\n\\nThis comprehensive PyTorch tutorial covers the essential aspects of machine learning and deep learning implementation, from basic tensor operations to production deployment. Key takeaways include:\\n\\n### Core Concepts Covered\\n1. **PyTorch Fundamentals**: Tensors, autograd, and basic operations\\n2. **Data Handling**: Custom datasets, data loaders, and preprocessing\\n3. **Neural Networks**: From simple MLPs to advanced architectures\\n4. **Training**: Optimization, loss functions, and training loops\\n5. **Computer Vision**: CNNs, transfer learning, and specialized architectures\\n6. **NLP**: RNNs, Transformers, and text processing\\n7. **Advanced Topics**: Custom losses, regularization, and interpretability\\n8. **Production**: Model optimization, quantization, and deployment\\n\\n### Best Practices\\n- Use appropriate data augmentation and preprocessing\\n- Implement proper validation and early stopping\\n- Monitor training with comprehensive metrics\\n- Apply regularization techniques to prevent overfitting\\n- Optimize models for production deployment\\n- Use mixed precision training for efficiency\\n- Implement proper error handling and logging\\n\\n### Next Steps\\n- Explore domain-specific applications\\n- Implement state-of-the-art architectures\\n- Experiment with distributed training\\n- Learn about model compression techniques\\n- Practice with real-world datasets\\n- Contribute to open-source projects\\n\\nThis tutorial provides a solid foundation for building and deploying machine learning models with PyTorch. Continue practicing with different datasets and architectures to master these concepts.\\n\\n---\\n\\n*Last updated: September 2025*"},{"id":"gitHub-beginner-guide","metadata":{"permalink":"/blog/gitHub-beginner-guide","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-04-01-gitHub-beginner-guide.md","source":"@site/blog/2021-04-01-gitHub-beginner-guide.md","title":"GitHub Beginner Guide","description":"Introduction","date":"2021-04-01T00:00:00.000Z","tags":[{"inline":false,"label":"Tutorial","permalink":"/blog/tags/tutorial","description":"Tutorial tag description"}],"readingTime":2.85,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"gitHub-beginner-guide","title":"GitHub Beginner Guide","authors":["liangchao"],"tags":["tutorial"]},"unlisted":false,"prevItem":{"title":"Complete PyTorch Tutorial for Machine Learning and Deep Learning","permalink":"/blog/pytorch-ml-dl-tutorial"},"nextItem":{"title":"MACOS Shortcuts","permalink":"/blog/macOS-shortcuts"}},"content":"## Introduction\\nGitHub is a web-based platform for version control and collaboration. It allows multiple people to work on projects together, track changes, and manage code repositories using Git.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Setting Up Git and GitHub\\n\\n### **1.1 Create a GitHub Account**\\n1. Go to [GitHub](https://github.com/).\\n2. Click **Sign up** and fill in your details.\\n3. Verify your email and set up your profile.\\n\\n### **1.2 Install Git**\\n#### **Windows:**\\nDownload and install Git from [Git for Windows](https://git-scm.com/).\\n\\n#### **Mac:**\\n```bash\\nbrew install git\\n```\\n\\n#### **Linux:**\\n```bash\\nsudo apt update\\nsudo apt install git\\n```\\n\\n### **1.3 Configure Git**\\nSet up your Git username and email:\\n```bash\\ngit config --global user.name \\"Your GitHub Username\\"\\ngit config --global user.email \\"Your GitHub Email\\"\\n```\\nCheck the configuration:\\n```bash\\ngit config --list\\n```\\n\\n---\\n\\n## 2. Creating a Repository on GitHub\\n1. Log in to [GitHub](https://github.com/).\\n2. Click **New repository**.\\n3. Enter a **repository name**, select visibility (Public or Private), and click **Create repository**.\\n\\n---\\n\\n## 3. Cloning a Repository\\nTo copy a GitHub repository to your local machine:\\n```bash\\ngit clone https://github.com/your-username/repository-name.git\\n```\\n\\nMove into the directory:\\n```bash\\ncd repository-name\\n```\\n\\n---\\n\\n## 4. Adding and Committing Changes\\n### **4.1 Create a new file**\\n```bash\\necho \\"# My Project\\" > README.md\\n```\\n\\n### **4.2 Add files to staging**\\n```bash\\ngit add README.md\\n```\\n\\n### **4.3 Commit changes**\\n```bash\\ngit commit -m \\"Initial commit\\"\\n```\\n\\n---\\n\\n## 5. Pushing Code to GitHub\\n```bash\\ngit push origin main\\n```\\n\\nIf your branch is different from `main`, use:\\n```bash\\ngit push origin your-branch-name\\n```\\n\\n---\\n\\n## 6. Pulling Updates from GitHub\\nTo get the latest changes from GitHub:\\n```bash\\ngit pull origin main\\n```\\n\\n---\\n\\n## 7. Branching and Merging\\n### **7.1 Create a New Branch**\\n```bash\\ngit branch feature-branch\\n```\\n\\n### **7.2 Switch to the New Branch**\\n```bash\\ngit checkout feature-branch\\n```\\n\\n### **7.3 Merge a Branch**\\n```bash\\ngit checkout main\\ngit merge feature-branch\\n```\\n\\n### **7.4 Delete a Branch**\\n```bash\\ngit branch -d feature-branch\\n```\\n\\n---\\n\\n## 8. Forking a Repository and Creating Pull Requests\\n### **8.1 Fork a Repository**\\n1. Go to the repository on GitHub.\\n2. Click **Fork** (top-right corner).\\n3. Clone your forked repository:\\n```bash\\ngit clone https://github.com/your-username/forked-repository.git\\n```\\n\\n### **8.2 Make Changes and Push**\\n```bash\\ngit add .\\ngit commit -m \\"Modified file\\"\\ngit push origin your-branch\\n```\\n\\n### **8.3 Create a Pull Request (PR)**\\n1. Go to your forked repository on GitHub.\\n2. Click **New pull request**.\\n3. Compare changes and click **Create pull request**.\\n\\n---\\n\\n## 9. Git Ignore and Undo Changes\\n### **9.1 Ignoring Files**\\nCreate a `.gitignore` file and add files or folders you want to ignore:\\n```\\nnode_modules/\\n*.log\\n.env\\n```\\n\\n### **9.2 Undo Changes**\\n#### **Undo uncommitted changes:**\\n```bash\\ngit checkout -- filename\\n```\\n\\n#### **Undo last commit (keep changes unstaged):**\\n```bash\\ngit reset --soft HEAD~1\\n```\\n\\n#### **Undo last commit (discard changes):**\\n```bash\\ngit reset --hard HEAD~1\\n```\\n\\n---\\n\\n## 10. Useful Git Commands Summary\\n| Command | Description |\\n|---------|-------------|\\n| `git init` | Initialize a Git repository |\\n| `git clone URL` | Clone a repository |\\n| `git status` | Show current changes |\\n| `git add .` | Add all files to staging |\\n| `git commit -m \\"message\\"` | Commit changes |\\n| `git push origin branch` | Push changes to GitHub |\\n| `git pull origin branch` | Pull changes from GitHub |\\n| `git branch branch-name` | Create a new branch |\\n| `git checkout branch-name` | Switch branches |\\n| `git merge branch-name` | Merge branches |\\n| `git reset --hard HEAD~1` | Undo last commit |\\n| `git log` | View commit history |\\n\\n---\\n\\n## Conclusion\\nThis guide covers the basics of Git and GitHub. As you become more familiar, you can explore advanced topics such as GitHub Actions, contributing to open-source projects, and automated deployments."},{"id":"macOS-shortcuts","metadata":{"permalink":"/blog/macOS-shortcuts","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-01-01-macos-shortcuts.md","source":"@site/blog/2021-01-01-macos-shortcuts.md","title":"MACOS Shortcuts","description":"Personal Information","date":"2021-01-01T00:00:00.000Z","tags":[{"inline":false,"label":"Tip","permalink":"/blog/tags/tip","description":"Tip tag description"}],"readingTime":1.35,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"PhD @ SHZU","url":"https://github.com/smiler488","page":{"permalink":"/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"macOS-shortcuts","title":"MACOS Shortcuts","authors":["liangchao"],"tags":["tip"]},"unlisted":false,"prevItem":{"title":"GitHub Beginner Guide","permalink":"/blog/gitHub-beginner-guide"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"## **Personal Information**\\n\\nBelow is a categorized list of commonly used macOS shortcuts, combining high-frequency usage scenarios and efficiency improvement techniques:\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n\\n### 1.\xa0**System-Level Operations**\\n\\n1. **Quick Search**  \\n    `\u2318 + Space`\xa0\u2013 Open Spotlight search for finding files, applications, calculations, etc.  \\n    `\u2303 + \u2318 + Space`\xa0\u2013 Open the system emoji panel.\\n    \\n2. **Window Management**\\n    \\n    - `\u2318 + Tab`\xa0\u2013 Switch between open applications.\\n    - `\u2318 + H`\xa0\u2013 Hide the current window;\xa0`\u2325 + \u2318 + H`\xa0\u2013 Hide all other windows.\\n3. **Screenshot & Screen Recording**\\n    \\n    - `\u2318 + Shift + 3`\xa0\u2013 Capture the entire screen.\\n    - `\u2318 + Shift + 4`\xa0\u2013 Capture a selected area (press Space to capture a window).\\n    - `\u2318 + Shift + 5`\xa0\u2013 Open the advanced screenshot/recording menu.\\n4. **Force Operations**\\n    \\n    - `\u2318 + \u2325 + Esc`\xa0\u2013 Force quit unresponsive applications.\\n    - `\u2303 + \u2318 + Power button`\xa0\u2013 Force shutdown.\\n\\n---\\n\\n### 2.\xa0**Files & Finder**\\n\\n1. **Basic Operations**\\n    \\n    - `\u2318 + Delete`\xa0\u2013 Move file to the Trash.\\n    - `\u2318 + Shift + Delete`\xa0\u2013 Empty the Trash.\\n    - `\u2318 + I`\xa0\u2013 Get file info.\\n    - `\u2318 + D`\xa0\u2013 Duplicate selected file.\\n2. **Navigation**\\n    \\n    - `\u2318 + \u2191`\xa0\u2013 Go up one folder level.\\n    - `\u2318 + \u2193`\xa0\u2013 Open selected file or folder.\\n    - `\u2318 + \u21e7 + G`\xa0\u2013 Go to a specific folder path.\\n3. **View & Sorting**\\n    \\n    - `\u2318 + 1`\xa0\u2013 Icon view.\\n    - `\u2318 + 2`\xa0\u2013 List view.\\n    - `\u2318 + 3`\xa0\u2013 Column view.\\n    - `\u2318 + 4`\xa0\u2013 Gallery view.\\n\\n---\\n\\n### 3.\xa0**Text Editing & Navigation**\\n\\n1. **Selection & Movement**\\n    \\n    - `\u2318 + A`\xa0\u2013 Select all.\\n    - `\u2318 + C`\xa0\u2013 Copy.\\n    - `\u2318 + X`\xa0\u2013 Cut.\\n    - `\u2318 + V`\xa0\u2013 Paste.\\n    - `\u2318 + Z`\xa0\u2013 Undo.\\n    - `\u2318 + \u21e7 + Z`\xa0\u2013 Redo.\\n2. **Text Navigation**\\n    \\n    - `\u2325 + \u2192`\xa0\u2013 Move cursor one word forward.\\n    - `\u2325 + \u2190`\xa0\u2013 Move cursor one word backward.\\n    - `\u2318 + \u2192`\xa0\u2013 Move cursor to the end of the line.\\n    - `\u2318 + \u2190`\xa0\u2013 Move cursor to the beginning of the line.\\n\\n---\\n\\n### 4.\xa0**Browser Shortcuts (Safari/Chrome)**\\n\\n1. **Tabs & Windows**\\n    \\n    - `\u2318 + T`\xa0\u2013 Open a new tab.\\n    - `\u2318 + W`\xa0\u2013 Close the current tab.\\n    - `\u2318 + \u21e7 + T`\xa0\u2013 Reopen the last closed tab.\\n    - `\u2318 + N`\xa0\u2013 Open a new window.\\n2. **Navigation**\\n    \\n    - `\u2318 + L`\xa0\u2013 Focus the address bar.\\n    - `\u2318 + R`\xa0\u2013 Refresh the page.\\n    - `\u2318 + [`\xa0\u2013 Go back.\\n    - `\u2318 + ]`\xa0\u2013 Go forward.\\n\\n---\\n\\nThese shortcuts will help improve efficiency when using macOS in various scenarios."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-26-pic-blog-post/index.md","source":"@site/blog/2020-08-26-pic-blog-post/index.md","title":"Welcome","description":"Docusaurus blogging features are powered by the blog plugin.","date":"2020-08-26T00:00:00.000Z","tags":[{"inline":false,"label":"Facebook","permalink":"/blog/tags/facebook","description":"Facebook tag description"},{"inline":false,"label":"Hello","permalink":"/blog/tags/hello","description":"Hello tag description"},{"inline":false,"label":"Docusaurus","permalink":"/blog/tags/docusaurus","description":"Docusaurus tag description"}],"readingTime":0.56,"hasTruncateMarker":true,"authors":[{"name":"S\xe9bastien Lorber","title":"Docusaurus maintainer","url":"https://sebastienlorber.com","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://github.com/slorber.png","key":"slorber"},{"name":"Yangshun Tay","title":"Front End Engineer @ Facebook","url":"https://github.com/yangshun","page":{"permalink":"/blog/authors/yangshun"},"socials":{"x":"https://x.com/yangshunz","github":"https://github.com/yangshun"},"imageURL":"https://github.com/yangshun.png","key":"yangshun"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["slorber","yangshun"],"tags":["facebook","hello","docusaurus"]},"unlisted":false,"prevItem":{"title":"MACOS Shortcuts","permalink":"/blog/macOS-shortcuts"},"nextItem":{"title":"MDX Blog Post","permalink":"/blog/mdx-blog-post"}},"content":"[Docusaurus blogging features](https://docusaurus.io/docs/blog) are powered by the [blog plugin](https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog).\\n\\nHere are a few tips you might find useful.\\n\\n\x3c!-- truncate --\x3e\\n\\nSimply add Markdown files (or folders) to the `blog` directory.\\n\\nRegular blog authors can be added to `authors.yml`.\\n\\nThe blog post date can be extracted from filenames, such as:\\n\\n- `2019-05-30-welcome.md`\\n- `2019-05-30-welcome/index.md`\\n\\nA blog post folder can be convenient to co-locate blog post images:\\n\\n![Docusaurus Plushie](./docusaurus-plushie-banner.jpeg)\\n\\nThe blog supports tags as well!\\n\\n**And if you don\'t want a blog**: just delete this directory, and use `blog: false` in your Docusaurus config."},{"id":"mdx-blog-post","metadata":{"permalink":"/blog/mdx-blog-post","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-01-mdx-blog-post.mdx","source":"@site/blog/2020-08-01-mdx-blog-post.mdx","title":"MDX Blog Post","description":"Blog posts support Docusaurus Markdown features, such as MDX.","date":"2020-08-01T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/blog/tags/docusaurus","description":"Docusaurus tag description"}],"readingTime":0.27,"hasTruncateMarker":true,"authors":[{"name":"S\xe9bastien Lorber","title":"Docusaurus maintainer","url":"https://sebastienlorber.com","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://github.com/slorber.png","key":"slorber"}],"frontMatter":{"slug":"mdx-blog-post","title":"MDX Blog Post","authors":["slorber"],"tags":["docusaurus"]},"unlisted":false,"prevItem":{"title":"Welcome","permalink":"/blog/welcome"},"nextItem":{"title":"Long Blog Post","permalink":"/blog/long-blog-post"}},"content":"Blog posts support [Docusaurus Markdown features](https://docusaurus.io/docs/markdown-features), such as [MDX](https://mdxjs.com/).\\n\\n:::tip\\n\\nUse the power of React to create interactive blog posts.\\n\\n:::\\n\\n{/* truncate */}\\n\\nFor example, use JSX to create an interactive button:\\n\\n```js\\n<button onClick={() => alert(\'button clicked!\')}>Click me!</button>\\n```\\n\\n<button onClick={() => alert(\'button clicked!\')}>Click me!</button>"},{"id":"long-blog-post","metadata":{"permalink":"/blog/long-blog-post","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-29-long-blog-post.md","source":"@site/blog/2019-05-29-long-blog-post.md","title":"Long Blog Post","description":"This is the summary of a very long blog post,","date":"2019-05-29T00:00:00.000Z","tags":[{"inline":false,"label":"Hello","permalink":"/blog/tags/hello","description":"Hello tag description"},{"inline":false,"label":"Docusaurus","permalink":"/blog/tags/docusaurus","description":"Docusaurus tag description"}],"readingTime":2.04,"hasTruncateMarker":true,"authors":[{"name":"Yangshun Tay","title":"Front End Engineer @ Facebook","url":"https://github.com/yangshun","page":{"permalink":"/blog/authors/yangshun"},"socials":{"x":"https://x.com/yangshunz","github":"https://github.com/yangshun"},"imageURL":"https://github.com/yangshun.png","key":"yangshun"}],"frontMatter":{"slug":"long-blog-post","title":"Long Blog Post","authors":"yangshun","tags":["hello","docusaurus"]},"unlisted":false,"prevItem":{"title":"MDX Blog Post","permalink":"/blog/mdx-blog-post"},"nextItem":{"title":"First Blog Post","permalink":"/blog/first-blog-post"}},"content":"This is the summary of a very long blog post,\\n\\nUse a `\x3c!--` `truncate` `--\x3e` comment to limit blog post size in the list view.\\n\\n\x3c!-- truncate --\x3e\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"},{"id":"first-blog-post","metadata":{"permalink":"/blog/first-blog-post","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-05-28-first-blog-post.md","source":"@site/blog/2019-05-28-first-blog-post.md","title":"First Blog Post","description":"Lorem ipsum dolor sit amet...","date":"2019-05-28T00:00:00.000Z","tags":[{"inline":false,"label":"Hola","permalink":"/blog/tags/hola","description":"Hola tag description"},{"inline":false,"label":"Docusaurus","permalink":"/blog/tags/docusaurus","description":"Docusaurus tag description"}],"readingTime":0.13,"hasTruncateMarker":true,"authors":[{"name":"S\xe9bastien Lorber","title":"Docusaurus maintainer","url":"https://sebastienlorber.com","page":{"permalink":"/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://github.com/slorber.png","key":"slorber"},{"name":"Yangshun Tay","title":"Front End Engineer @ Facebook","url":"https://github.com/yangshun","page":{"permalink":"/blog/authors/yangshun"},"socials":{"x":"https://x.com/yangshunz","github":"https://github.com/yangshun"},"imageURL":"https://github.com/yangshun.png","key":"yangshun"}],"frontMatter":{"slug":"first-blog-post","title":"First Blog Post","authors":["slorber","yangshun"],"tags":["hola","docusaurus"]},"unlisted":false,"prevItem":{"title":"Long Blog Post","permalink":"/blog/long-blog-post"}},"content":"Lorem ipsum dolor sit amet...\\n\\n\x3c!-- truncate --\x3e\\n\\n...consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"}]}}')}}]);