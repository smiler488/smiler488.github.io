"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[3743],{207:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var r=i(86113),a=i(74848),t=i(28453);const o={slug:"local-image-quantification-tutorial",title:"Local Image Quantification Tool - A Standalone Python Solution for Agricultural Analysis",authors:["liangchao"],tags:["Python","Computer Vision","Agriculture","Image Analysis","OpenCV"],image:"/img/blog-default.jpg"},s=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Overview",id:"overview",level:2},{value:"Features",id:"features",level:2},{value:"Core Capabilities",id:"core-capabilities",level:3},{value:"Technical Highlights",id:"technical-highlights",level:3},{value:"Installation",id:"installation",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Required Libraries",id:"required-libraries",level:3},{value:"Setup Steps",id:"setup-steps",level:3},{value:"Complete Code",id:"complete-code",level:2},{value:"Python API Usage",id:"python-api-usage",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Image Capture Guidelines",id:"image-capture-guidelines",level:3},{value:"Reference Object Selection",id:"reference-object-selection",level:3},{value:"Parameter Tuning",id:"parameter-tuning",level:3},{value:"Quality Control",id:"quality-control",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"1. Reference Object Not Detected",id:"1-reference-object-not-detected",level:4},{value:"2. Too Few Samples Detected",id:"2-too-few-samples-detected",level:4},{value:"3. Too Many Samples Detected",id:"3-too-many-samples-detected",level:4},{value:"4. Inaccurate Measurements",id:"4-inaccurate-measurements",level:4},{value:"5. Color Metrics Are Off",id:"5-color-metrics-are-off",level:4},{value:"Error Messages",id:"error-messages",level:3},{value:"&quot;Analysis failed: [error message]&quot;",id:"analysis-failed-error-message",level:4},{value:"&quot;No objects found&quot;",id:"no-objects-found",level:4},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"For Large Datasets",id:"for-large-datasets",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Output Formats",id:"output-formats",level:2},{value:"CSV Columns",id:"csv-columns",level:3},{value:"JSON Structure",id:"json-structure",level:3},{value:"Advanced Usage",id:"advanced-usage",level:2},{value:"Custom Analysis Pipeline",id:"custom-analysis-pipeline",level:3},{value:"Integration with Other Tools",id:"integration-with-other-tools",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,a.jsx)(n.p,{children:"This tutorial introduces a powerful standalone Python tool for agricultural image quantification analysis. The tool provides automated detection and measurement of plant samples from images, including morphological and color metrics."}),"\n",(0,a.jsx)(n.h1,{id:"local-image-quantification-tool---a-standalone-python-solution-for-agricultural-analysis",children:"Local Image Quantification Tool - A Standalone Python Solution for Agricultural Analysis"}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"The image quantification tool is designed for researchers and agricultural professionals who need to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Automatically detect and segment plant samples from images"}),"\n",(0,a.jsx)(n.li,{children:"Measure morphological properties (length, width, area, perimeter, aspect ratio, circularity)"}),"\n",(0,a.jsx)(n.li,{children:"Extract color information (RGB, HSV, green index, brown index)"}),"\n",(0,a.jsx)(n.li,{children:"Handle reference objects for scale calibration"}),"\n",(0,a.jsx)(n.li,{children:"Generate comprehensive analysis reports"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,a.jsx)(n.h3,{id:"core-capabilities",children:"Core Capabilities"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Automatic Reference Detection"}),": Identifies reference objects for scale calibration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robust Segmentation"}),": Uses foreground masking and connected component analysis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PCA-based Measurements"}),": Accurate orientation and dimension calculations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Color Analysis"}),": RGB, HSV values and vegetation indices"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multiple Output Formats"}),": CSV, JSON, and visual overlays"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"technical-highlights",children:"Technical Highlights"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Downsampling"}),": Automatic image scaling for processing efficiency"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Morphological Operations"}),": Noise reduction and shape refinement"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Component Analysis"}),": Statistical analysis of detected objects"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visualization"}),": Annotated output images with measurements"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Python 3.8+\npip install opencv-python numpy pandas pillow\n"})}),"\n",(0,a.jsx)(n.h3,{id:"required-libraries",children:"Required Libraries"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport tempfile\nfrom typing import List, Tuple, Optional, Dict, Any\n"})}),"\n",(0,a.jsx)(n.h3,{id:"setup-steps",children:"Setup Steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create a new directory for your project"}),"\n",(0,a.jsxs)(n.li,{children:["Save the complete code as ",(0,a.jsx)(n.code,{children:"image_quantification.py"})]}),"\n",(0,a.jsx)(n.li,{children:"Install required dependencies"}),"\n",(0,a.jsx)(n.li,{children:"Prepare your plant images with a reference object"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"complete-code",children:"Complete Code"}),"\n",(0,a.jsxs)(n.p,{children:["Save the following code as ",(0,a.jsx)(n.code,{children:"image_quantification.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import tempfile\nfrom typing import List, Tuple, Optional, Dict, Any\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\n# -----------------------------\n# Global configuration\n# -----------------------------\n\nMAX_SIDE = 1024  # Moderate downsampling for speed\n\n\n# -----------------------------\n# Utility functions\n# -----------------------------\n\ndef downscale_bgr(img: np.ndarray) -> Tuple[np.ndarray, float]:\n    """Downscale image so that the longest side is <= MAX_SIDE.\n\n    Returns\n    -------\n    img_resized : np.ndarray\n        Possibly downscaled BGR image.\n    scale : float\n        Applied scale factor (<= 1).\n    """\n    h, w = img.shape[:2]\n    max_hw = max(h, w)\n    if max_hw <= MAX_SIDE:\n        return img, 1.0\n    scale = MAX_SIDE / float(max_hw)\n    img_resized = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n    return img_resized, scale\n\n\ndef normalize_angle(angle: float, size_w: float, size_h: float) -> float:\n    """Normalize OpenCV minAreaRect angle to [0, 180) degrees.\n\n    OpenCV returns angles depending on whether width < height. We fix it so that\n    the *long side* is treated as length and angle is always in [0, 180).\n    """\n    a = angle\n    if size_w < size_h:\n        a += 90.0\n    a = ((a % 180.0) + 180.0) % 180.0\n    return a\n\n\n# -----------------------------\n# Reference object detection\n# -----------------------------\n\ndef detect_reference(\n    img_bgr: np.ndarray,\n    mode: str,\n    ref_size_mm: Optional[float],\n) -> Tuple[float, Optional[Tuple[int, int]], Optional[str], Optional[Tuple[int, int, int, int]]]:\n    """Detect reference object: the first object in the upper-left corner\n    \n    Parameters:\n        img_bgr: BGR image\n        mode: Reference mode ("auto", "manual")\n        ref_size_mm: Reference object bounding box size in millimeters\n    \n    Returns:\n        px_per_mm: Pixels per millimeter ratio\n        ref_center: Reference object center coordinates\n        ref_type: Reference object type\n        ref_bbox: Reference object bounding box\n    """\n    h, w = img_bgr.shape[:2]\n\n    # Build foreground mask\n    mask = build_foreground_mask(img_bgr)\n\n    # Connected component analysis\n    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask)\n\n    # Find the upper-left reference object\n    candidates = []\n    min_area = (h * w) // 500  # Minimum area\n    max_area = (h * w) // 20   # Maximum area\n    \n    for i in range(1, num_labels):\n        x, y, ww, hh, area = stats[i]\n        \n        # Area filtering\n        if area < min_area or area > max_area:\n            continue\n            \n        # Position filtering: must be in upper-left region\n        if x > w * 0.4 or y > h * 0.4:\n            continue\n            \n        # Shape filtering: reference should be near square\n        aspect_ratio = max(ww, hh) / (min(ww, hh) + 1e-6)\n        if aspect_ratio > 3.0:\n            continue\n\n        cx, cy = centroids[i]\n        # Sort by position: closer to upper-left is better\n        score = x + y\n        candidates.append((score, i, (x, y, ww, hh), area, (int(cx), int(cy))))\n\n    if not candidates:\n        return 4.0, None, "square", None\n\n    # Select the most upper-left candidate\n    candidates.sort(key=lambda c: c[0])\n    score, label_idx, bbox, area, center = candidates[0]\n    \n    x, y, ww, hh = bbox\n\n    # Calculate pixels per millimeter ratio\n    ref_bbox_size_px = max(ww, hh)\n    px_per_mm = ref_bbox_size_px / ref_size_mm\n\n    return px_per_mm, center, "square", (x, y, ww, hh)\n\n\n# -----------------------------\n# Segmentation & measurements\n# -----------------------------\n\ndef build_foreground_mask(img_bgr: np.ndarray) -> np.ndarray:\n    """Simple foreground mask construction"""\n    h, w = img_bgr.shape[:2]\n    \n    # Convert to LAB color space\n    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n    \n    # Estimate background color from four corners\n    corner_size = min(h, w) // 10\n    corners = [\n        lab[:corner_size, :corner_size],\n        lab[:corner_size, -corner_size:],\n        lab[-corner_size:, :corner_size],\n        lab[-corner_size:, -corner_size:]\n    ]\n    corner_pixels = np.vstack([c.reshape(-1, 3) for c in corners])\n    bg_color = np.mean(corner_pixels, axis=0)\n    \n    # Calculate distance of each pixel from background\n    diff = lab.astype(np.float32) - bg_color\n    dist = np.sqrt(np.sum(diff * diff, axis=2))\n    \n    # Use Otsu thresholding\n    dist_uint8 = np.clip(dist * 3, 0, 255).astype(np.uint8)\n    _, mask = cv2.threshold(dist_uint8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Morphological processing\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\n\ndef segment(\n    img_bgr: np.ndarray,\n    sample_type: str = "leaves",\n    hsv_low_h: int = 35,\n    hsv_high_h: int = 85,\n    color_tol: int = 40,\n    min_area_px: float = 100,\n    max_area_px: float = 1e9,\n    tip_detection_sensitivity: float = 0.7,\n    tip_preservation_priority: float = 0.8,\n    edge_detection_scales: List[float] = None,\n    morphology_adaptation: bool = True,\n) -> List[Dict[str, Any]]:\n    """Simple and reliable segmentation algorithm"""\n    \n    # Use simple foreground mask\n    mask = build_foreground_mask(img_bgr)\n    \n    # Connected component analysis\n    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask)\n    \n    components: List[Dict[str, Any]] = []\n    \n    # Sort by position, skip first (usually reference)\n    all_objects = []\n    for i in range(1, num_labels):\n        x, y, ww, hh, area = stats[i]\n        if area < min_area_px or area > max_area_px:\n            continue\n        \n        cx, cy = centroids[i]\n        # Simple position scoring: left to right\n        score = x + y * 0.1  # Prioritize x coordinate\n        all_objects.append((score, i, (x, y, ww, hh), area, (int(cx), int(cy))))\n    \n    if len(all_objects) == 0:\n        return []\n    \n    # Sort and skip first (reference)\n    all_objects.sort(key=lambda obj: obj[0])\n    \n    # Simple check to skip the first object\n    skip_first = False\n    if len(all_objects) > 0:\n        _, _, (x, y, ww, hh), area, _ = all_objects[0]\n        h, w = img_bgr.shape[:2]\n        \n        # If the first object is in the upper-left corner and has reasonable shape, skip it\n        is_topleft = (x < w * 0.3 and y < h * 0.3)\n        aspect_ratio = max(ww, hh) / (min(ww, hh) + 1e-6)\n        is_reasonable_shape = aspect_ratio < 3.0\n        \n        skip_first = is_topleft and is_reasonable_shape\n    \n    # Process objects\n    start_idx = 1 if skip_first else 0\n    for obj_data in all_objects[start_idx:]:\n        _, label_idx, bbox, area, center = obj_data\n        \n        # Extract contour\n        component_mask = (labels == label_idx).astype(np.uint8) * 255\n        cnts, _ = cv2.findContours(component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        if len(cnts) == 0:\n            continue\n        \n        cnt = cnts[0]\n        \n        # Calculate geometric features\n        rect = cv2.minAreaRect(cnt)\n        box = cv2.boxPoints(rect).astype(np.int32)\n        \n        peri = cv2.arcLength(cnt, True)\n        \n        # Fix OpenCV minAreaRect axis correspondence issue\n        center_x, center_y = rect[0]\n        size_w, size_h = rect[1]\n        angle_cv = rect[2]\n        \n        # OpenCV angle definition has issues, need to recalculate\n        # Use PCA method to ensure long axis corresponds to maximum eigenvalue direction\n        \n        # Extract contour points\n        contour_points = cnt.reshape(-1, 2).astype(np.float32)\n        \n        # Calculate centroid\n        cx = np.mean(contour_points[:, 0])\n        cy = np.mean(contour_points[:, 1])\n        \n        # Calculate covariance matrix\n        centered_points = contour_points - np.array([cx, cy])\n        cov_matrix = np.cov(centered_points.T)\n        \n        # Calculate eigenvalues and eigenvectors\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        \n        # Sort by eigenvalues in descending order\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        \n        # Main direction (eigenvector corresponding to maximum eigenvalue)\n        main_direction = eigenvectors[:, 0]\n        \n        # Project to main and secondary directions\n        proj_main = np.dot(centered_points, main_direction)\n        proj_secondary = np.dot(centered_points, eigenvectors[:, 1])\n        \n        # Calculate projection boundaries\n        min_main = np.min(proj_main)\n        max_main = np.max(proj_main)\n        min_secondary = np.min(proj_secondary)\n        max_secondary = np.max(proj_secondary)\n        \n        # Calculate actual long and short axis lengths\n        length_main = max_main - min_main\n        length_secondary = max_secondary - min_secondary\n        \n        # Ensure long axis corresponds to longer direction and save correct projection boundaries\n        if length_main >= length_secondary:\n            w_obb = length_main\n            h_obb = length_secondary\n            angle = np.arctan2(main_direction[1], main_direction[0]) * 180.0 / np.pi\n            # Long axis is main direction\n            long_direction = main_direction\n            short_direction = eigenvectors[:, 1]\n            min_long_proj = min_main\n            max_long_proj = max_main\n            min_short_proj = min_secondary\n            max_short_proj = max_secondary\n        else:\n            w_obb = length_secondary  \n            h_obb = length_main\n            secondary_direction = eigenvectors[:, 1]\n            angle = np.arctan2(secondary_direction[1], secondary_direction[0]) * 180.0 / np.pi\n            # Long axis is secondary direction\n            long_direction = eigenvectors[:, 1]\n            short_direction = main_direction\n            min_long_proj = min_secondary\n            max_long_proj = max_secondary\n            min_short_proj = min_main\n            max_short_proj = max_main\n        \n        # Normalize angle to [0, 180)\n        angle = ((angle % 180.0) + 180.0) % 180.0\n        \n        components.append({\n            "contour": cnt,\n            "rect": rect,\n            "box": box,\n            "area_px": float(area),\n            "peri_px": float(peri),\n            "center": (int(cx), int(cy)),  # Centroid calculated using PCA\n            "pca_center": (cx, cy),        # Save precise PCA centroid\n            "angle": float(angle),\n            "length_px": float(w_obb),\n            "width_px": float(h_obb),\n            # Save projection boundaries for correct bounding box drawing\n            "min_long_proj": float(min_long_proj),\n            "max_long_proj": float(max_long_proj),\n            "min_short_proj": float(min_short_proj),\n            "max_short_proj": float(max_short_proj),\n        })\n    \n    return components\n\n\ndef compute_color_metrics(img_bgr: np.ndarray, mask: np.ndarray) -> Tuple[float, float, float, int, int, int, float, float]:\n    """Compute mean RGB / HSV and simple color indices in a mask region."""\n    mean_bgr = cv2.mean(img_bgr, mask=mask)\n    mean_b, mean_g, mean_b = mean_bgr[0], mean_bgr[1], mean_bgr[2]\n\n    rgb = np.array([[[mean_r, mean_g, mean_b]]], dtype=np.uint8)\n    hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)[0, 0]\n    h, s, v = int(hsv[0]), int(hsv[1]), int(hsv[2])\n\n    denom = (mean_r + mean_g + mean_b + 1e-6)\n    green_index = (2.0 * mean_g - mean_r - mean_b) / denom\n    brown_index = (mean_r - mean_b) / denom\n    return mean_r, mean_g, mean_b, h, s, v, green_index, brown_index\n\n\ndef compute_metrics(\n    img_bgr: np.ndarray,\n    components: List[Dict[str, Any]],\n    px_per_mm: float,\n) -> pd.DataFrame:\n    """Calculate morphological and color metrics for each sample"""\n    rows: List[Dict[str, Any]] = []\n\n    for i, comp in enumerate(components, start=1):\n        # Use new length_px and width_px fields\n        length_mm = comp["length_px"] / px_per_mm\n        width_mm = comp["width_px"] / px_per_mm\n        area_mm2 = comp["area_px"] / (px_per_mm * px_per_mm)\n        perimeter_mm = comp["peri_px"] / px_per_mm\n\n        aspect_ratio = length_mm / (width_mm + 1e-6)\n        \n        # Calculate circularity (4\u03c0*area/perimeter\xb2)\n        circularity = (4.0 * np.pi * area_mm2) / (perimeter_mm * perimeter_mm + 1e-6)\n\n        # Calculate color metrics\n        mask_single = np.zeros(img_bgr.shape[:2], dtype=np.uint8)\n        cv2.drawContours(mask_single, [comp["contour"]], -1, 255, thickness=-1)\n        mean_r, mean_g, mean_b, h, s, v, gi, bi = compute_color_metrics(img_bgr, mask_single)\n        \n        rows.append(\n            {\n                "id": f"S{i}",\n                "label": f"S{i}",\n                "centerX_px": int(comp["center"][0]),\n                "centerY_px": int(comp["center"][1]),\n                "lengthMm": round(length_mm, 2),\n                "length_mm": round(length_mm, 2),\n                "widthMm": round(width_mm, 2),\n                "width_mm": round(width_mm, 2),\n                "areaMm2": round(area_mm2, 2),\n                "area_mm2": round(area_mm2, 2),\n                "perimeterMm": round(perimeter_mm, 2),\n                "perimeter_mm": round(perimeter_mm, 2),\n                "aspectRatio": round(aspect_ratio, 2),\n                "aspect_ratio": round(aspect_ratio, 2),\n                "circularity": round(circularity, 3),\n                "angleDeg": round(float(comp["angle"]), 1),\n                "angle_deg": round(float(comp["angle"]), 1),\n                "meanR": int(round(mean_r)),\n                "meanG": int(round(mean_g)),\n                "meanB": int(round(mean_b)),\n                "hue": h,\n                "saturation": s,\n                "value": v,\n                "greenIndex": round(float(gi), 3),\n                "green_index": round(float(gi), 3),\n                "brownIndex": round(float(bi), 3),\n                "brown_index": round(float(bi), 3),\n            }\n        )\n\n    if not rows:\n        return pd.DataFrame()\n    return pd.DataFrame(rows)\n\n\ndef render_overlay(\n    img_bgr: np.ndarray,\n    px_per_mm: float,\n    ref: Tuple[Optional[Tuple[int, int]], Optional[str]],\n    components: List[Dict[str, Any]],\n    ref_bbox: Optional[Tuple[int, int, int, int]] = None,\n    enhanced: bool = False,\n    tip_detection_sensitivity: float = 0.7,\n) -> np.ndarray:\n    """Draw reference + sample annotations with reliable visualization."""\n    return render_overlay_original(img_bgr, px_per_mm, ref, components, ref_bbox)\n\n\ndef render_overlay_original(\n    img_bgr: np.ndarray,\n    px_per_mm: float,\n    ref: Tuple[Optional[Tuple[int, int]], Optional[str]],\n    components: List[Dict[str, Any]],\n    ref_bbox: Optional[Tuple[int, int, int, int]] = None,\n) -> np.ndarray:\n    """Complete visualization rendering showing contours, bounding boxes, and axes"""\n    out = img_bgr.copy()\n\n    # Draw reference (red rectangle)\n    ref_center, ref_type = ref\n    if ref_bbox is not None:\n        x, y, w, h = ref_bbox\n        cv2.rectangle(out, (int(x), int(y)), (int(x + w), int(y + h)), (0, 0, 255), 2)\n        cv2.putText(\n            out,\n            "REF",\n            (int(x), int(y) - 10),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.6,\n            (0, 0, 255),\n            2,\n            cv2.LINE_AA,\n        )\n\n    # Draw sample objects (complete annotations)\n    for i, comp in enumerate(components, start=1):\n        # 1. Draw complete contour (blue, bold)\n        cv2.drawContours(out, [comp["contour"]], -1, (255, 0, 0), 3)\n        \n        # 2. Draw corrected OBB bounding box\n        # Use actual projection boundaries\n        corners = []\n        \n        # Get saved projection boundaries\n        min_long_proj = comp["min_long_proj"]\n        max_long_proj = comp["max_long_proj"]\n        min_short_proj = comp["min_short_proj"]\n        max_short_proj = comp["max_short_proj"]\n        \n        # Get PCA center\n        cx, cy = comp["pca_center"]\n        angle_deg = comp["angle"]\n        angle_rad = np.radians(angle_deg)\n        \n        # Get long and short axis direction vectors\n        long_dir = np.array([np.cos(angle_rad), np.sin(angle_rad)])\n        short_dir = np.array([-np.sin(angle_rad), np.cos(angle_rad)])\n        \n        # Build corner points using actual projection boundaries\n        for long_proj, short_proj in [(max_long_proj, max_short_proj),    # top-right\n                                      (min_long_proj, max_short_proj),    # top-left  \n                                      (min_long_proj, min_short_proj),    # bottom-left\n                                      (max_long_proj, min_short_proj)]:   # bottom-right\n            corner_point = np.array([cx, cy]) + long_proj * long_dir + short_proj * short_dir\n            corners.append([int(corner_point[0]), int(corner_point[1])])\n        \n        # Draw OBB\n        corners = np.array(corners, dtype=np.int32)\n        cv2.drawContours(out, [corners], -1, (255, 0, 0), 2)\n        \n        # Draw long and short axes (boundary lines)\n        edge_mids = []\n        for edge_idx in range(4):\n            next_edge_idx = (edge_idx + 1) % 4\n            mid_x = (corners[edge_idx][0] + corners[next_edge_idx][0]) / 2\n            mid_y = (corners[edge_idx][1] + corners[next_edge_idx][1]) / 2\n            edge_mids.append((int(mid_x), int(mid_y)))\n        \n        # Determine which edge is longest\n        edge_lengths = []\n        for edge_idx in range(4):\n            next_edge_idx = (edge_idx + 1) % 4\n            length = np.sqrt((corners[next_edge_idx][0] - corners[edge_idx][0])**2 + (corners[next_edge_idx][1] - corners[edge_idx][1])**2)\n            edge_lengths.append(length)\n        \n        max_edge_idx = np.argmax(edge_lengths)\n        opposite_edge_idx = (max_edge_idx + 2) % 4\n        \n        # Draw long axis\n        long_mid1 = edge_mids[max_edge_idx]\n        long_mid2 = edge_mids[opposite_edge_idx]\n        cv2.line(out, long_mid1, long_mid2, (255, 0, 0), 3)\n        \n        # Draw short axis\n        short_edge1_idx = (max_edge_idx + 1) % 4\n        short_edge2_idx = (max_edge_idx + 3) % 4\n        short_mid1 = edge_mids[short_edge1_idx]\n        short_mid2 = edge_mids[short_edge2_idx]\n        cv2.line(out, short_mid1, short_mid2, (255, 0, 0), 2)\n\n        # 4. Draw center point and label\n        label_cx, label_cy = comp["pca_center"]\n        cv2.circle(out, (int(label_cx), int(label_cy)), 15, (0, 0, 0), -1)\n        cv2.putText(\n            out,\n            f"S{i}",\n            (int(label_cx) - 10, int(label_cy) + 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 255, 255),\n            2,\n            cv2.LINE_AA,\n        )\n\n    return cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n\n\ndef analyze(\n    image: Optional[np.ndarray],\n    sample_type: str = "leaves",\n    expected_count: int = 0,\n    ref_mode: str = "auto",\n    ref_size_mm: float = 25.0,\n    min_area_px: float = 100,\n    max_area_px: float = 1e9,\n    color_tol: int = 40,\n    hsv_low_h: int = 35,\n    hsv_high_h: int = 85,\n) -> Tuple[Optional[np.ndarray], pd.DataFrame, Optional[str], List[Dict[str, Any]], Dict[str, Any]]:\n    """\n    Main function for image quantification analysis\n    \n    Parameters:\n        image: RGB image\n        ref_size_mm: Reference object bounding box size in millimeters\n        other parameters: Basic analysis parameters\n    \n    Returns:\n        overlay: Annotated RGB image\n        df: Measurement results DataFrame\n        csv_path: CSV file path\n        js: JSON format results\n        state_dict: State dictionary\n    """\n    try:\n        if image is None:\n            return None, pd.DataFrame(), None, [], {}\n        \n        # Convert to BGR\n        img_rgb = np.array(image)\n        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n        \n        # Moderate downsampling\n        img_bgr, scale = downscale_bgr(img_bgr)\n        \n        # Detect reference object (first object in upper-left)\n        px_per_mm, ref_center, ref_type, ref_bbox = detect_reference(img_bgr, ref_mode, ref_size_mm)\n        \n        # Segment all samples (excluding reference)\n        comps = segment(\n            img_bgr,\n            sample_type=sample_type,\n            hsv_low_h=hsv_low_h,\n            hsv_high_h=hsv_high_h,\n            color_tol=color_tol,\n            min_area_px=min_area_px,\n            max_area_px=max_area_px,\n        )\n        \n        # Calculate measurement metrics\n        df = compute_metrics(img_bgr, comps, px_per_mm)\n        \n        # Draw annotated image (REF in red, S1-Sn in blue)\n        overlay = render_overlay(\n            img_bgr.copy(), \n            px_per_mm, \n            (ref_center, ref_type), \n            comps, \n            ref_bbox\n        )\n        \n        # Save CSV\n        csv = df.to_csv(index=False)\n        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".csv", mode=\'w\')\n        tmp.write(csv)\n        tmp.close()\n        \n        # Convert to JSON\n        js = df.to_dict(orient="records")\n        \n        # Add reference information to results\n        if ref_center and ref_bbox:\n            ref_x, ref_y, ref_w, ref_h = ref_bbox\n            ref_size_detected = max(ref_w, ref_h) / px_per_mm\n            ref_info = {\n                "id": "REF",\n                "label": "REF", \n                "is_reference": True,\n                "lengthMm": round(ref_size_detected, 2),\n                "widthMm": round(ref_size_detected, 2),\n                "areaMm2": round((ref_w * ref_h) / (px_per_mm * px_per_mm), 2),\n                "centerX_px": ref_center[0],\n                "centerY_px": ref_center[1]\n            }\n            js.insert(0, ref_info)  # Reference object first\n\n        # State information\n        sample_objects = [j for j in js if not j.get("is_reference", False)]\n        \n        state_dict: Dict[str, Any] = {\n            "ref_size_mm": ref_size_mm,\n            "num_samples": len(sample_objects),\n            "px_per_mm": px_per_mm,\n            "reference_detected": ref_center is not None,\n        }\n\n        return overlay, df, tmp.name, js, state_dict\n    except Exception as e:\n        print(f"Analysis failed: {e}")\n        error_state = {\n            "error": str(e),\n            "reference_detected": False,\n            "num_samples": 0,\n        }\n        return None, pd.DataFrame(), None, [], error_state\n\n\n# -----------------------------\n# Standalone Usage Example\n# -----------------------------\n\ndef main():\n    """Example usage of the image quantification tool"""\n    import sys\n    from PIL import Image\n    \n    if len(sys.argv) < 2:\n        print("Usage: python image_quantification.py <image_path> [ref_size_mm]")\n        print("Example: python image_quantification.py plant_sample.jpg 25.0")\n        return\n    \n    image_path = sys.argv[1]\n    ref_size_mm = float(sys.argv[2]) if len(sys.argv) > 2 else 25.0\n    \n    try:\n        # Load image\n        pil_image = Image.open(image_path)\n        if pil_image.mode != \'RGB\':\n            pil_image = pil_image.convert(\'RGB\')\n        image_array = np.array(pil_image)\n        \n        print(f"Analyzing image: {image_path}")\n        print(f"Reference size: {ref_size_mm} mm")\n        \n        # Run analysis\n        overlay, df, csv_path, js, state = analyze(\n            image=image_array,\n            sample_type="leaves",\n            ref_size_mm=ref_size_mm,\n            ref_mode="auto",\n            min_area_px=100,\n        )\n        \n        if overlay is not None:\n            # Save overlay image\n            output_image_path = image_path.replace(\'.\', \'_annotated.\')\n            if output_image_path == image_path:\n                output_image_path = image_path + \'_annotated.jpg\'\n            \n            overlay_pil = Image.fromarray(overlay)\n            overlay_pil.save(output_image_path)\n            print(f"Annotated image saved to: {output_image_path}")\n            \n            # Save results\n            output_csv_path = image_path.replace(\'.\', \'_results.\')\n            if output_csv_path == image_path:\n                output_csv_path = image_path + \'_results.csv\'\n            df.to_csv(output_csv_path, index=False)\n            print(f"Results saved to: {output_csv_path}")\n            \n            # Print summary\n            print(f"\\nAnalysis Summary:")\n            print(f"Reference detected: {state[\'reference_detected\']}")\n            print(f"Number of samples: {state[\'num_samples\']}")\n            print(f"Scale: {state[\'px_per_mm\']:.2f} px/mm")\n            \n            if len(df) > 0:\n                print(f"\\nSample Measurements:")\n                print(df[[\'id\', \'lengthMm\', \'widthMm\', \'areaMm2\', \'aspectRatio\', \'greenIndex\']].to_string(index=False))\n                \n        else:\n            print("Analysis failed. Check the image quality and reference object.")\n            \n    except Exception as e:\n        print(f"Error: {e}")\n        import traceback\n        traceback.print_exc()\n\n\nif __name__ == "__main__":\n    main()\n\n\n# -----------------------------\n# Python API Usage Example\n# -----------------------------\n\ndef example_python_api():\n    """Example of using the tool as a Python library"""\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    \n    # Load your image\n    image = Image.open("your_plant_image.jpg")\n    \n    # Run analysis\n    overlay, df, csv_path, js, state = analyze(\n        image=np.array(image),\n        sample_type="leaves",\n        ref_size_mm=25.0,  # Your reference object size\n        ref_mode="auto",\n        min_area_px=100,\n    )\n    \n    # Display results\n    if overlay is not None:\n        plt.figure(figsize=(12, 6))\n        \n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.title("Original Image")\n        plt.axis(\'off\')\n        \n        plt.subplot(1, 2, 2)\n        plt.imshow(overlay)\n        plt.title("Analysis Results")\n        plt.axis(\'off\')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print detailed results\n        print("\\nDetailed Measurements:")\n        print(df.to_string(index=False))\n        \n        # Save to CSV for further analysis\n        df.to_csv("plant_measurements.csv", index=False)\n        print("\\nResults saved to \'plant_measurements.csv\'")\n\n\n## Usage Guide\n\n### Command Line Usage\n```bash\n# Basic usage\npython image_quantification.py plant_image.jpg\n\n# Specify reference object size\npython image_quantification.py plant_image.jpg 25.0\n\n# Process multiple images\nfor img in *.jpg; do\n    python image_quantification.py "$img" 25.0\ndone\n'})}),"\n",(0,a.jsx)(n.h3,{id:"python-api-usage",children:"Python API Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from image_quantification import analyze\nfrom PIL import Image\nimport numpy as np\n\n# Load image\nimage = Image.open("plant.jpg")\nimage_array = np.array(image)\n\n# Run analysis\noverlay, df, csv_path, js, state = analyze(\n    image=image_array,\n    sample_type="leaves",\n    ref_size_mm=25.0,\n    ref_mode="auto",\n    min_area_px=100,\n)\n\n# Access results\nif overlay is not None:\n    # Save annotated image\n    Image.fromarray(overlay).save("result.jpg")\n    \n    # Save measurements\n    df.to_csv("measurements.csv", index=False)\n    \n    # Print summary\n    print(f"Detected {state[\'num_samples\']} samples")\n    print(f"Scale: {state[\'px_per_mm\']:.2f} px/mm")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"image-capture-guidelines",children:"Image Capture Guidelines"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lighting"}),": Ensure even, diffuse lighting to minimize shadows"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Background"}),": Use a uniform, contrasting background"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reference Object"}),": Place a known-size object in the upper-left corner"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera Position"}),": Keep camera perpendicular to the sample plane"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Use high resolution (at least 1920x1080) for better accuracy"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"reference-object-selection",children:"Reference Object Selection"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use a square or near-square object"}),"\n",(0,a.jsx)(n.li,{children:"Common choices: calibration cards, coins, printed squares"}),"\n",(0,a.jsx)(n.li,{children:"Measure the actual size accurately (in millimeters)"}),"\n",(0,a.jsx)(n.li,{children:"Place it consistently in the upper-left corner"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"parameter-tuning",children:"Parameter Tuning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# For small leaves (1-5 cm)\nmin_area_px = 50\nref_size_mm = 25.0\n\n# For large leaves (5-20 cm)\nmin_area_px = 200\nref_size_mm = 50.0\n\n# For seeds/grains\nmin_area_px = 10\nref_size_mm = 10.0\n"})}),"\n",(0,a.jsx)(n.h3,{id:"quality-control",children:"Quality Control"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual Inspection"}),": Always check the annotated output image"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reasonable Values"}),": Verify measurements are within expected ranges"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sample Count"}),": Ensure the number of detected samples matches reality"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scale Validation"}),": Check that px_per_mm is reasonable for your setup"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsx)(n.h4,{id:"1-reference-object-not-detected",children:"1. Reference Object Not Detected"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Symptoms"}),": ",(0,a.jsx)(n.code,{children:"reference_detected: False"}),", scale = 4.0 px/mm"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Ensure reference object is in the upper-left corner"}),"\n",(0,a.jsx)(n.li,{children:"Check that reference object is clearly visible"}),"\n",(0,a.jsx)(n.li,{children:"Verify reference object size is reasonable (not too small/large)"}),"\n",(0,a.jsxs)(n.li,{children:["Adjust ",(0,a.jsx)(n.code,{children:"min_area_px"})," and ",(0,a.jsx)(n.code,{children:"max_area_px"})," parameters"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"2-too-few-samples-detected",children:"2. Too Few Samples Detected"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Symptoms"}),": Fewer objects than expected"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Decrease ",(0,a.jsx)(n.code,{children:"min_area_px"})," to detect smaller objects"]}),"\n",(0,a.jsx)(n.li,{children:"Check if objects are touching (may be merged)"}),"\n",(0,a.jsx)(n.li,{children:"Verify background contrast is sufficient"}),"\n",(0,a.jsx)(n.li,{children:"Ensure lighting is even"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"3-too-many-samples-detected",children:"3. Too Many Samples Detected"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Symptoms"}),": More objects than expected, noise detected"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Increase ",(0,a.jsx)(n.code,{children:"min_area_px"})," to filter out small noise"]}),"\n",(0,a.jsx)(n.li,{children:"Improve image quality (reduce noise)"}),"\n",(0,a.jsx)(n.li,{children:"Check for background artifacts"}),"\n",(0,a.jsx)(n.li,{children:"Use morphological operations to clean up"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"4-inaccurate-measurements",children:"4. Inaccurate Measurements"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Symptoms"}),": Measurements don't match manual measurements"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Verify reference object size is correct"}),"\n",(0,a.jsx)(n.li,{children:"Ensure camera is perpendicular to sample plane"}),"\n",(0,a.jsx)(n.li,{children:"Check for lens distortion (use calibration if needed)"}),"\n",(0,a.jsx)(n.li,{children:"Verify lighting is even (no shadows)"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"5-color-metrics-are-off",children:"5. Color Metrics Are Off"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Symptoms"}),": Unexpected RGB/HSV values or indices"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check white balance in original image"}),"\n",(0,a.jsx)(n.li,{children:"Ensure consistent lighting conditions"}),"\n",(0,a.jsx)(n.li,{children:"Verify sample type parameter is appropriate"}),"\n",(0,a.jsx)(n.li,{children:"Consider using color calibration cards"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"error-messages",children:"Error Messages"}),"\n",(0,a.jsx)(n.h4,{id:"analysis-failed-error-message",children:'"Analysis failed: [error message]"'}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check image file format (should be JPEG, PNG, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Verify image is not corrupted"}),"\n",(0,a.jsx)(n.li,{children:"Ensure all dependencies are installed"}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"no-objects-found",children:'"No objects found"'}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Increase ",(0,a.jsx)(n.code,{children:"min_area_px"})," or decrease ",(0,a.jsx)(n.code,{children:"max_area_px"})]}),"\n",(0,a.jsx)(n.li,{children:"Check image quality and contrast"}),"\n",(0,a.jsx)(n.li,{children:"Verify objects are present in the image"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"for-large-datasets",children:"For Large Datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Process multiple images efficiently\nimport glob\n\nimage_files = glob.glob("*.jpg")\nresults = []\n\nfor img_path in image_files:\n    image = Image.open(img_path)\n    overlay, df, _, js, state = analyze(\n        image=np.array(image),\n        ref_size_mm=25.0,\n        min_area_px=100,\n    )\n    if df is not None and len(df) > 0:\n        df[\'filename\'] = img_path\n        results.append(df)\n\n# Combine all results\nall_results = pd.concat(results, ignore_index=True)\nall_results.to_csv("batch_results.csv", index=False)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Images are automatically downscaled to MAX_SIDE (1024px)"}),"\n",(0,a.jsx)(n.li,{children:"For very large datasets, process in batches"}),"\n",(0,a.jsxs)(n.li,{children:["Use ",(0,a.jsx)(n.code,{children:"tempfile"})," for intermediate storage"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"output-formats",children:"Output Formats"}),"\n",(0,a.jsx)(n.h3,{id:"csv-columns",children:"CSV Columns"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"id"}),": Sample identifier (S1, S2, ...)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"lengthMm"}),", ",(0,a.jsx)(n.code,{children:"widthMm"}),": Dimensions in millimeters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"areaMm2"}),": Area in square millimeters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"perimeterMm"}),": Perimeter in millimeters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"aspectRatio"}),": Length/Width ratio"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"circularity"}),": Shape compactness (1 = perfect circle)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"angleDeg"}),": Orientation angle in degrees"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"meanR"}),", ",(0,a.jsx)(n.code,{children:"meanG"}),", ",(0,a.jsx)(n.code,{children:"meanB"}),": Average RGB values"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"hue"}),", ",(0,a.jsx)(n.code,{children:"saturation"}),", ",(0,a.jsx)(n.code,{children:"value"}),": HSV color space"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"greenIndex"}),": Vegetation index (higher = greener)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"brownIndex"}),": Browning index (higher = browner)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"json-structure",children:"JSON Structure"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "id": "REF",\n    "is_reference": true,\n    "lengthMm": 25.0,\n    "widthMm": 25.0,\n    "areaMm2": 625.0\n  },\n  {\n    "id": "S1",\n    "lengthMm": 45.2,\n    "widthMm": 23.1,\n    "areaMm2": 820.5,\n    "greenIndex": 0.45,\n    ...\n  }\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-usage",children:"Advanced Usage"}),"\n",(0,a.jsx)(n.h3,{id:"custom-analysis-pipeline",children:"Custom Analysis Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from image_quantification import analyze, segment, compute_metrics\n\n# Step 1: Load and preprocess\nimage = Image.open("plant.jpg")\nimg_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n# Step 2: Custom segmentation\ncomponents = segment(\n    img_bgr,\n    sample_type="leaves",\n    min_area_px=50,\n)\n\n# Step 3: Manual reference detection\npx_per_mm = 4.0  # Custom scale\n\n# Step 4: Compute metrics\ndf = compute_metrics(img_bgr, components, px_per_mm)\n\n# Step 5: Custom visualization\noverlay = render_overlay(img_bgr, px_per_mm, (None, None), components)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"integration-with-other-tools",children:"Integration with Other Tools"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Export to Excel\ndf.to_excel(\"results.xlsx\", index=False)\n\n# Upload to database\nimport sqlite3\nconn = sqlite3.connect('measurements.db')\ndf.to_sql('plant_data', conn, if_exists='append')\n\n# Generate reports\nimport matplotlib.pyplot as plt\ndf.boxplot(column=['lengthMm', 'widthMm'])\nplt.title('Size Distribution')\nplt.savefig('distribution.png')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"This standalone image quantification tool provides a powerful, easy-to-use solution for agricultural image analysis. Key benefits:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"No internet required"}),": Fully local processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fast and efficient"}),": Optimized for agricultural images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Comprehensive metrics"}),": Morphology + color analysis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Flexible output"}),": CSV, JSON, visual overlays"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Easy integration"}),": Both CLI and Python API"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The tool is particularly suitable for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Plant phenotyping studies"}),"\n",(0,a.jsx)(n.li,{children:"Quality control in agriculture"}),"\n",(0,a.jsx)(n.li,{children:"Research projects requiring precise measurements"}),"\n",(0,a.jsx)(n.li,{children:"Educational purposes in computer vision"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For questions or improvements, refer to the code comments or extend the modular functions provided."})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var r=i(96540);const a={},t=r.createContext(a);function o(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},86113:e=>{e.exports=JSON.parse('{"permalink":"/zh-Hans/blog/local-image-quantification-tutorial","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-07-13-local-image-quantification-tutorial.md","source":"@site/blog/2025-07-13-local-image-quantification-tutorial.md","title":"Local Image Quantification Tool - A Standalone Python Solution for Agricultural Analysis","description":"Project Overview","date":"2025-07-13T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/zh-Hans/blog/tags/python","description":"Python programming language"},{"inline":false,"label":"Computer Vision","permalink":"/zh-Hans/blog/tags/computer-vision","description":"Computer vision and image processing"},{"inline":false,"label":"Agriculture","permalink":"/zh-Hans/blog/tags/agriculture","description":"Agricultural technology and applications"},{"inline":false,"label":"Image Analysis","permalink":"/zh-Hans/blog/tags/image-analysis","description":"Image analysis techniques"},{"inline":false,"label":"OpenCV","permalink":"/zh-Hans/blog/tags/opencv","description":"Open Source Computer Vision Library"}],"readingTime":18.04,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"Ph.D. Candidate @ SHZU @CAS-Cemps","url":"https://github.com/smiler488","page":{"permalink":"/zh-Hans/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"/zh-Hans/img/cv_person.png","key":"liangchao"}],"frontMatter":{"slug":"local-image-quantification-tutorial","title":"Local Image Quantification Tool - A Standalone Python Solution for Agricultural Analysis","authors":["liangchao"],"tags":["Python","Computer Vision","Agriculture","Image Analysis","OpenCV"],"image":"/img/blog-default.jpg"},"unlisted":false,"prevItem":{"title":"Guide to Local AI Agent","permalink":"/zh-Hans/blog/local-ai-agent-deployment"},"nextItem":{"title":"Guide for scientific papers","permalink":"/zh-Hans/blog/academic-paper-publication-guide"}}')}}]);