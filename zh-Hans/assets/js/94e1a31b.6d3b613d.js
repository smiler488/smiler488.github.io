"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[191],{28453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>l});var r=t(96540);const i={},a=r.createContext(i);function o(n){const e=r.useContext(a);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),r.createElement(a.Provider,{value:e},n.children)}},29494:n=>{n.exports=JSON.parse('{"permalink":"/zh-Hans/blog/local-llm-training-guide-en","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-11-local-llm-training-guide.md","source":"@site/blog/2024-01-11-local-llm-training-guide.md","title":"Guide to Local LLM","description":"Project Overview","date":"2024-01-11T00:00:00.000Z","tags":[{"inline":false,"label":"LLM","permalink":"/zh-Hans/blog/tags/llm","description":"Large Language Models"},{"inline":false,"label":"AI","permalink":"/zh-Hans/blog/tags/ai","description":"Artificial Intelligence"},{"inline":false,"label":"Training","permalink":"/zh-Hans/blog/tags/training","description":"Model training and optimization"},{"inline":false,"label":"Fine-tuning","permalink":"/zh-Hans/blog/tags/fine-tuning","description":"Model fine-tuning techniques"},{"inline":false,"label":"Local Deployment","permalink":"/zh-Hans/blog/tags/local-deployment","description":"Local model deployment"},{"inline":false,"label":"Machine Learning","permalink":"/zh-Hans/blog/tags/machine-learning","description":"Machine learning techniques and applications"}],"readingTime":24.79,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"Ph.D. Candidate @ SHZU @CAS-Cemps","url":"https://github.com/smiler488","page":{"permalink":"/zh-Hans/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"/zh-Hans/img/cv_person.png","key":"liangchao"}],"frontMatter":{"slug":"local-llm-training-guide-en","title":"Guide to Local LLM","authors":["liangchao"],"tags":["LLM","AI","training","fine-tuning","local deployment","machine learning"],"image":"/img/blog-default.jpg"},"unlisted":false,"prevItem":{"title":"Guide to 3D Reconstruction with AI","permalink":"/zh-Hans/blog/hunyuan3d-plant-reconstruction-guide"},"nextItem":{"title":"Canopy Photosynthesis Modeling","permalink":"/zh-Hans/blog/canopy-photosynthesis-modeling-en"}}')},81877:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var r=t(29494),i=t(74848),a=t(28453);const o={slug:"local-llm-training-guide-en",title:"Guide to Local LLM",authors:["liangchao"],tags:["LLM","AI","training","fine-tuning","local deployment","machine learning"],image:"/img/blog-default.jpg"},l=void 0,s={authorsImageUrls:[void 0]},d=[{value:"Project Overview",id:"project-overview",level:2},{value:"Technical Workflow Overview",id:"technical-workflow-overview",level:2},{value:"Quick Start (10 Minutes)",id:"quick-start-10-minutes",level:2},{value:"Installation and First Run",id:"installation-and-first-run",level:3},{value:"Quick Environment Check",id:"quick-environment-check",level:3},{value:"What&#39;s Next?",id:"whats-next",level:3},{value:"Environment Setup",id:"environment-setup",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Environment Setup",id:"software-environment-setup",level:3},{value:"Model Deployment",id:"model-deployment",level:2},{value:"Quick Deployment with Ollama",id:"quick-deployment-with-ollama",level:3},{value:"High-Performance Deployment with vLLM",id:"high-performance-deployment-with-vllm",level:3},{value:"Using Text Generation WebUI",id:"using-text-generation-webui",level:3},{value:"Data Preparation and Preprocessing",id:"data-preparation-and-preprocessing",level:2},{value:"Dataset Formats",id:"dataset-formats",level:3},{value:"Data Preprocessing Script",id:"data-preprocessing-script",level:3},{value:"LoRA Fine-tuning",id:"lora-fine-tuning",level:2},{value:"Efficient Fine-tuning with Unsloth",id:"efficient-fine-tuning-with-unsloth",level:3},{value:"Professional Fine-tuning with Axolotl",id:"professional-fine-tuning-with-axolotl",level:3},{value:"Full Parameter Fine-tuning",id:"full-parameter-fine-tuning",level:2},{value:"Large Model Training with DeepSpeed",id:"large-model-training-with-deepspeed",level:3},{value:"Pre-training",id:"pre-training",level:2},{value:"Pre-training from Scratch",id:"pre-training-from-scratch",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Automatic Evaluation Metrics",id:"automatic-evaluation-metrics",level:3},{value:"Human Evaluation Framework",id:"human-evaluation-framework",level:3},{value:"Model Quantization and Optimization",id:"model-quantization-and-optimization",level:2},{value:"GPTQ Quantization",id:"gptq-quantization",level:3},{value:"AWQ Quantization",id:"awq-quantization",level:3},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Docker Containerization",id:"docker-containerization",level:3},{value:"Kubernetes Deployment",id:"kubernetes-deployment",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:2},{value:"Prometheus Monitoring",id:"prometheus-monitoring",level:3},{value:"Structured Logging",id:"structured-logging",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:3},{value:"Security Considerations",id:"security-considerations",level:3},{value:"Cost Control",id:"cost-control",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"1. CUDA Out of Memory Error",id:"1-cuda-out-of-memory-error",level:3},{value:"2. Model Loading Fails with &quot;Safetensors&quot; Error",id:"2-model-loading-fails-with-safetensors-error",level:3},{value:"3. Flash Attention Installation Fails",id:"3-flash-attention-installation-fails",level:3},{value:"4. Tokenizer Padding Issues",id:"4-tokenizer-padding-issues",level:3},{value:"5. DeepSpeed Configuration Errors",id:"5-deepspeed-configuration-errors",level:3},{value:"6. Training Loss Not Decreasing",id:"6-training-loss-not-decreasing",level:3},{value:"7. Slow Training Speed",id:"7-slow-training-speed",level:3},{value:"8. vLLM Initialization Fails",id:"8-vllm-initialization-fails",level:3},{value:"9. Hugging Face Hub Authentication",id:"9-hugging-face-hub-authentication",level:3},{value:"10. Multi-GPU Training Issues",id:"10-multi-gpu-training-issues",level:3},{value:"Complete Setup Validation",id:"complete-setup-validation",level:2}];function c(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(e.p,{children:"This comprehensive guide covers the complete process of deploying, training, and fine-tuning large language models in local environments, from environment setup to production deployment."}),"\n",(0,i.jsx)(e.h1,{id:"guide-to-local-llm-deployment-training-and-fine-tuning",children:"Guide to Local LLM Deployment, Training and Fine-tuning"}),"\n",(0,i.jsx)(e.h2,{id:"technical-workflow-overview",children:"Technical Workflow Overview"}),"\n",(0,i.jsx)(e.mermaid,{value:"graph TD\n    A[Environment Setup] --\x3e B[Hardware & Software Configuration]\n    B --\x3e C[Model Deployment]\n    C --\x3e D[Data Preparation]\n    D --\x3e E[Fine-tuning Strategy]\n    E --\x3e F[LoRA Fine-tuning]\n    E --\x3e G[Full Parameter Training]\n    F --\x3e H[Model Evaluation]\n    G --\x3e H\n    H --\x3e I[Production Deployment]\n    I --\x3e J[Performance Monitoring]\n    \n    B --\x3e B1[GPU/CPU Requirements]\n    B --\x3e B2[Software Dependencies]\n    \n    C --\x3e C1[Ollama Deployment]\n    C --\x3e C2[vLLM High-Performance]\n    C --\x3e C3[Text Generation WebUI]\n    \n    D --\x3e D1[Dataset Formatting]\n    D --\x3e D2[Data Preprocessing]\n    D --\x3e D3[Quality Validation]\n    \n    E --\x3e E1[Parameter Selection]\n    E --\x3e E2[Hyperparameter Tuning]\n    \n    F --\x3e F1[Unsloth Framework]\n    F --\x3e F2[Axolotl Configuration]\n    \n    G --\x3e G1[DeepSpeed Optimization]\n    G --\x3e G2[Memory Management]\n    \n    H --\x3e H1[Accuracy Metrics]\n    H --\x3e H2[Performance Benchmarks]\n    \n    I --\x3e I1[API Integration]\n    I --\x3e I2[Scalability Testing]"}),"\n",(0,i.jsx)(e.p,{children:"This workflow illustrates the end-to-end process for local LLM training and deployment, highlighting key decision points and alternative approaches at each stage."}),"\n",(0,i.jsx)(e.h2,{id:"quick-start-10-minutes",children:"Quick Start (10 Minutes)"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"For beginners who want to test LLM deployment before diving into training:"})}),"\n",(0,i.jsx)(e.p,{children:"This quick start gets you running a local LLM using Ollama, which is the easiest way to experiment with LLMs on your local machine without any complex setup."}),"\n",(0,i.jsx)(e.h3,{id:"installation-and-first-run",children:"Installation and First Run"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# 1. Install Ollama (works on macOS, Linux, Windows)\n# macOS/Linux:\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows: Download from https://ollama.com/download\n\n# 2. Download and run your first model (7B model, ~4GB download)\nollama pull llama2:7b\n\n# 3. Test the model\nollama run llama2:7b\n\n# Try asking: "Explain what machine learning is in simple terms"\n'})}),"\n",(0,i.jsx)(e.h3,{id:"quick-environment-check",children:"Quick Environment Check"}),"\n",(0,i.jsx)(e.p,{children:"Before proceeding with training, verify your system is ready:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# save as check_llm_env.py\nimport sys\nimport subprocess\n\ndef check_environment():\n    """Check if your system is ready for LLM work"""\n\n    print("=== LLM Environment Check ===\\n")\n\n    # 1. Python version\n    print(f"\u2713 Python version: {sys.version.split()[0]}")\n    if sys.version_info < (3, 10):\n        print("  \u26a0 Warning: Python 3.10+ recommended for best compatibility")\n\n    # 2. CUDA availability\n    try:\n        import torch\n        print(f"\u2713 PyTorch installed: {torch.__version__}")\n\n        if torch.cuda.is_available():\n            print(f"\u2713 CUDA available: Yes")\n            print(f"  GPU count: {torch.cuda.device_count()}")\n            for i in range(torch.cuda.device_count()):\n                gpu_name = torch.cuda.get_device_name(i)\n                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB VRAM)")\n\n                # Hardware recommendations\n                if gpu_memory < 12:\n                    print(f"    \u26a0 Warning: {gpu_memory:.1f}GB VRAM is low for training")\n                    print(f"    \u2192 Consider cloud GPU options (see below)")\n                elif gpu_memory >= 24:\n                    print(f"    \u2713 Excellent! Can train 7B-13B models with LoRA")\n        else:\n            print("\u26a0 CUDA not available - CPU only mode")\n            print("  \u2192 You can run inference with Ollama, but training will be very slow")\n            print("  \u2192 Recommend cloud GPU for training (see Cloud Options below)")\n    except ImportError:\n        print("\u2717 PyTorch not installed")\n        print("  Install: pip install torch torchvision torchaudio")\n\n    # 3. RAM check\n    try:\n        import psutil\n        ram_gb = psutil.virtual_memory().total / 1024**3\n        print(f"\u2713 System RAM: {ram_gb:.1f} GB")\n        if ram_gb < 32:\n            print("  \u26a0 Warning: 32GB+ RAM recommended for training")\n    except ImportError:\n        print("\u26a0 psutil not installed (can\'t check RAM)")\n        print("  Install: pip install psutil")\n\n    # 4. Disk space\n    try:\n        import shutil\n        disk = shutil.disk_usage("/")\n        free_gb = disk.free / 1024**3\n        print(f"\u2713 Free disk space: {free_gb:.1f} GB")\n        if free_gb < 100:\n            print("  \u26a0 Warning: 100GB+ free space recommended")\n    except Exception as e:\n        print(f"\u26a0 Could not check disk space: {e}")\n\n    print("\\n=== Cloud GPU Options (if local GPU insufficient) ===")\n    print("\u2022 Google Colab Pro ($10/month): T4/V100 GPUs, good for learning")\n    print("\u2022 Vast.ai: Rent GPUs starting at $0.20/hour")\n    print("\u2022 RunPod: $0.39/hour for RTX 3090, easy setup")\n    print("\u2022 Lambda Labs: $0.50/hour for A100, professional tier")\n    print("\u2022 Paperspace Gradient: $8/month for basic GPU access")\n\n    print("\\n=== Recommended Starting Points ===")\n    try:\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n            if gpu_memory >= 24:\n                print("\u2713 Your GPU: Start with 7B LoRA fine-tuning (Section: LoRA Fine-tuning)")\n            elif gpu_memory >= 12:\n                print("\u2713 Your GPU: Start with quantized training (4-bit LoRA)")\n            else:\n                print("\u2192 Your GPU: Inference only, use cloud for training")\n        else:\n            print("\u2192 No GPU: Start with Ollama for inference, use Colab for training")\n    except:\n        print("\u2192 Install PyTorch first, then rerun this check")\n\nif __name__ == "__main__":\n    check_environment()\n'})}),"\n",(0,i.jsx)(e.p,{children:"Run the check:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"python check_llm_env.py\n"})}),"\n",(0,i.jsx)(e.h3,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsx)(e.p,{children:"Based on your hardware:"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"If you have RTX 3090/4090 (24GB) or better:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Continue with "Environment Setup" below'}),"\n",(0,i.jsx)(e.li,{children:'Jump to "LoRA Fine-tuning" section for your first training'}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"If you have RTX 3060/3070 (8-12GB):"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use 4-bit quantization (covered in LoRA section)"}),"\n",(0,i.jsx)(e.li,{children:"Start with smaller datasets (< 10k samples)"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"If you have no GPU or < 8GB VRAM:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Continue using Ollama for inference"}),"\n",(0,i.jsx)(e.li,{children:"Use Google Colab Pro for training experiments"}),"\n",(0,i.jsx)(e.li,{children:"Consider cloud GPU rental for serious projects"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,i.jsx)(e.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Minimum Configuration (7B models):"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"GPU: RTX 3090/4090 (24GB VRAM) or A100 (40GB)"}),"\n",(0,i.jsx)(e.li,{children:"CPU: 16+ cores"}),"\n",(0,i.jsx)(e.li,{children:"Memory: 64GB DDR4/DDR5"}),"\n",(0,i.jsx)(e.li,{children:"Storage: 1TB NVMe SSD"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Recommended Configuration (13B-70B models):"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"GPU: Multi-card A100/H100 (80GB VRAM)"}),"\n",(0,i.jsx)(e.li,{children:"CPU: 32+ cores"}),"\n",(0,i.jsx)(e.li,{children:"Memory: 128GB+"}),"\n",(0,i.jsx)(e.li,{children:"Storage: 2TB NVMe SSD"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"software-environment-setup",children:"Software Environment Setup"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Create conda environment\nconda create -n llm-training python=3.10\nconda activate llm-training\n\n# Install PyTorch (CUDA 12.1)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# Install core dependencies\npip install transformers datasets accelerate\npip install deepspeed bitsandbytes\npip install wandb tensorboard\npip install flash-attn --no-build-isolation\n\n# Install training frameworks\npip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\npip install axolotl[flash-attn,deepspeed] @ git+https://github.com/OpenAccess-AI-Collective/axolotl.git\n"})}),"\n",(0,i.jsx)(e.h2,{id:"model-deployment",children:"Model Deployment"}),"\n",(0,i.jsx)(e.h3,{id:"quick-deployment-with-ollama",children:"Quick Deployment with Ollama"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Download and run models\nollama pull llama2:7b\nollama pull qwen2:7b\nollama pull codellama:7b\n\n# Start API service\nollama serve\n\n# Test API\ncurl http://localhost:11434/api/generate -d \'{\n  "model": "llama2:7b",\n  "prompt": "Why is the sky blue?",\n  "stream": false\n}\'\n'})}),"\n",(0,i.jsx)(e.h3,{id:"high-performance-deployment-with-vllm",children:"High-Performance Deployment with vLLM"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\nimport torch\n\n# Check GPU availability\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU count: {torch.cuda.device_count()}")\n\n# Initialize vLLM\nllm = LLM(\n    model="Qwen/Qwen2-7B-Instruct",\n    tensor_parallel_size=1,  # Number of GPUs\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n    trust_remote_code=True,\n    dtype="half"  # Use FP16 to save VRAM\n)\n\n# Set sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=512\n)\n\n# Batch inference\nprompts = [\n    "Explain what machine learning is",\n    "Write a Python sorting algorithm",\n    "Introduce basic concepts of deep learning"\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f"Prompt: {prompt}")\n    print(f"Generated: {generated_text}")\n    print("-" * 50)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"using-text-generation-webui",children:"Using Text Generation WebUI"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Clone repository\ngit clone https://github.com/oobabooga/text-generation-webui.git\ncd text-generation-webui\n\n# Install dependencies\npip install -r requirements.txt\n\n# Start WebUI\npython server.py --model-dir ./models --listen --api\n\n# Download models to models directory\n# Supports HuggingFace, GGUF, AWQ, GPTQ formats\n"})}),"\n",(0,i.jsx)(e.h2,{id:"data-preparation-and-preprocessing",children:"Data Preparation and Preprocessing"}),"\n",(0,i.jsx)(e.h3,{id:"dataset-formats",children:"Dataset Formats"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Instruction Fine-tuning Format (Alpaca):"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'{\n  "instruction": "Please explain what artificial intelligence is",\n  "input": "",\n  "output": "Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence..."\n}\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Conversation Format (ChatML):"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'{\n  "messages": [\n    {"role": "system", "content": "You are a helpful AI assistant"},\n    {"role": "user", "content": "What is deep learning?"},\n    {"role": "assistant", "content": "Deep learning is a subset of machine learning..."}\n  ]\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"data-preprocessing-script",children:"Data Preprocessing Script"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import json\nimport pandas as pd\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer\n\ndef prepare_alpaca_dataset(data_path, tokenizer, max_length=2048):\n    """Prepare Alpaca format dataset"""\n  \n    # Load data\n    with open(data_path, \'r\', encoding=\'utf-8\') as f:\n        data = json.load(f)\n  \n    def format_prompt(example):\n        if example[\'input\']:\n            prompt = f"### Instruction:\\n{example[\'instruction\']}\\n\\n### Input:\\n{example[\'input\']}\\n\\n### Response:\\n"\n        else:\n            prompt = f"### Instruction:\\n{example[\'instruction\']}\\n\\n### Response:\\n"\n    \n        full_text = prompt + example[\'output\']\n        return {"text": full_text}\n  \n    # Format data\n    formatted_data = [format_prompt(item) for item in data]\n    dataset = Dataset.from_list(formatted_data)\n  \n    # Tokenize\n    def tokenize_function(examples):\n        return tokenizer(\n            examples["text"],\n            truncation=True,\n            padding=False,\n            max_length=max_length,\n            return_overflowing_tokens=False,\n        )\n  \n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=dataset.column_names\n    )\n  \n    return tokenized_dataset\n\n# Usage example\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_dataset = prepare_alpaca_dataset("train_data.json", tokenizer)\neval_dataset = prepare_alpaca_dataset("eval_data.json", tokenizer)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"lora-fine-tuning",children:"LoRA Fine-tuning"}),"\n",(0,i.jsx)(e.h3,{id:"efficient-fine-tuning-with-unsloth",children:"Efficient Fine-tuning with Unsloth"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from unsloth import FastLanguageModel\nimport torch\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name="unsloth/qwen2-7b-bnb-4bit",  # 4bit quantized version\n    max_seq_length=2048,\n    dtype=None,  # Auto detect\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",\n                   "gate_proj", "up_proj", "down_proj"],\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias="none",\n    use_gradient_checkpointing="unsloth",\n    random_state=3407,\n)\n\n# Training configuration\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    dataset_text_field="text",\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=1,\n        optim="adamw_8bit",\n        weight_decay=0.01,\n        lr_scheduler_type="linear",\n        seed=3407,\n        output_dir="outputs",\n        save_steps=25,\n        eval_steps=25,\n        evaluation_strategy="steps",\n        load_best_model_at_end=True,\n        metric_for_best_model="eval_loss",\n        greater_is_better=False,\n    ),\n)\n\n# Start training\ntrainer_stats = trainer.train()\n\n# Save model\nmodel.save_pretrained("lora_model")\ntokenizer.save_pretrained("lora_model")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"professional-fine-tuning-with-axolotl",children:"Professional Fine-tuning with Axolotl"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Configuration file (config.yml):"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:"base_model: Qwen/Qwen2-7B-Instruct\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: ./data/train.jsonl\n    type: alpaca\n    conversation: false\n\ndataset_prepared_path: ./prepared_data\nval_set_size: 0.1\noutput_dir: ./outputs\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\n\nadapter: lora\nlora_model_dir:\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: llm-finetune\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\n\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Start training:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Prepare data\npython -m axolotl.cli.preprocess config.yml\n\n# Start training\npython -m axolotl.cli.train config.yml\n\n# Inference test\npython -m axolotl.cli.inference config.yml --lora_model_dir="./outputs"\n'})}),"\n",(0,i.jsx)(e.h2,{id:"full-parameter-fine-tuning",children:"Full Parameter Fine-tuning"}),"\n",(0,i.jsx)(e.h3,{id:"large-model-training-with-deepspeed",children:"Large Model Training with DeepSpeed"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"DeepSpeed configuration (ds_config.json):"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-json",children:'{\n  "fp16": {\n    "enabled": "auto",\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 16,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n  "bf16": {\n    "enabled": "auto"\n  },\n  "optimizer": {\n    "type": "AdamW",\n    "params": {\n      "lr": "auto",\n      "betas": "auto",\n      "eps": "auto",\n      "weight_decay": "auto"\n    }\n  },\n  "scheduler": {\n    "type": "WarmupLR",\n    "params": {\n      "warmup_min_lr": "auto",\n      "warmup_max_lr": "auto",\n      "warmup_num_steps": "auto"\n    }\n  },\n  "zero_optimization": {\n    "stage": 3,\n    "offload_optimizer": {\n      "device": "cpu",\n      "pin_memory": true\n    },\n    "offload_param": {\n      "device": "cpu",\n      "pin_memory": true\n    },\n    "overlap_comm": true,\n    "contiguous_gradients": true,\n    "sub_group_size": 1e9,\n    "reduce_bucket_size": "auto",\n    "stage3_prefetch_bucket_size": "auto",\n    "stage3_param_persistence_threshold": "auto",\n    "stage3_max_live_parameters": 1e9,\n    "stage3_max_reuse_distance": 1e9,\n    "stage3_gather_16bit_weights_on_model_save": true\n  },\n  "gradient_accumulation_steps": "auto",\n  "gradient_clipping": "auto",\n  "steps_per_print": 2000,\n  "train_batch_size": "auto",\n  "train_micro_batch_size_per_gpu": "auto",\n  "wall_clock_breakdown": false\n}\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Training script:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nimport deepspeed\n\ndef main():\n    # Model and tokenizer\n    model_name = "Qwen/Qwen2-7B"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n  \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True\n    )\n  \n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n  \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir="./full_finetune_output",\n        overwrite_output_dir=True,\n        num_train_epochs=3,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=8,\n        evaluation_strategy="steps",\n        eval_steps=500,\n        save_steps=1000,\n        logging_steps=100,\n        learning_rate=5e-5,\n        weight_decay=0.01,\n        warmup_steps=100,\n        lr_scheduler_type="cosine",\n        bf16=True,\n        dataloader_pin_memory=False,\n        deepspeed="ds_config.json",\n        report_to="wandb",\n        run_name="qwen2-7b-full-finetune"\n    )\n  \n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n  \n    # Start training\n    trainer.train()\n  \n    # Save model\n    trainer.save_model()\n    tokenizer.save_pretrained(training_args.output_dir)\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Start multi-GPU training:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"deepspeed --num_gpus=4 train_full.py\n"})}),"\n",(0,i.jsx)(e.h2,{id:"pre-training",children:"Pre-training"}),"\n",(0,i.jsx)(e.h3,{id:"pre-training-from-scratch",children:"Pre-training from Scratch"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Data preparation:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport multiprocessing\n\ndef prepare_pretraining_data():\n    # Load large-scale text data\n    dataset = load_dataset("wikitext", "wikitext-103-raw-v1")\n  \n    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\n  \n    def tokenize_function(examples):\n        return tokenizer(\n            examples["text"],\n            truncation=True,\n            padding=False,\n            max_length=2048,\n            return_overflowing_tokens=True,\n            return_length=True,\n        )\n  \n    # Parallel processing\n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        num_proc=multiprocessing.cpu_count(),\n        remove_columns=dataset["train"].column_names,\n    )\n  \n    return tokenized_dataset\n\n# Group texts\ndef group_texts(examples, block_size=2048):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n  \n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n  \n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result["labels"] = result["input_ids"].copy()\n    return result\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Pre-training configuration:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer\n)\n\n# Model configuration\nconfig = AutoConfig.from_pretrained("Qwen/Qwen2-7B")\nconfig.vocab_size = len(tokenizer)\n\n# Initialize model\nmodel = AutoModelForCausalLM.from_config(config)\n\n# Pre-training parameters\ntraining_args = TrainingArguments(\n    output_dir="./pretrain_output",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=16,\n    save_steps=10000,\n    logging_steps=1000,\n    learning_rate=1e-4,\n    weight_decay=0.1,\n    warmup_steps=10000,\n    lr_scheduler_type="cosine",\n    bf16=True,\n    deepspeed="ds_config_pretrain.json",\n    dataloader_num_workers=4,\n    remove_unused_columns=False,\n)\n\n# Pre-training\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=grouped_dataset["train"],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\ntrainer.train()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,i.jsx)(e.h3,{id:"automatic-evaluation-metrics",children:"Automatic Evaluation Metrics"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nfrom transformers import pipeline\nfrom datasets import load_metric\nimport numpy as np\n\ndef evaluate_model(model_path, test_dataset):\n    # Load model\n    generator = pipeline(\n        "text-generation",\n        model=model_path,\n        tokenizer=model_path,\n        torch_dtype=torch.float16,\n        device_map="auto"\n    )\n  \n    # BLEU score\n    bleu_metric = load_metric("bleu")\n  \n    predictions = []\n    references = []\n  \n    for example in test_dataset:\n        # Generate response\n        prompt = example["instruction"]\n        generated = generator(\n            prompt,\n            max_length=512,\n            num_return_sequences=1,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=generator.tokenizer.eos_token_id\n        )[0]["generated_text"]\n    \n        # Extract generated part\n        generated_text = generated[len(prompt):].strip()\n    \n        predictions.append(generated_text)\n        references.append([example["output"]])\n  \n    # Calculate BLEU score\n    bleu_score = bleu_metric.compute(\n        predictions=predictions,\n        references=references\n    )\n  \n    print(f"BLEU Score: {bleu_score[\'bleu\']:.4f}")\n  \n    return {\n        "bleu": bleu_score["bleu"],\n        "predictions": predictions,\n        "references": references\n    }\n\n# Perplexity evaluation\ndef calculate_perplexity(model, tokenizer, test_texts):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n  \n    with torch.no_grad():\n        for text in test_texts:\n            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n            outputs = model(**inputs, labels=inputs["input_ids"])\n            loss = outputs.loss\n        \n            total_loss += loss.item() * inputs["input_ids"].size(1)\n            total_tokens += inputs["input_ids"].size(1)\n  \n    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n    return perplexity.item()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"human-evaluation-framework",children:"Human Evaluation Framework"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import gradio as gr\nimport json\nfrom datetime import datetime\n\nclass ModelEvaluator:\n    def __init__(self, models_dict):\n        self.models = models_dict\n        self.results = []\n  \n    def create_evaluation_interface(self):\n        def evaluate_response(prompt, model_name, response, relevance, accuracy, fluency, helpfulness, comments):\n            result = {\n                "timestamp": datetime.now().isoformat(),\n                "prompt": prompt,\n                "model": model_name,\n                "response": response,\n                "scores": {\n                    "relevance": relevance,\n                    "accuracy": accuracy,\n                    "fluency": fluency,\n                    "helpfulness": helpfulness\n                },\n                "comments": comments,\n                "overall_score": (relevance + accuracy + fluency + helpfulness) / 4\n            }\n        \n            self.results.append(result)\n        \n            # Save results\n            with open("evaluation_results.json", "w", encoding="utf-8") as f:\n                json.dump(self.results, f, ensure_ascii=False, indent=2)\n        \n            return f"Evaluation saved! Overall score: {result[\'overall_score\']:.2f}"\n    \n        def generate_response(prompt, model_name):\n            if model_name in self.models:\n                generator = self.models[model_name]\n                response = generator(prompt, max_length=512, temperature=0.7)[0]["generated_text"]\n                return response[len(prompt):].strip()\n            return "Model not found"\n    \n        # Gradio interface\n        with gr.Blocks(title="LLM Model Evaluation System") as demo:\n            gr.Markdown("# LLM Model Evaluation System")\n        \n            with gr.Row():\n                with gr.Column():\n                    prompt_input = gr.Textbox(label="Input Prompt", lines=3)\n                    model_dropdown = gr.Dropdown(\n                        choices=list(self.models.keys()),\n                        label="Select Model",\n                        value=list(self.models.keys())[0] if self.models else None\n                    )\n                    generate_btn = gr.Button("Generate Response")\n            \n                with gr.Column():\n                    response_output = gr.Textbox(label="Model Response", lines=5)\n        \n            gr.Markdown("## Evaluation Metrics (1-5 scale)")\n        \n            with gr.Row():\n                relevance_slider = gr.Slider(1, 5, value=3, label="Relevance")\n                accuracy_slider = gr.Slider(1, 5, value=3, label="Accuracy")\n                fluency_slider = gr.Slider(1, 5, value=3, label="Fluency")\n                helpfulness_slider = gr.Slider(1, 5, value=3, label="Helpfulness")\n        \n            comments_input = gr.Textbox(label="Comments", lines=2)\n            evaluate_btn = gr.Button("Submit Evaluation")\n            result_output = gr.Textbox(label="Evaluation Result")\n        \n            # Event binding\n            generate_btn.click(\n                generate_response,\n                inputs=[prompt_input, model_dropdown],\n                outputs=response_output\n            )\n        \n            evaluate_btn.click(\n                evaluate_response,\n                inputs=[\n                    prompt_input, model_dropdown, response_output,\n                    relevance_slider, accuracy_slider, fluency_slider, helpfulness_slider,\n                    comments_input\n                ],\n                outputs=result_output\n            )\n    \n        return demo\n\n# Usage example\nmodels = {\n    "Original Model": pipeline("text-generation", model="Qwen/Qwen2-7B"),\n    "Fine-tuned Model": pipeline("text-generation", model="./lora_model")\n}\n\nevaluator = ModelEvaluator(models)\ndemo = evaluator.create_evaluation_interface()\ndemo.launch(share=True)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"model-quantization-and-optimization",children:"Model Quantization and Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"gptq-quantization",children:"GPTQ Quantization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom transformers import AutoTokenizer\n\n# Quantization configuration\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # 4bit quantization\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.1,\n    sym=True,\n    true_sequential=True,\n)\n\n# Load model\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    "Qwen/Qwen2-7B",\n    quantize_config=quantize_config,\n    low_cpu_mem_usage=True,\n    device_map="auto"\n)\n\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\n\n# Prepare calibration data\ncalibration_dataset = [\n    "What is the history of artificial intelligence development?",\n    "Please explain the basic principles of deep learning.",\n    "What are the main algorithms in machine learning?",\n    # More calibration data...\n]\n\n# Execute quantization\nmodel.quantize(calibration_dataset)\n\n# Save quantized model\nmodel.save_quantized("./qwen2-7b-gptq")\ntokenizer.save_pretrained("./qwen2-7b-gptq")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"awq-quantization",children:"AWQ Quantization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n# Load model\nmodel_path = "Qwen/Qwen2-7B"\nquant_path = "qwen2-7b-awq"\n\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantization configuration\nquant_config = {\n    "zero_point": True,\n    "q_group_size": 128,\n    "w_bit": 4,\n    "version": "GEMM"\n}\n\n# Execute quantization\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f"Quantized model saved to: {quant_path}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"production-deployment",children:"Production Deployment"}),"\n",(0,i.jsx)(e.h3,{id:"docker-containerization",children:"Docker Containerization"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Dockerfile:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-dockerfile",children:'FROM nvidia/cuda:12.1-devel-ubuntu22.04\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    python3.10 python3-pip git wget curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements file\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Set environment variables\nENV PYTHONPATH=/app\nENV CUDA_VISIBLE_DEVICES=0\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Start command\nCMD ["python", "serve.py"]\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Service script (serve.py):"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\nimport torch\nimport uvicorn\nfrom typing import Optional\n\napp = FastAPI(title="LLM Service API", version="1.0.0")\n\n# Global variables\ngenerator = None\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    top_p: float = 0.9\n    do_sample: bool = True\n\nclass GenerateResponse(BaseModel):\n    generated_text: str\n    prompt: str\n\n@app.on_event("startup")\nasync def load_model():\n    global generator\n    print("Loading model...")\n  \n    generator = pipeline(\n        "text-generation",\n        model="./qwen2-7b-awq",  # Quantized model path\n        tokenizer="./qwen2-7b-awq",\n        torch_dtype=torch.float16,\n        device_map="auto",\n        trust_remote_code=True\n    )\n  \n    print("Model loaded successfully!")\n\n@app.get("/health")\nasync def health_check():\n    return {"status": "healthy", "model_loaded": generator is not None}\n\n@app.post("/generate", response_model=GenerateResponse)\nasync def generate_text(request: GenerateRequest):\n    if generator is None:\n        raise HTTPException(status_code=503, detail="Model not loaded")\n  \n    try:\n        result = generator(\n            request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            top_p=request.top_p,\n            do_sample=request.do_sample,\n            pad_token_id=generator.tokenizer.eos_token_id\n        )\n    \n        generated_text = result[0]["generated_text"][len(request.prompt):].strip()\n    \n        return GenerateResponse(\n            generated_text=generated_text,\n            prompt=request.prompt\n        )\n  \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"kubernetes-deployment",children:"Kubernetes Deployment"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"deployment.yaml:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-service\n  labels:\n    app: llm-service\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: llm-service\n  template:\n    metadata:\n      labels:\n        app: llm-service\n    spec:\n      containers:\n      - name: llm-service\n        image: your-registry/llm-service:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            nvidia.com/gpu: 1\n            memory: "16Gi"\n            cpu: "4"\n          limits:\n            nvidia.com/gpu: 1\n            memory: "24Gi"\n            cpu: "6"\n        env:\n        - name: MODEL_PATH\n          value: "/models/qwen2-7b-awq"\n        - name: CUDA_VISIBLE_DEVICES\n          value: "0"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n      nodeSelector:\n        accelerator: nvidia-gpu\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: llm-service\nspec:\n  selector:\n    app: llm-service\n  ports:\n  - port: 80\n    targetPort: 8000\n    protocol: TCP\n  type: LoadBalancer\n'})}),"\n",(0,i.jsx)(e.h2,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,i.jsx)(e.h3,{id:"prometheus-monitoring",children:"Prometheus Monitoring"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\nimport functools\n\n# Define metrics\nREQUEST_COUNT = Counter('llm_requests_total', 'Total requests', ['method', 'endpoint', 'status'])\nREQUEST_LATENCY = Histogram('llm_request_duration_seconds', 'Request latency')\nACTIVE_REQUESTS = Gauge('llm_active_requests', 'Active requests')\nGPU_MEMORY = Gauge('llm_gpu_memory_usage_bytes', 'GPU memory usage', ['gpu_id'])\nMODEL_LOAD_TIME = Gauge('llm_model_load_time_seconds', 'Model load time')\n\ndef monitor_requests(func):\n    @functools.wraps(func)\n    async def wrapper(*args, **kwargs):\n        start_time = time.time()\n        ACTIVE_REQUESTS.inc()\n    \n        try:\n            result = await func(*args, **kwargs)\n            REQUEST_COUNT.labels(method='POST', endpoint='/generate', status='success').inc()\n            return result\n        except Exception as e:\n            REQUEST_COUNT.labels(method='POST', endpoint='/generate', status='error').inc()\n            raise\n        finally:\n            ACTIVE_REQUESTS.dec()\n            REQUEST_LATENCY.observe(time.time() - start_time)\n  \n    return wrapper\n\n# Use in FastAPI\n@app.post(\"/generate\")\n@monitor_requests\nasync def generate_text(request: GenerateRequest):\n    # Original logic\n    pass\n\n# Start Prometheus metrics server\nstart_http_server(9090)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"structured-logging",children:"Structured Logging"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import logging\nimport json\nfrom datetime import datetime\nimport sys\n\nclass StructuredLogger:\n    def __init__(self, name):\n        self.logger = logging.getLogger(name)\n        self.logger.setLevel(logging.INFO)\n    \n        # Create handler\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(self.JSONFormatter())\n    \n        self.logger.addHandler(handler)\n  \n    class JSONFormatter(logging.Formatter):\n        def format(self, record):\n            log_entry = {\n                "timestamp": datetime.utcnow().isoformat(),\n                "level": record.levelname,\n                "logger": record.name,\n                "message": record.getMessage(),\n                "module": record.module,\n                "function": record.funcName,\n                "line": record.lineno\n            }\n        \n            # Add extra fields\n            if hasattr(record, \'user_id\'):\n                log_entry[\'user_id\'] = record.user_id\n            if hasattr(record, \'request_id\'):\n                log_entry[\'request_id\'] = record.request_id\n            if hasattr(record, \'model_name\'):\n                log_entry[\'model_name\'] = record.model_name\n        \n            return json.dumps(log_entry, ensure_ascii=False)\n  \n    def info(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.info(message, extra=extra)\n  \n    def error(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.error(message, extra=extra)\n  \n    def warning(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.warning(message, extra=extra)\n\n# Usage example\nlogger = StructuredLogger("llm_service")\n\n@app.post("/generate")\nasync def generate_text(request: GenerateRequest):\n    request_id = str(uuid.uuid4())\n  \n    logger.info(\n        "Received generation request",\n        request_id=request_id,\n        prompt_length=len(request.prompt),\n        max_length=request.max_length,\n        temperature=request.temperature\n    )\n  \n    try:\n        start_time = time.time()\n        result = generator(request.prompt, ...)\n        generation_time = time.time() - start_time\n    \n        logger.info(\n            "Generation completed",\n            request_id=request_id,\n            generation_time=generation_time,\n            output_length=len(result[0]["generated_text"])\n        )\n    \n        return result\n    \n    except Exception as e:\n        logger.error(\n            "Generation failed",\n            request_id=request_id,\n            error=str(e),\n            error_type=type(e).__name__\n        )\n        raise\n'})}),"\n",(0,i.jsx)(e.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,i.jsx)(e.h3,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Memory Management"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use gradient checkpointing to reduce memory usage"}),"\n",(0,i.jsx)(e.li,{children:"Enable CPU offloading for large models"}),"\n",(0,i.jsx)(e.li,{children:"Set appropriate batch size and sequence length"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Training Acceleration"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use FlashAttention-2"}),"\n",(0,i.jsx)(e.li,{children:"Enable mixed precision training (FP16/BF16)"}),"\n",(0,i.jsx)(e.li,{children:"Use DeepSpeed ZeRO optimization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Inference Optimization"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Model quantization (GPTQ/AWQ)"}),"\n",(0,i.jsx)(e.li,{children:"Use vLLM for efficient inference"}),"\n",(0,i.jsx)(e.li,{children:"Batch requests to improve throughput"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Data Security"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Anonymize training data"}),"\n",(0,i.jsx)(e.li,{children:"Filter model output content"}),"\n",(0,i.jsx)(e.li,{children:"Validate and sanitize user inputs"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Model Security"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Regular backup of model checkpoints"}),"\n",(0,i.jsx)(e.li,{children:"Version control and rollback mechanisms"}),"\n",(0,i.jsx)(e.li,{children:"Access control and permissions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Deployment Security"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"API authentication and authorization"}),"\n",(0,i.jsx)(e.li,{children:"Request rate limiting"}),"\n",(0,i.jsx)(e.li,{children:"Network security configuration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"cost-control",children:"Cost Control"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Compute Resources"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use Spot instances to reduce costs"}),"\n",(0,i.jsx)(e.li,{children:"Auto-scaling based on load"}),"\n",(0,i.jsx)(e.li,{children:"Choose appropriate GPU models"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Storage Optimization"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Model compression and quantization"}),"\n",(0,i.jsx)(e.li,{children:"Data deduplication and compression"}),"\n",(0,i.jsx)(e.li,{children:"Tiered storage for hot/cold data"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"This complete guide covers the entire workflow from environment setup to production deployment, and you can choose the appropriate technical solutions based on specific requirements."}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"1-cuda-out-of-memory-error",children:"1. CUDA Out of Memory Error"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"RuntimeError: CUDA out of memory. Tried to allocate X GB\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Option 1: Reduce batch size\nper_device_train_batch_size=1  # Instead of 2 or 4\ngradient_accumulation_steps=8  # Increase to maintain effective batch size\n\n# Option 2: Enable gradient checkpointing\ngradient_checkpointing=True\n\n# Option 3: Use 4-bit quantization\nload_in_4bit=True\n\n# Option 4: Reduce max sequence length\nmax_seq_length=1024  # Instead of 2048\n\n# Option 5: Clear cache between batches\nimport torch\ntorch.cuda.empty_cache()\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-model-loading-fails-with-safetensors-error",children:'2. Model Loading Fails with "Safetensors" Error'}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"OSError: Unable to load weights from safetensors file\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Solution 1: Install safetensors\npip install safetensors\n\n# Solution 2: Download model files manually\ngit lfs install\ngit clone https://huggingface.co/Qwen/Qwen2-7B\n\n# Solution 3: Use legacy format\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_safetensors=False  # Use legacy .bin files\n)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"3-flash-attention-installation-fails",children:"3. Flash Attention Installation Fails"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"ERROR: Failed building wheel for flash-attn\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Solution 1: Install prebuilt wheel\npip install flash-attn --no-build-isolation\n\n# Solution 2: Check CUDA version compatibility\n# Flash Attention requires CUDA 11.6+\npython -c "import torch; print(torch.version.cuda)"\n\n# Solution 3: Install from source with correct CUDA arch\nCUDA_HOME=/usr/local/cuda pip install flash-attn --no-build-isolation\n\n# Solution 4: Skip flash attention\n# Remove flash_attention: true from config\n# Training will be slower but functional\n'})}),"\n",(0,i.jsx)(e.h3,{id:"4-tokenizer-padding-issues",children:"4. Tokenizer Padding Issues"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"ValueError: Asking to pad but the tokenizer does not have a padding token\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Solution 1: Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Solution 2: Add special tokens\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Solution 3: Use padding_side parameter\ntokenizer.padding_side = \"right\"\n"})}),"\n",(0,i.jsx)(e.h3,{id:"5-deepspeed-configuration-errors",children:"5. DeepSpeed Configuration Errors"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"AssertionError: [deepspeed] Expected X parameters, but got Y\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Solution 1: Verify DeepSpeed installation\npip install deepspeed --upgrade\n\n# Solution 2: Check ZeRO stage compatibility\n# Stage 3 requires specific model modifications\n# Try Stage 2 first:\n{\n  "zero_optimization": {\n    "stage": 2  # Instead of 3\n  }\n}\n\n# Solution 3: Clear old checkpoints\nrm -rf outputs/\nrm -rf ~/.cache/huggingface/accelerate/\n\n# Solution 4: Disable CPU offload for debugging\n"offload_optimizer": {\n  "device": "none"  # Instead of "cpu"\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"6-training-loss-not-decreasing",children:"6. Training Loss Not Decreasing"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem:"})," Loss stays constant or increases during training"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Check 1: Verify data format\n# Print first example to ensure it's formatted correctly\nprint(tokenizer.decode(train_dataset[0]['input_ids']))\n\n# Check 2: Adjust learning rate\nlearning_rate=5e-5  # Try different values: 1e-5, 2e-4, 5e-5\n\n# Check 3: Check LoRA rank\nlora_r=32  # Try higher: 64, 128 (uses more memory)\n\n# Check 4: Disable train_on_inputs for instruction tuning\ntrain_on_inputs=False  # Only train on outputs\n\n# Check 5: Verify labels are set correctly\n# Labels should match input_ids for causal LM\n"})}),"\n",(0,i.jsx)(e.h3,{id:"7-slow-training-speed",children:"7. Slow Training Speed"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem:"})," Training is significantly slower than expected"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Solution 1: Enable mixed precision\nbf16=True  # For Ampere+ GPUs (RTX 30 series, A100)\nfp16=True  # For older GPUs\n\n# Solution 2: Use faster optimizer\noptim="adamw_8bit"  # Instead of standard adamw\n\n# Solution 3: Enable compilation (PyTorch 2.0+)\nimport torch\nmodel = torch.compile(model)\n\n# Solution 4: Optimize dataloader\ndataloader_num_workers=4  # Adjust based on CPU cores\ndataloader_pin_memory=True\n\n# Solution 5: Use gradient checkpointing selectively\ngradient_checkpointing=True\ngradient_checkpointing_kwargs={"use_reentrant": False}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"8-vllm-initialization-fails",children:"8. vLLM Initialization Fails"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"ValueError: Model architecture not supported by vLLM\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Solution 1: Check supported models\n# vLLM supports: Llama, Mistral, Qwen2, GPT-2, etc.\n# See: https://docs.vllm.ai/en/latest/models/supported_models.html\n\n# Solution 2: Use transformers pipeline instead\nfrom transformers import pipeline\ngenerator = pipeline('text-generation', model=\"model_path\")\n\n# Solution 3: Convert model format\n# Some models need conversion to vLLM format\npython -m vllm.entrypoints.openai.api_server \\\n    --model model_path \\\n    --tensor-parallel-size 1\n\n# Solution 4: Update vLLM\npip install vllm --upgrade\n"})}),"\n",(0,i.jsx)(e.h3,{id:"9-hugging-face-hub-authentication",children:"9. Hugging Face Hub Authentication"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"OSError: You are trying to access a gated repo\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Solution 1: Login to Hugging Face\npip install huggingface_hub\nhuggingface-cli login\n\n# Solution 2: Use access token\nfrom huggingface_hub import login\nlogin(token="your_token_here")\n\n# Solution 3: Accept model license\n# Visit the model page on HuggingFace\n# Example: https://huggingface.co/meta-llama/Llama-2-7b\n# Click "Agree and access repository"\n\n# Solution 4: Use local model files\nmodel_path = "./local_models/qwen2-7b"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"10-multi-gpu-training-issues",children:"10. Multi-GPU Training Issues"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Error:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"RuntimeError: Distributed package doesn't have NCCL built in\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Solutions:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'# Solution 1: Reinstall PyTorch with NCCL\npip uninstall torch\npip install torch --index-url https://download.pytorch.org/whl/cu121\n\n# Solution 2: Use correct launcher\n# Instead of: python train.py\n# Use: torchrun --nproc_per_node=4 train.py\n# Or: accelerate launch train.py\n\n# Solution 3: Check GPU visibility\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython -c "import torch; print(torch.cuda.device_count())"\n\n# Solution 4: Use DeepSpeed launcher\ndeepspeed --num_gpus=4 train.py --deepspeed ds_config.json\n'})}),"\n",(0,i.jsx)(e.h2,{id:"complete-setup-validation",children:"Complete Setup Validation"}),"\n",(0,i.jsx)(e.p,{children:"Save this script to verify your complete environment:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# save as validate_llm_setup.py\nimport sys\nimport subprocess\nfrom typing import List, Tuple\n\ndef run_checks() -> List[Tuple[str, bool, str]]:\n    """Run comprehensive environment checks"""\n    results = []\n\n    # 1. Python packages\n    print("\\n=== Checking Python Packages ===")\n    required_packages = [\n        \'torch\',\n        \'transformers\',\n        \'datasets\',\n        \'accelerate\',\n        \'peft\',\n        \'bitsandbytes\',\n        \'sentencepiece\',\n    ]\n\n    for package in required_packages:\n        try:\n            __import__(package)\n            version = __import__(package).__version__\n            results.append((f"{package}", True, f"v{version}"))\n            print(f"\u2713 {package:20s} v{version}")\n        except ImportError:\n            results.append((f"{package}", False, "Not installed"))\n            print(f"\u2717 {package:20s} NOT INSTALLED")\n\n    # 2. CUDA check\n    print("\\n=== Checking CUDA ===")\n    try:\n        import torch\n        if torch.cuda.is_available():\n            cuda_version = torch.version.cuda\n            gpu_count = torch.cuda.device_count()\n            results.append(("CUDA", True, f"v{cuda_version}, {gpu_count} GPU(s)"))\n            print(f"\u2713 CUDA v{cuda_version}")\n            print(f"\u2713 {gpu_count} GPU(s) available")\n\n            for i in range(gpu_count):\n                name = torch.cuda.get_device_name(i)\n                memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f"  GPU {i}: {name} ({memory:.1f} GB)")\n                results.append((f"GPU {i}", True, f"{name} ({memory:.1f}GB)"))\n        else:\n            results.append(("CUDA", False, "Not available"))\n            print("\u2717 CUDA not available (CPU-only mode)")\n    except Exception as e:\n        results.append(("CUDA", False, str(e)))\n        print(f"\u2717 CUDA check failed: {e}")\n\n    # 3. Quick inference test\n    print("\\n=== Testing Model Inference ===")\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        import torch\n\n        print("Loading tiny test model...")\n        model_name = "hf-internal-testing/tiny-random-GPTJForCausalLM"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n        # Test inference\n        inputs = tokenizer("Hello world", return_tensors="pt")\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_length=10)\n\n        results.append(("Model Inference", True, "Success"))\n        print("\u2713 Model inference test passed")\n    except Exception as e:\n        results.append(("Model Inference", False, str(e)))\n        print(f"\u2717 Model inference test failed: {e}")\n\n    # 4. Ollama check\n    print("\\n=== Checking Ollama ===")\n    try:\n        result = subprocess.run([\'ollama\', \'--version\'],\n                              capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            version = result.stdout.strip()\n            results.append(("Ollama", True, version))\n            print(f"\u2713 Ollama installed: {version}")\n        else:\n            results.append(("Ollama", False, "Not working"))\n            print("\u2717 Ollama installed but not working")\n    except FileNotFoundError:\n        results.append(("Ollama", False, "Not installed"))\n        print("\u2717 Ollama not installed")\n    except Exception as e:\n        results.append(("Ollama", False, str(e)))\n        print(f"\u26a0 Ollama check failed: {e}")\n\n    # 5. Flash Attention check\n    print("\\n=== Checking Flash Attention ===")\n    try:\n        import flash_attn\n        results.append(("Flash Attention", True, flash_attn.__version__))\n        print(f"\u2713 Flash Attention installed: {flash_attn.__version__}")\n    except ImportError:\n        results.append(("Flash Attention", False, "Not installed (optional)"))\n        print("\u26a0 Flash Attention not installed (optional, improves speed)")\n\n    return results\n\ndef print_summary(results: List[Tuple[str, bool, str]]):\n    """Print final summary"""\n    print("\\n" + "="*60)\n    print("VALIDATION SUMMARY")\n    print("="*60)\n\n    passed = sum(1 for _, success, _ in results if success)\n    total = len(results)\n    critical_failures = [name for name, success, _ in results\n                        if not success and name in [\'torch\', \'transformers\', \'datasets\']]\n\n    print(f"\\nPassed: {passed}/{total} checks")\n\n    if critical_failures:\n        print(f"\\n\u2717 CRITICAL: Missing required packages: {\', \'.join(critical_failures)}")\n        print("  Install with: pip install torch transformers datasets accelerate")\n        print("\\n\u26a0 Cannot proceed with LLM training until these are installed.")\n    elif passed == total:\n        print("\\n\u2713 ALL CHECKS PASSED! Your environment is ready for LLM training.")\n        print("\\nRecommended next steps:")\n        print("1. Start with Ollama for quick inference testing")\n        print("2. Try LoRA fine-tuning with a small dataset")\n        print("3. Scale up to larger models as needed")\n    else:\n        print("\\n\u26a0 Some optional checks failed, but you can still proceed.")\n        print("  Missing components may limit functionality or performance.")\n\n    print("\\n" + "="*60)\n\ndef main():\n    print("="*60)\n    print("LLM ENVIRONMENT VALIDATION")\n    print("="*60)\n    print("\\nThis script validates your environment for LLM training and deployment.")\n\n    results = run_checks()\n    print_summary(results)\n\n    # Save results\n    try:\n        with open("validation_results.txt", "w") as f:\n            f.write("LLM Environment Validation Results\\n")\n            f.write("="*50 + "\\n\\n")\n            for name, success, detail in results:\n                status = "\u2713 PASS" if success else "\u2717 FAIL"\n                f.write(f"{status} | {name:25s} | {detail}\\n")\n\n        print(f"\\nResults saved to: validation_results.txt")\n    except Exception as e:\n        print(f"\\n\u26a0 Could not save results: {e}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.p,{children:"Run the validation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"python validate_llm_setup.py\n"})}),"\n",(0,i.jsx)(e.p,{children:"This will check all critical components and give you a clear picture of what's working and what needs attention before starting your LLM projects."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.em,{children:"Last updated: January 2025"})})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(c,{...n})}):c(n)}}}]);