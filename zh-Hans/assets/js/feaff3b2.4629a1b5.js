"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[936],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var a=t(96540);const i={},o=a.createContext(i);function r(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(o.Provider,{value:n},e.children)}},58578:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var a=t(82397),i=t(74848),o=t(28453);const r={slug:"local-ai-agent-deployment",title:"Guide to Local AI Agent",authors:["liangchao"],tags:["ai","machine learning","deployment","tutorial","local development"]},s="Guide to Local AI Agent Deployment",l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"Technical Workflow Overview",id:"technical-workflow-overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Prerequisites",id:"software-prerequisites",level:3},{value:"Method 1: Ollama - The Simplest Approach",id:"method-1-ollama---the-simplest-approach",level:2},{value:"Installation",id:"installation",level:3},{value:"Basic Usage",id:"basic-usage",level:3},{value:"API Integration",id:"api-integration",level:3},{value:"Available Models",id:"available-models",level:3},{value:"Method 2: Docker-based Deployment",id:"method-2-docker-based-deployment",level:2},{value:"Create Docker Environment",id:"create-docker-environment",level:3},{value:"FastAPI Application",id:"fastapi-application",level:3},{value:"Method 3: LangChain with Local Models",id:"method-3-langchain-with-local-models",level:2},{value:"Setup LangChain Environment",id:"setup-langchain-environment",level:3},{value:"Method 4: Multi-Modal AI Agent",id:"method-4-multi-modal-ai-agent",level:2},{value:"Vision-Language Model Setup",id:"vision-language-model-setup",level:3},{value:"Method 5: RAG (Retrieval-Augmented Generation) System",id:"method-5-rag-retrieval-augmented-generation-system",level:2},{value:"Vector Database Setup",id:"vector-database-setup",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Model Quantization",id:"model-quantization",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:2},{value:"System Monitoring",id:"system-monitoring",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"API Security",id:"api-security",level:3},{value:"Input Sanitization",id:"input-sanitization",level:3},{value:"Deployment Scripts",id:"deployment-scripts",level:2},{value:"Automated Setup Script",id:"automated-setup-script",level:3},{value:"Systemd Service",id:"systemd-service",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Deploying AI agents locally offers numerous advantages including data privacy, reduced latency, cost control, and independence from cloud services. This comprehensive guide covers multiple approaches to setting up AI agents on your local infrastructure, from simple chatbots to complex multi-modal systems."}),"\n",(0,i.jsx)(n.h2,{id:"technical-workflow-overview",children:"Technical Workflow Overview"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    A[Prerequisites Analysis] --\x3e B[Hardware Requirements]\n    A --\x3e C[Software Prerequisites]\n    B --\x3e D[Method Selection]\n    C --\x3e D\n    D --\x3e E[Ollama Simple Deployment]\n    D --\x3e F[Docker-based Deployment]\n    D --\x3e G[LangChain Integration]\n    D --\x3e H[Multi-Modal Systems]\n    D --\x3e I[RAG Systems]\n    E --\x3e J[API Integration]\n    F --\x3e K[Container Orchestration]\n    G --\x3e L[Memory Management]\n    H --\x3e M[Vision-Language Models]\n    I --\x3e N[Vector Database Setup]\n    J --\x3e O[Performance Testing]\n    K --\x3e O\n    L --\x3e O\n    M --\x3e O\n    N --\x3e O\n    O --\x3e P[Production Deployment]\n    P --\x3e Q[Monitoring & Maintenance]\n    \n    B --\x3e B1[CPU/RAM Requirements]\n    B --\x3e B2[GPU Acceleration]\n    B --\x3e B3[Storage Considerations]\n    \n    C --\x3e C1[OS Compatibility]\n    C --\x3e C2[Docker Setup]\n    C --\x3e C3[Python Environment]\n    \n    E --\x3e E1[Model Selection]\n    E --\x3e E2[Service Configuration]\n    \n    F --\x3e F1[Container Definition]\n    F --\x3e F2[Service Orchestration]\n    \n    G --\x3e G1[Chain Configuration]\n    G --\x3e G2[Prompt Engineering]\n    \n    H --\x3e H1[Image Processing]\n    H --\x3e H2[Multi-modal Fusion]\n    \n    I --\x3e I1[Document Processing]\n    I --\x3e I2[Vector Embeddings]\n    I --\x3e I3[Retrieval Optimization]\n    \n    O --\x3e O1[Load Testing]\n    O --\x3e O2[Security Assessment]\n    O --\x3e O3[Scalability Analysis]\n    \n    P --\x3e P1[API Gateway]\n    P --\x3e P2[Load Balancing]\n    P --\x3e P3[Failover Mechanisms]\n    \n    Q --\x3e Q1[Performance Metrics]\n    Q --\x3e Q2[Resource Monitoring]\n    Q --\x3e Q3[Update Management]"}),"\n",(0,i.jsx)(n.p,{children:"This workflow outlines the comprehensive process for deploying AI agents locally, highlighting multiple deployment strategies and their integration points for building robust, scalable AI systems."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Minimum Configuration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CPU: 8-core processor (Intel i7/AMD Ryzen 7 or equivalent)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 16GB DDR4"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 100GB available SSD space"}),"\n",(0,i.jsx)(n.li,{children:"GPU: Optional but recommended (NVIDIA GTX 1060 or better)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recommended Configuration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CPU: 12+ core processor (Intel i9/AMD Ryzen 9 or equivalent)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 32GB+ DDR4/DDR5"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 500GB+ NVMe SSD"}),"\n",(0,i.jsx)(n.li,{children:"GPU: NVIDIA RTX 3080/4070 or better with 12GB+ VRAM"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"software-prerequisites",children:"Software Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Operating System: Ubuntu 20.04+, macOS 12+, or Windows 10/11"}),"\n",(0,i.jsx)(n.li,{children:"Docker and Docker Compose"}),"\n",(0,i.jsx)(n.li,{children:"Python 3.8+ with pip"}),"\n",(0,i.jsx)(n.li,{children:"Git"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA drivers (for GPU acceleration)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"method-1-ollama---the-simplest-approach",children:"Method 1: Ollama - The Simplest Approach"}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Linux/macOS:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.ai/install.sh | sh\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Windows:"}),"\nDownload and install from ",(0,i.jsx)(n.a,{href:"https://ollama.ai/download",children:"https://ollama.ai/download"})]}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Pull a model\nollama pull llama2\n\n# Run interactive chat\nollama run llama2\n\n# Start as service\nollama serve\n"})}),"\n",(0,i.jsx)(n.h3,{id:"api-integration",children:"API Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\n\ndef chat_with_ollama(message, model="llama2"):\n    url = "http://localhost:11434/api/generate"\n    payload = {\n        "model": model,\n        "prompt": message,\n        "stream": False\n    }\n  \n    response = requests.post(url, json=payload)\n    return response.json()["response"]\n\n# Example usage\nresponse = chat_with_ollama("Explain quantum computing")\nprint(response)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"available-models",children:"Available Models"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"llama2"}),": General purpose conversational AI"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"codellama"}),": Code generation and analysis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"mistral"}),": Efficient multilingual model"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"neural-chat"}),": Optimized for dialogue"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"llava"}),": Vision-language model"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"method-2-docker-based-deployment",children:"Method 2: Docker-based Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"create-docker-environment",children:"Create Docker Environment"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dockerfile:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'FROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\nEXPOSE 8000\n\nCMD ["python", "app.py"]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"requirements.txt:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"fastapi==0.104.1\nuvicorn==0.24.0\ntransformers==4.35.0\ntorch==2.1.0\naccelerate==0.24.1\nlangchain==0.0.335\nchromadb==0.4.15\nsentence-transformers==2.2.2\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"docker-compose.yml:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  ai-agent:\n    build: .\n    ports:\n      - "8000:8000"\n    volumes:\n      - ./models:/app/models\n      - ./data:/app/data\n    environment:\n      - CUDA_VISIBLE_DEVICES=0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  vector-db:\n    image: chromadb/chroma:latest\n    ports:\n      - "8001:8000"\n    volumes:\n      - ./chroma_data:/chroma/chroma\n'})}),"\n",(0,i.jsx)(n.h3,{id:"fastapi-application",children:"FastAPI Application"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"app.py:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport uvicorn\n\napp = FastAPI(title="Local AI Agent API")\n\nclass ChatRequest(BaseModel):\n    message: str\n    max_length: int = 512\n    temperature: float = 0.7\n\nclass AIAgent:\n    def __init__(self, model_name="microsoft/DialoGPT-medium"):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.model.to(self.device)\n    \n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def generate_response(self, message, max_length=512, temperature=0.7):\n        inputs = self.tokenizer.encode(message, return_tensors="pt").to(self.device)\n    \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_length=max_length,\n                temperature=temperature,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n    \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response[len(message):].strip()\n\n# Initialize agent\nagent = AIAgent()\n\n@app.post("/chat")\nasync def chat(request: ChatRequest):\n    try:\n        response = agent.generate_response(\n            request.message,\n            request.max_length,\n            request.temperature\n        )\n        return {"response": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get("/health")\nasync def health_check():\n    return {"status": "healthy", "device": str(agent.device)}\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-3-langchain-with-local-models",children:"Method 3: LangChain with Local Models"}),"\n",(0,i.jsx)(n.h3,{id:"setup-langchain-environment",children:"Setup LangChain Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from langchain.llms import LlamaCpp\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\n\nclass LocalAIAgent:\n    def __init__(self, model_path):\n        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n    \n        self.llm = LlamaCpp(\n            model_path=model_path,\n            temperature=0.7,\n            max_tokens=512,\n            top_p=1,\n            callback_manager=callback_manager,\n            verbose=True,\n            n_ctx=2048,\n            n_gpu_layers=35  # Adjust based on your GPU\n        )\n    \n        self.memory = ConversationBufferMemory()\n    \n        template = """\n        You are a helpful AI assistant. Have a conversation with the human.\n    \n        Current conversation:\n        {history}\n        Human: {input}\n        AI Assistant:"""\n    \n        prompt = PromptTemplate(\n            input_variables=["history", "input"],\n            template=template\n        )\n    \n        self.conversation = ConversationChain(\n            llm=self.llm,\n            memory=self.memory,\n            prompt=prompt,\n            verbose=True\n        )\n  \n    def chat(self, message):\n        return self.conversation.predict(input=message)\n\n# Usage\nagent = LocalAIAgent("./models/llama-2-7b-chat.gguf")\nresponse = agent.chat("What is machine learning?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-4-multi-modal-ai-agent",children:"Method 4: Multi-Modal AI Agent"}),"\n",(0,i.jsx)(n.h3,{id:"vision-language-model-setup",children:"Vision-Language Model Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nclass MultiModalAgent:\n    def __init__(self):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n    \n        # Load vision-language model\n        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\n        self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n        self.model.to(self.device)\n  \n    def analyze_image(self, image_path_or_url, question=None):\n        # Load image\n        if image_path_or_url.startswith(\'http\'):\n            response = requests.get(image_path_or_url)\n            image = Image.open(BytesIO(response.content))\n        else:\n            image = Image.open(image_path_or_url)\n    \n        if question:\n            # Visual question answering\n            inputs = self.processor(image, question, return_tensors="pt").to(self.device)\n            out = self.model.generate(**inputs, max_length=50)\n            answer = self.processor.decode(out[0], skip_special_tokens=True)\n            return answer\n        else:\n            # Image captioning\n            inputs = self.processor(image, return_tensors="pt").to(self.device)\n            out = self.model.generate(**inputs, max_length=50)\n            caption = self.processor.decode(out[0], skip_special_tokens=True)\n            return caption\n\n# Usage\nagent = MultiModalAgent()\ncaption = agent.analyze_image("path/to/image.jpg")\nanswer = agent.analyze_image("path/to/image.jpg", "What color is the car?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"method-5-rag-retrieval-augmented-generation-system",children:"Method 5: RAG (Retrieval-Augmented Generation) System"}),"\n",(0,i.jsx)(n.h3,{id:"vector-database-setup",children:"Vector Database Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\n\nclass RAGAgent:\n    def __init__(self, documents_path, persist_directory="./chroma_db"):\n        # Initialize embeddings\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name="sentence-transformers/all-MiniLM-L6-v2"\n        )\n    \n        # Load and process documents\n        loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader)\n        documents = loader.load()\n    \n        # Split documents\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        texts = text_splitter.split_documents(documents)\n    \n        # Create vector store\n        self.vectorstore = Chroma.from_documents(\n            documents=texts,\n            embedding=self.embeddings,\n            persist_directory=persist_directory\n        )\n    \n        # Initialize LLM (using Ollama)\n        from langchain.llms import Ollama\n        self.llm = Ollama(model="llama2")\n  \n    def query(self, question, k=3):\n        # Retrieve relevant documents\n        docs = self.vectorstore.similarity_search(question, k=k)\n    \n        # Create context from retrieved documents\n        context = "\\n\\n".join([doc.page_content for doc in docs])\n    \n        # Generate response\n        prompt = f"""\n        Based on the following context, answer the question:\n    \n        Context:\n        {context}\n    \n        Question: {question}\n    \n        Answer:"""\n    \n        response = self.llm(prompt)\n        return response, docs\n\n# Usage\nrag_agent = RAGAgent("./documents")\nanswer, sources = rag_agent.query("What is the main topic discussed?")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Check GPU availability\nimport torch\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU count: {torch.cuda.device_count()}")\nif torch.cuda.is_available():\n    print(f"GPU name: {torch.cuda.get_device_name(0)}")\n\n# Optimize memory usage\ntorch.cuda.empty_cache()\n\n# Use mixed precision\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    # Your model inference here\n    pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type="nf4"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "model_name",\n    quantization_config=quantization_config,\n    device_map="auto"\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,i.jsx)(n.h3,{id:"system-monitoring",children:"System Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import psutil\nimport GPUtil\nimport logging\nfrom datetime import datetime\n\nclass SystemMonitor:\n    def __init__(self):\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('ai_agent.log'),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n  \n    def log_system_stats(self):\n        # CPU usage\n        cpu_percent = psutil.cpu_percent(interval=1)\n    \n        # Memory usage\n        memory = psutil.virtual_memory()\n        memory_percent = memory.percent\n    \n        # GPU usage\n        gpus = GPUtil.getGPUs()\n        gpu_stats = []\n        for gpu in gpus:\n            gpu_stats.append({\n                'id': gpu.id,\n                'name': gpu.name,\n                'load': gpu.load * 100,\n                'memory_used': gpu.memoryUsed,\n                'memory_total': gpu.memoryTotal,\n                'temperature': gpu.temperature\n            })\n    \n        self.logger.info(f\"CPU: {cpu_percent}%, Memory: {memory_percent}%\")\n        for gpu_stat in gpu_stats:\n            self.logger.info(f\"GPU {gpu_stat['id']}: {gpu_stat['load']:.1f}% load, \"\n                           f\"{gpu_stat['memory_used']}/{gpu_stat['memory_total']}MB memory\")\n\nmonitor = SystemMonitor()\nmonitor.log_system_stats()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"api-security",children:"API Security"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nimport jwt\nimport hashlib\nimport os\n\napp = FastAPI()\nsecurity = HTTPBearer()\n\nSECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    try:\n        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=["HS256"])\n        return payload\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail="Invalid authentication credentials"\n        )\n\n@app.post("/secure-chat")\nasync def secure_chat(request: ChatRequest, user=Depends(verify_token)):\n    # Your secure chat logic here\n    pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"input-sanitization",children:"Input Sanitization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import str\n\ndef sanitize_input(text: str) -> str:\n    # Remove potentially harmful characters\n    text = re.sub(r'[<>\"\\']', '', text)\n  \n    # Limit length\n    text = text[:1000]\n  \n    # Remove excessive whitespace\n    text = ' '.join(text.split())\n  \n    return text\n\ndef validate_input(text: str) -> bool:\n    # Check for common injection patterns\n    dangerous_patterns = [\n        r'<script',\n        r'javascript:',\n        r'eval\\(',\n        r'exec\\(',\n        r'import\\s+os',\n        r'__import__'\n    ]\n  \n    for pattern in dangerous_patterns:\n        if re.search(pattern, text, re.IGNORECASE):\n            return False\n  \n    return True\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment-scripts",children:"Deployment Scripts"}),"\n",(0,i.jsx)(n.h3,{id:"automated-setup-script",children:"Automated Setup Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n\n# setup_ai_agent.sh\n\nset -e\n\necho "Setting up Local AI Agent Environment..."\n\n# Update system\nsudo apt update && sudo apt upgrade -y\n\n# Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\nsudo usermod -aG docker $USER\n\n# Install Docker Compose\nsudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\n# Install NVIDIA Container Toolkit (if GPU present)\nif lspci | grep -i nvidia; then\n    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n    sudo apt-get update && sudo apt-get install -y nvidia-docker2\n    sudo systemctl restart docker\nfi\n\n# Install Python dependencies\npip3 install --upgrade pip\npip3 install -r requirements.txt\n\n# Download models\nmkdir -p models\ncd models\n\n# Download Llama 2 model (example)\nwget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf\n\necho "Setup complete! Run \'docker-compose up\' to start the AI agent."\n'})}),"\n",(0,i.jsx)(n.h3,{id:"systemd-service",children:"Systemd Service"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ini",children:"# /etc/systemd/system/ai-agent.service\n\n[Unit]\nDescription=Local AI Agent Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=aiagent\nWorkingDirectory=/opt/ai-agent\nExecStart=/usr/local/bin/docker-compose up\nExecStop=/usr/local/bin/docker-compose down\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Out of Memory Errors:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Reduce batch size\nbatch_size = 1\n\n# Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Clear cache regularly\ntorch.cuda.empty_cache()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Slow Inference:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Use torch.no_grad() for inference\nwith torch.no_grad():\n    output = model(input_ids)\n\n# Optimize for inference\nmodel.eval()\ntorch.backends.cudnn.benchmark = True\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Model Loading Issues:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Check available disk space\nimport shutil\nfree_space = shutil.disk_usage(\'.\').free / (1024**3)  # GB\nprint(f"Free space: {free_space:.2f} GB")\n\n# Use model caching\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained("model_name", cache_dir="./model_cache")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Management"}),": Monitor CPU, GPU, and memory usage continuously"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Selection"}),": Choose models appropriate for your hardware capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caching"}),": Implement proper caching for models and embeddings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging"}),": Maintain comprehensive logs for debugging and monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security"}),": Implement proper authentication and input validation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Backup"}),": Regular backup of models and configuration files"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Updates"}),": Keep dependencies and models updated"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing"}),": Implement comprehensive testing for all components"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Local AI agent deployment offers significant advantages in terms of privacy, control, and cost-effectiveness. The methods outlined in this guide provide various approaches depending on your specific requirements, from simple chatbots using Ollama to complex multi-modal RAG systems."}),"\n",(0,i.jsx)(n.p,{children:"Choose the approach that best fits your hardware capabilities, technical requirements, and use case. Start with simpler methods like Ollama for proof-of-concept, then scale up to more complex deployments as needed."}),"\n",(0,i.jsx)(n.p,{children:"Remember to continuously monitor performance, implement proper security measures, and maintain your deployment for optimal results."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Last updated: September 2025"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},82397:e=>{e.exports=JSON.parse('{"permalink":"/zh-Hans/blog/local-ai-agent-deployment","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-08-20-local-ai-agent-deployment.md","source":"@site/blog/2025-08-20-local-ai-agent-deployment.md","title":"Guide to Local AI Agent","description":"Introduction","date":"2025-08-20T00:00:00.000Z","tags":[{"inline":false,"label":"AI","permalink":"/zh-Hans/blog/tags/ai-alt","description":"Artificial Intelligence"},{"inline":false,"label":"Machine Learning","permalink":"/zh-Hans/blog/tags/machine-learning","description":"Machine learning techniques and applications"},{"inline":false,"label":"Deployment","permalink":"/zh-Hans/blog/tags/deployment","description":"Software deployment and infrastructure"},{"inline":false,"label":"Tutorial","permalink":"/zh-Hans/blog/tags/tutorial","description":"Tutorial tag description"},{"inline":false,"label":"Local Development","permalink":"/zh-Hans/blog/tags/local-development","description":"Local development environments and tools"}],"readingTime":9.96,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"Ph.D. Candidate @ SHZU @CAS-Cemps","url":"https://github.com/smiler488","page":{"permalink":"/zh-Hans/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"/zh-Hans/img/cv_person.png","key":"liangchao"}],"frontMatter":{"slug":"local-ai-agent-deployment","title":"Guide to Local AI Agent","authors":["liangchao"],"tags":["ai","machine learning","deployment","tutorial","local development"]},"unlisted":false,"prevItem":{"title":"Predicting Leaf Optical Properties with BRDF and Phenotypic Traits","permalink":"/zh-Hans/blog/brdf-paper"},"nextItem":{"title":"Guide to 3D Reconstruction with AI","permalink":"/zh-Hans/blog/hunyuan3d-plant-reconstruction-guide"}}')}}]);