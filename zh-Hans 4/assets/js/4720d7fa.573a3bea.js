"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[7030],{365:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>a,toc:()=>d});var a=t(90087),r=t(74848),i=t(28453);const o={slug:"local-llm-training-guide-en",title:"Guide to Local LLM",authors:["liangchao"],tags:["LLM","AI","training","fine-tuning","local deployment","machine learning"]},l="Guide to Local LLM Deployment, Training and Fine-tuning",s={authorsImageUrls:[void 0]},d=[{value:"Technical Workflow Overview",id:"technical-workflow-overview",level:2},{value:"Environment Setup",id:"environment-setup",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Environment Setup",id:"software-environment-setup",level:3},{value:"Model Deployment",id:"model-deployment",level:2},{value:"Quick Deployment with Ollama",id:"quick-deployment-with-ollama",level:3},{value:"High-Performance Deployment with vLLM",id:"high-performance-deployment-with-vllm",level:3},{value:"Using Text Generation WebUI",id:"using-text-generation-webui",level:3},{value:"Data Preparation and Preprocessing",id:"data-preparation-and-preprocessing",level:2},{value:"Dataset Formats",id:"dataset-formats",level:3},{value:"Data Preprocessing Script",id:"data-preprocessing-script",level:3},{value:"LoRA Fine-tuning",id:"lora-fine-tuning",level:2},{value:"Efficient Fine-tuning with Unsloth",id:"efficient-fine-tuning-with-unsloth",level:3},{value:"Professional Fine-tuning with Axolotl",id:"professional-fine-tuning-with-axolotl",level:3},{value:"Full Parameter Fine-tuning",id:"full-parameter-fine-tuning",level:2},{value:"Large Model Training with DeepSpeed",id:"large-model-training-with-deepspeed",level:3},{value:"Pre-training",id:"pre-training",level:2},{value:"Pre-training from Scratch",id:"pre-training-from-scratch",level:3},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Automatic Evaluation Metrics",id:"automatic-evaluation-metrics",level:3},{value:"Human Evaluation Framework",id:"human-evaluation-framework",level:3},{value:"Model Quantization and Optimization",id:"model-quantization-and-optimization",level:2},{value:"GPTQ Quantization",id:"gptq-quantization",level:3},{value:"AWQ Quantization",id:"awq-quantization",level:3},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Docker Containerization",id:"docker-containerization",level:3},{value:"Kubernetes Deployment",id:"kubernetes-deployment",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:2},{value:"Prometheus Monitoring",id:"prometheus-monitoring",level:3},{value:"Structured Logging",id:"structured-logging",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:3},{value:"Security Considerations",id:"security-considerations",level:3},{value:"Cost Control",id:"cost-control",level:3}];function p(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"This comprehensive guide covers the complete process of deploying, training, and fine-tuning large language models in local environments, from environment setup to production deployment."}),"\n",(0,r.jsx)(n.h2,{id:"technical-workflow-overview",children:"Technical Workflow Overview"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    A[Environment Setup] --\x3e B[Hardware & Software Configuration]\n    B --\x3e C[Model Deployment]\n    C --\x3e D[Data Preparation]\n    D --\x3e E[Fine-tuning Strategy]\n    E --\x3e F[LoRA Fine-tuning]\n    E --\x3e G[Full Parameter Training]\n    F --\x3e H[Model Evaluation]\n    G --\x3e H\n    H --\x3e I[Production Deployment]\n    I --\x3e J[Performance Monitoring]\n    \n    B --\x3e B1[GPU/CPU Requirements]\n    B --\x3e B2[Software Dependencies]\n    \n    C --\x3e C1[Ollama Deployment]\n    C --\x3e C2[vLLM High-Performance]\n    C --\x3e C3[Text Generation WebUI]\n    \n    D --\x3e D1[Dataset Formatting]\n    D --\x3e D2[Data Preprocessing]\n    D --\x3e D3[Quality Validation]\n    \n    E --\x3e E1[Parameter Selection]\n    E --\x3e E2[Hyperparameter Tuning]\n    \n    F --\x3e F1[Unsloth Framework]\n    F --\x3e F2[Axolotl Configuration]\n    \n    G --\x3e G1[DeepSpeed Optimization]\n    G --\x3e G2[Memory Management]\n    \n    H --\x3e H1[Accuracy Metrics]\n    H --\x3e H2[Performance Benchmarks]\n    \n    I --\x3e I1[API Integration]\n    I --\x3e I2[Scalability Testing]"}),"\n",(0,r.jsx)(n.p,{children:"This workflow illustrates the end-to-end process for local LLM training and deployment, highlighting key decision points and alternative approaches at each stage."}),"\n",(0,r.jsx)(n.h2,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,r.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Minimum Configuration (7B models):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPU: RTX 3090/4090 (24GB VRAM) or A100 (40GB)"}),"\n",(0,r.jsx)(n.li,{children:"CPU: 16+ cores"}),"\n",(0,r.jsx)(n.li,{children:"Memory: 64GB DDR4/DDR5"}),"\n",(0,r.jsx)(n.li,{children:"Storage: 1TB NVMe SSD"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Recommended Configuration (13B-70B models):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPU: Multi-card A100/H100 (80GB VRAM)"}),"\n",(0,r.jsx)(n.li,{children:"CPU: 32+ cores"}),"\n",(0,r.jsx)(n.li,{children:"Memory: 128GB+"}),"\n",(0,r.jsx)(n.li,{children:"Storage: 2TB NVMe SSD"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"software-environment-setup",children:"Software Environment Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Create conda environment\nconda create -n llm-training python=3.10\nconda activate llm-training\n\n# Install PyTorch (CUDA 12.1)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# Install core dependencies\npip install transformers datasets accelerate\npip install deepspeed bitsandbytes\npip install wandb tensorboard\npip install flash-attn --no-build-isolation\n\n# Install training frameworks\npip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\npip install axolotl[flash-attn,deepspeed] @ git+https://github.com/OpenAccess-AI-Collective/axolotl.git\n"})}),"\n",(0,r.jsx)(n.h2,{id:"model-deployment",children:"Model Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"quick-deployment-with-ollama",children:"Quick Deployment with Ollama"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Download and run models\nollama pull llama2:7b\nollama pull qwen2:7b\nollama pull codellama:7b\n\n# Start API service\nollama serve\n\n# Test API\ncurl http://localhost:11434/api/generate -d \'{\n  "model": "llama2:7b",\n  "prompt": "Why is the sky blue?",\n  "stream": false\n}\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"high-performance-deployment-with-vllm",children:"High-Performance Deployment with vLLM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\nimport torch\n\n# Check GPU availability\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU count: {torch.cuda.device_count()}")\n\n# Initialize vLLM\nllm = LLM(\n    model="Qwen/Qwen2-7B-Instruct",\n    tensor_parallel_size=1,  # Number of GPUs\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n    trust_remote_code=True,\n    dtype="half"  # Use FP16 to save VRAM\n)\n\n# Set sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=512\n)\n\n# Batch inference\nprompts = [\n    "Explain what machine learning is",\n    "Write a Python sorting algorithm",\n    "Introduce basic concepts of deep learning"\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f"Prompt: {prompt}")\n    print(f"Generated: {generated_text}")\n    print("-" * 50)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"using-text-generation-webui",children:"Using Text Generation WebUI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Clone repository\ngit clone https://github.com/oobabooga/text-generation-webui.git\ncd text-generation-webui\n\n# Install dependencies\npip install -r requirements.txt\n\n# Start WebUI\npython server.py --model-dir ./models --listen --api\n\n# Download models to models directory\n# Supports HuggingFace, GGUF, AWQ, GPTQ formats\n"})}),"\n",(0,r.jsx)(n.h2,{id:"data-preparation-and-preprocessing",children:"Data Preparation and Preprocessing"}),"\n",(0,r.jsx)(n.h3,{id:"dataset-formats",children:"Dataset Formats"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Instruction Fine-tuning Format (Alpaca):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "instruction": "Please explain what artificial intelligence is",\n  "input": "",\n  "output": "Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence..."\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Conversation Format (ChatML):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "messages": [\n    {"role": "system", "content": "You are a helpful AI assistant"},\n    {"role": "user", "content": "What is deep learning?"},\n    {"role": "assistant", "content": "Deep learning is a subset of machine learning..."}\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"data-preprocessing-script",children:"Data Preprocessing Script"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import json\nimport pandas as pd\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer\n\ndef prepare_alpaca_dataset(data_path, tokenizer, max_length=2048):\n    """Prepare Alpaca format dataset"""\n  \n    # Load data\n    with open(data_path, \'r\', encoding=\'utf-8\') as f:\n        data = json.load(f)\n  \n    def format_prompt(example):\n        if example[\'input\']:\n            prompt = f"### Instruction:\\n{example[\'instruction\']}\\n\\n### Input:\\n{example[\'input\']}\\n\\n### Response:\\n"\n        else:\n            prompt = f"### Instruction:\\n{example[\'instruction\']}\\n\\n### Response:\\n"\n    \n        full_text = prompt + example[\'output\']\n        return {"text": full_text}\n  \n    # Format data\n    formatted_data = [format_prompt(item) for item in data]\n    dataset = Dataset.from_list(formatted_data)\n  \n    # Tokenize\n    def tokenize_function(examples):\n        return tokenizer(\n            examples["text"],\n            truncation=True,\n            padding=False,\n            max_length=max_length,\n            return_overflowing_tokens=False,\n        )\n  \n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=dataset.column_names\n    )\n  \n    return tokenized_dataset\n\n# Usage example\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_dataset = prepare_alpaca_dataset("train_data.json", tokenizer)\neval_dataset = prepare_alpaca_dataset("eval_data.json", tokenizer)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"lora-fine-tuning",children:"LoRA Fine-tuning"}),"\n",(0,r.jsx)(n.h3,{id:"efficient-fine-tuning-with-unsloth",children:"Efficient Fine-tuning with Unsloth"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from unsloth import FastLanguageModel\nimport torch\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name="unsloth/qwen2-7b-bnb-4bit",  # 4bit quantized version\n    max_seq_length=2048,\n    dtype=None,  # Auto detect\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",\n                   "gate_proj", "up_proj", "down_proj"],\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias="none",\n    use_gradient_checkpointing="unsloth",\n    random_state=3407,\n)\n\n# Training configuration\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    dataset_text_field="text",\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=1,\n        optim="adamw_8bit",\n        weight_decay=0.01,\n        lr_scheduler_type="linear",\n        seed=3407,\n        output_dir="outputs",\n        save_steps=25,\n        eval_steps=25,\n        evaluation_strategy="steps",\n        load_best_model_at_end=True,\n        metric_for_best_model="eval_loss",\n        greater_is_better=False,\n    ),\n)\n\n# Start training\ntrainer_stats = trainer.train()\n\n# Save model\nmodel.save_pretrained("lora_model")\ntokenizer.save_pretrained("lora_model")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"professional-fine-tuning-with-axolotl",children:"Professional Fine-tuning with Axolotl"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Configuration file (config.yml):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"base_model: Qwen/Qwen2-7B-Instruct\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: ./data/train.jsonl\n    type: alpaca\n    conversation: false\n\ndataset_prepared_path: ./prepared_data\nval_set_size: 0.1\noutput_dir: ./outputs\n\nsequence_len: 2048\nsample_packing: true\npad_to_sequence_len: true\n\nadapter: lora\nlora_model_dir:\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: llm-finetune\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\n\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Start training:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Prepare data\npython -m axolotl.cli.preprocess config.yml\n\n# Start training\npython -m axolotl.cli.train config.yml\n\n# Inference test\npython -m axolotl.cli.inference config.yml --lora_model_dir="./outputs"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"full-parameter-fine-tuning",children:"Full Parameter Fine-tuning"}),"\n",(0,r.jsx)(n.h3,{id:"large-model-training-with-deepspeed",children:"Large Model Training with DeepSpeed"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"DeepSpeed configuration (ds_config.json):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "fp16": {\n    "enabled": "auto",\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 16,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n  "bf16": {\n    "enabled": "auto"\n  },\n  "optimizer": {\n    "type": "AdamW",\n    "params": {\n      "lr": "auto",\n      "betas": "auto",\n      "eps": "auto",\n      "weight_decay": "auto"\n    }\n  },\n  "scheduler": {\n    "type": "WarmupLR",\n    "params": {\n      "warmup_min_lr": "auto",\n      "warmup_max_lr": "auto",\n      "warmup_num_steps": "auto"\n    }\n  },\n  "zero_optimization": {\n    "stage": 3,\n    "offload_optimizer": {\n      "device": "cpu",\n      "pin_memory": true\n    },\n    "offload_param": {\n      "device": "cpu",\n      "pin_memory": true\n    },\n    "overlap_comm": true,\n    "contiguous_gradients": true,\n    "sub_group_size": 1e9,\n    "reduce_bucket_size": "auto",\n    "stage3_prefetch_bucket_size": "auto",\n    "stage3_param_persistence_threshold": "auto",\n    "stage3_max_live_parameters": 1e9,\n    "stage3_max_reuse_distance": 1e9,\n    "stage3_gather_16bit_weights_on_model_save": true\n  },\n  "gradient_accumulation_steps": "auto",\n  "gradient_clipping": "auto",\n  "steps_per_print": 2000,\n  "train_batch_size": "auto",\n  "train_micro_batch_size_per_gpu": "auto",\n  "wall_clock_breakdown": false\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training script:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nimport deepspeed\n\ndef main():\n    # Model and tokenizer\n    model_name = "Qwen/Qwen2-7B"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n  \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True\n    )\n  \n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n  \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir="./full_finetune_output",\n        overwrite_output_dir=True,\n        num_train_epochs=3,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=8,\n        evaluation_strategy="steps",\n        eval_steps=500,\n        save_steps=1000,\n        logging_steps=100,\n        learning_rate=5e-5,\n        weight_decay=0.01,\n        warmup_steps=100,\n        lr_scheduler_type="cosine",\n        bf16=True,\n        dataloader_pin_memory=False,\n        deepspeed="ds_config.json",\n        report_to="wandb",\n        run_name="qwen2-7b-full-finetune"\n    )\n  \n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n  \n    # Start training\n    trainer.train()\n  \n    # Save model\n    trainer.save_model()\n    tokenizer.save_pretrained(training_args.output_dir)\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Start multi-GPU training:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"deepspeed --num_gpus=4 train_full.py\n"})}),"\n",(0,r.jsx)(n.h2,{id:"pre-training",children:"Pre-training"}),"\n",(0,r.jsx)(n.h3,{id:"pre-training-from-scratch",children:"Pre-training from Scratch"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data preparation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport multiprocessing\n\ndef prepare_pretraining_data():\n    # Load large-scale text data\n    dataset = load_dataset("wikitext", "wikitext-103-raw-v1")\n  \n    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\n  \n    def tokenize_function(examples):\n        return tokenizer(\n            examples["text"],\n            truncation=True,\n            padding=False,\n            max_length=2048,\n            return_overflowing_tokens=True,\n            return_length=True,\n        )\n  \n    # Parallel processing\n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        num_proc=multiprocessing.cpu_count(),\n        remove_columns=dataset["train"].column_names,\n    )\n  \n    return tokenized_dataset\n\n# Group texts\ndef group_texts(examples, block_size=2048):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n  \n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n  \n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result["labels"] = result["input_ids"].copy()\n    return result\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pre-training configuration:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer\n)\n\n# Model configuration\nconfig = AutoConfig.from_pretrained("Qwen/Qwen2-7B")\nconfig.vocab_size = len(tokenizer)\n\n# Initialize model\nmodel = AutoModelForCausalLM.from_config(config)\n\n# Pre-training parameters\ntraining_args = TrainingArguments(\n    output_dir="./pretrain_output",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=16,\n    save_steps=10000,\n    logging_steps=1000,\n    learning_rate=1e-4,\n    weight_decay=0.1,\n    warmup_steps=10000,\n    lr_scheduler_type="cosine",\n    bf16=True,\n    deepspeed="ds_config_pretrain.json",\n    dataloader_num_workers=4,\n    remove_unused_columns=False,\n)\n\n# Pre-training\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=grouped_dataset["train"],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\ntrainer.train()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"model-evaluation",children:"Model Evaluation"}),"\n",(0,r.jsx)(n.h3,{id:"automatic-evaluation-metrics",children:"Automatic Evaluation Metrics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import pipeline\nfrom datasets import load_metric\nimport numpy as np\n\ndef evaluate_model(model_path, test_dataset):\n    # Load model\n    generator = pipeline(\n        "text-generation",\n        model=model_path,\n        tokenizer=model_path,\n        torch_dtype=torch.float16,\n        device_map="auto"\n    )\n  \n    # BLEU score\n    bleu_metric = load_metric("bleu")\n  \n    predictions = []\n    references = []\n  \n    for example in test_dataset:\n        # Generate response\n        prompt = example["instruction"]\n        generated = generator(\n            prompt,\n            max_length=512,\n            num_return_sequences=1,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=generator.tokenizer.eos_token_id\n        )[0]["generated_text"]\n    \n        # Extract generated part\n        generated_text = generated[len(prompt):].strip()\n    \n        predictions.append(generated_text)\n        references.append([example["output"]])\n  \n    # Calculate BLEU score\n    bleu_score = bleu_metric.compute(\n        predictions=predictions,\n        references=references\n    )\n  \n    print(f"BLEU Score: {bleu_score[\'bleu\']:.4f}")\n  \n    return {\n        "bleu": bleu_score["bleu"],\n        "predictions": predictions,\n        "references": references\n    }\n\n# Perplexity evaluation\ndef calculate_perplexity(model, tokenizer, test_texts):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n  \n    with torch.no_grad():\n        for text in test_texts:\n            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n            outputs = model(**inputs, labels=inputs["input_ids"])\n            loss = outputs.loss\n        \n            total_loss += loss.item() * inputs["input_ids"].size(1)\n            total_tokens += inputs["input_ids"].size(1)\n  \n    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n    return perplexity.item()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"human-evaluation-framework",children:"Human Evaluation Framework"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import gradio as gr\nimport json\nfrom datetime import datetime\n\nclass ModelEvaluator:\n    def __init__(self, models_dict):\n        self.models = models_dict\n        self.results = []\n  \n    def create_evaluation_interface(self):\n        def evaluate_response(prompt, model_name, response, relevance, accuracy, fluency, helpfulness, comments):\n            result = {\n                "timestamp": datetime.now().isoformat(),\n                "prompt": prompt,\n                "model": model_name,\n                "response": response,\n                "scores": {\n                    "relevance": relevance,\n                    "accuracy": accuracy,\n                    "fluency": fluency,\n                    "helpfulness": helpfulness\n                },\n                "comments": comments,\n                "overall_score": (relevance + accuracy + fluency + helpfulness) / 4\n            }\n        \n            self.results.append(result)\n        \n            # Save results\n            with open("evaluation_results.json", "w", encoding="utf-8") as f:\n                json.dump(self.results, f, ensure_ascii=False, indent=2)\n        \n            return f"Evaluation saved! Overall score: {result[\'overall_score\']:.2f}"\n    \n        def generate_response(prompt, model_name):\n            if model_name in self.models:\n                generator = self.models[model_name]\n                response = generator(prompt, max_length=512, temperature=0.7)[0]["generated_text"]\n                return response[len(prompt):].strip()\n            return "Model not found"\n    \n        # Gradio interface\n        with gr.Blocks(title="LLM Model Evaluation System") as demo:\n            gr.Markdown("# LLM Model Evaluation System")\n        \n            with gr.Row():\n                with gr.Column():\n                    prompt_input = gr.Textbox(label="Input Prompt", lines=3)\n                    model_dropdown = gr.Dropdown(\n                        choices=list(self.models.keys()),\n                        label="Select Model",\n                        value=list(self.models.keys())[0] if self.models else None\n                    )\n                    generate_btn = gr.Button("Generate Response")\n            \n                with gr.Column():\n                    response_output = gr.Textbox(label="Model Response", lines=5)\n        \n            gr.Markdown("## Evaluation Metrics (1-5 scale)")\n        \n            with gr.Row():\n                relevance_slider = gr.Slider(1, 5, value=3, label="Relevance")\n                accuracy_slider = gr.Slider(1, 5, value=3, label="Accuracy")\n                fluency_slider = gr.Slider(1, 5, value=3, label="Fluency")\n                helpfulness_slider = gr.Slider(1, 5, value=3, label="Helpfulness")\n        \n            comments_input = gr.Textbox(label="Comments", lines=2)\n            evaluate_btn = gr.Button("Submit Evaluation")\n            result_output = gr.Textbox(label="Evaluation Result")\n        \n            # Event binding\n            generate_btn.click(\n                generate_response,\n                inputs=[prompt_input, model_dropdown],\n                outputs=response_output\n            )\n        \n            evaluate_btn.click(\n                evaluate_response,\n                inputs=[\n                    prompt_input, model_dropdown, response_output,\n                    relevance_slider, accuracy_slider, fluency_slider, helpfulness_slider,\n                    comments_input\n                ],\n                outputs=result_output\n            )\n    \n        return demo\n\n# Usage example\nmodels = {\n    "Original Model": pipeline("text-generation", model="Qwen/Qwen2-7B"),\n    "Fine-tuned Model": pipeline("text-generation", model="./lora_model")\n}\n\nevaluator = ModelEvaluator(models)\ndemo = evaluator.create_evaluation_interface()\ndemo.launch(share=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"model-quantization-and-optimization",children:"Model Quantization and Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"gptq-quantization",children:"GPTQ Quantization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom transformers import AutoTokenizer\n\n# Quantization configuration\nquantize_config = BaseQuantizeConfig(\n    bits=4,  # 4bit quantization\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.1,\n    sym=True,\n    true_sequential=True,\n)\n\n# Load model\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    "Qwen/Qwen2-7B",\n    quantize_config=quantize_config,\n    low_cpu_mem_usage=True,\n    device_map="auto"\n)\n\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")\n\n# Prepare calibration data\ncalibration_dataset = [\n    "What is the history of artificial intelligence development?",\n    "Please explain the basic principles of deep learning.",\n    "What are the main algorithms in machine learning?",\n    # More calibration data...\n]\n\n# Execute quantization\nmodel.quantize(calibration_dataset)\n\n# Save quantized model\nmodel.save_quantized("./qwen2-7b-gptq")\ntokenizer.save_pretrained("./qwen2-7b-gptq")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"awq-quantization",children:"AWQ Quantization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n# Load model\nmodel_path = "Qwen/Qwen2-7B"\nquant_path = "qwen2-7b-awq"\n\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantization configuration\nquant_config = {\n    "zero_point": True,\n    "q_group_size": 128,\n    "w_bit": 4,\n    "version": "GEMM"\n}\n\n# Execute quantization\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f"Quantized model saved to: {quant_path}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"production-deployment",children:"Production Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"docker-containerization",children:"Docker Containerization"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dockerfile:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-dockerfile",children:'FROM nvidia/cuda:12.1-devel-ubuntu22.04\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    python3.10 python3-pip git wget curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements file\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Set environment variables\nENV PYTHONPATH=/app\nENV CUDA_VISIBLE_DEVICES=0\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Start command\nCMD ["python", "serve.py"]\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Service script (serve.py):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\nimport torch\nimport uvicorn\nfrom typing import Optional\n\napp = FastAPI(title="LLM Service API", version="1.0.0")\n\n# Global variables\ngenerator = None\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    max_length: int = 512\n    temperature: float = 0.7\n    top_p: float = 0.9\n    do_sample: bool = True\n\nclass GenerateResponse(BaseModel):\n    generated_text: str\n    prompt: str\n\n@app.on_event("startup")\nasync def load_model():\n    global generator\n    print("Loading model...")\n  \n    generator = pipeline(\n        "text-generation",\n        model="./qwen2-7b-awq",  # Quantized model path\n        tokenizer="./qwen2-7b-awq",\n        torch_dtype=torch.float16,\n        device_map="auto",\n        trust_remote_code=True\n    )\n  \n    print("Model loaded successfully!")\n\n@app.get("/health")\nasync def health_check():\n    return {"status": "healthy", "model_loaded": generator is not None}\n\n@app.post("/generate", response_model=GenerateResponse)\nasync def generate_text(request: GenerateRequest):\n    if generator is None:\n        raise HTTPException(status_code=503, detail="Model not loaded")\n  \n    try:\n        result = generator(\n            request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            top_p=request.top_p,\n            do_sample=request.do_sample,\n            pad_token_id=generator.tokenizer.eos_token_id\n        )\n    \n        generated_text = result[0]["generated_text"][len(request.prompt):].strip()\n    \n        return GenerateResponse(\n            generated_text=generated_text,\n            prompt=request.prompt\n        )\n  \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"kubernetes-deployment",children:"Kubernetes Deployment"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"deployment.yaml:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-service\n  labels:\n    app: llm-service\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: llm-service\n  template:\n    metadata:\n      labels:\n        app: llm-service\n    spec:\n      containers:\n      - name: llm-service\n        image: your-registry/llm-service:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            nvidia.com/gpu: 1\n            memory: "16Gi"\n            cpu: "4"\n          limits:\n            nvidia.com/gpu: 1\n            memory: "24Gi"\n            cpu: "6"\n        env:\n        - name: MODEL_PATH\n          value: "/models/qwen2-7b-awq"\n        - name: CUDA_VISIBLE_DEVICES\n          value: "0"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n      nodeSelector:\n        accelerator: nvidia-gpu\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: llm-service\nspec:\n  selector:\n    app: llm-service\n  ports:\n  - port: 80\n    targetPort: 8000\n    protocol: TCP\n  type: LoadBalancer\n'})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,r.jsx)(n.h3,{id:"prometheus-monitoring",children:"Prometheus Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\nimport functools\n\n# Define metrics\nREQUEST_COUNT = Counter('llm_requests_total', 'Total requests', ['method', 'endpoint', 'status'])\nREQUEST_LATENCY = Histogram('llm_request_duration_seconds', 'Request latency')\nACTIVE_REQUESTS = Gauge('llm_active_requests', 'Active requests')\nGPU_MEMORY = Gauge('llm_gpu_memory_usage_bytes', 'GPU memory usage', ['gpu_id'])\nMODEL_LOAD_TIME = Gauge('llm_model_load_time_seconds', 'Model load time')\n\ndef monitor_requests(func):\n    @functools.wraps(func)\n    async def wrapper(*args, **kwargs):\n        start_time = time.time()\n        ACTIVE_REQUESTS.inc()\n    \n        try:\n            result = await func(*args, **kwargs)\n            REQUEST_COUNT.labels(method='POST', endpoint='/generate', status='success').inc()\n            return result\n        except Exception as e:\n            REQUEST_COUNT.labels(method='POST', endpoint='/generate', status='error').inc()\n            raise\n        finally:\n            ACTIVE_REQUESTS.dec()\n            REQUEST_LATENCY.observe(time.time() - start_time)\n  \n    return wrapper\n\n# Use in FastAPI\n@app.post(\"/generate\")\n@monitor_requests\nasync def generate_text(request: GenerateRequest):\n    # Original logic\n    pass\n\n# Start Prometheus metrics server\nstart_http_server(9090)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"structured-logging",children:"Structured Logging"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nimport json\nfrom datetime import datetime\nimport sys\n\nclass StructuredLogger:\n    def __init__(self, name):\n        self.logger = logging.getLogger(name)\n        self.logger.setLevel(logging.INFO)\n    \n        # Create handler\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(self.JSONFormatter())\n    \n        self.logger.addHandler(handler)\n  \n    class JSONFormatter(logging.Formatter):\n        def format(self, record):\n            log_entry = {\n                "timestamp": datetime.utcnow().isoformat(),\n                "level": record.levelname,\n                "logger": record.name,\n                "message": record.getMessage(),\n                "module": record.module,\n                "function": record.funcName,\n                "line": record.lineno\n            }\n        \n            # Add extra fields\n            if hasattr(record, \'user_id\'):\n                log_entry[\'user_id\'] = record.user_id\n            if hasattr(record, \'request_id\'):\n                log_entry[\'request_id\'] = record.request_id\n            if hasattr(record, \'model_name\'):\n                log_entry[\'model_name\'] = record.model_name\n        \n            return json.dumps(log_entry, ensure_ascii=False)\n  \n    def info(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.info(message, extra=extra)\n  \n    def error(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.error(message, extra=extra)\n  \n    def warning(self, message, **kwargs):\n        extra = {k: v for k, v in kwargs.items()}\n        self.logger.warning(message, extra=extra)\n\n# Usage example\nlogger = StructuredLogger("llm_service")\n\n@app.post("/generate")\nasync def generate_text(request: GenerateRequest):\n    request_id = str(uuid.uuid4())\n  \n    logger.info(\n        "Received generation request",\n        request_id=request_id,\n        prompt_length=len(request.prompt),\n        max_length=request.max_length,\n        temperature=request.temperature\n    )\n  \n    try:\n        start_time = time.time()\n        result = generator(request.prompt, ...)\n        generation_time = time.time() - start_time\n    \n        logger.info(\n            "Generation completed",\n            request_id=request_id,\n            generation_time=generation_time,\n            output_length=len(result[0]["generated_text"])\n        )\n    \n        return result\n    \n    except Exception as e:\n        logger.error(\n            "Generation failed",\n            request_id=request_id,\n            error=str(e),\n            error_type=type(e).__name__\n        )\n        raise\n'})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,r.jsx)(n.h3,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Memory Management"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use gradient checkpointing to reduce memory usage"}),"\n",(0,r.jsx)(n.li,{children:"Enable CPU offloading for large models"}),"\n",(0,r.jsx)(n.li,{children:"Set appropriate batch size and sequence length"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training Acceleration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use FlashAttention-2"}),"\n",(0,r.jsx)(n.li,{children:"Enable mixed precision training (FP16/BF16)"}),"\n",(0,r.jsx)(n.li,{children:"Use DeepSpeed ZeRO optimization"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Inference Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model quantization (GPTQ/AWQ)"}),"\n",(0,r.jsx)(n.li,{children:"Use vLLM for efficient inference"}),"\n",(0,r.jsx)(n.li,{children:"Batch requests to improve throughput"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Security"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Anonymize training data"}),"\n",(0,r.jsx)(n.li,{children:"Filter model output content"}),"\n",(0,r.jsx)(n.li,{children:"Validate and sanitize user inputs"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Security"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Regular backup of model checkpoints"}),"\n",(0,r.jsx)(n.li,{children:"Version control and rollback mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Access control and permissions"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deployment Security"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"API authentication and authorization"}),"\n",(0,r.jsx)(n.li,{children:"Request rate limiting"}),"\n",(0,r.jsx)(n.li,{children:"Network security configuration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"cost-control",children:"Cost Control"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Compute Resources"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use Spot instances to reduce costs"}),"\n",(0,r.jsx)(n.li,{children:"Auto-scaling based on load"}),"\n",(0,r.jsx)(n.li,{children:"Choose appropriate GPU models"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Storage Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model compression and quantization"}),"\n",(0,r.jsx)(n.li,{children:"Data deduplication and compression"}),"\n",(0,r.jsx)(n.li,{children:"Tiered storage for hot/cold data"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This complete guide covers the entire workflow from environment setup to production deployment, and you can choose the appropriate technical solutions based on specific requirements."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Last updated: January 2025"})})]})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(96540);const r={},i=a.createContext(r);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(i.Provider,{value:n},e.children)}},90087:e=>{e.exports=JSON.parse('{"permalink":"/zh-Hans/blog/local-llm-training-guide-en","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-20-local-llm-training-guide.md","source":"@site/blog/2024-01-20-local-llm-training-guide.md","title":"Guide to Local LLM","description":"This comprehensive guide covers the complete process of deploying, training, and fine-tuning large language models in local environments, from environment setup to production deployment.","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"LLM","permalink":"/zh-Hans/blog/tags/llm","description":"Large Language Models"},{"inline":false,"label":"AI","permalink":"/zh-Hans/blog/tags/ai","description":"Artificial Intelligence"},{"inline":false,"label":"Training","permalink":"/zh-Hans/blog/tags/training","description":"Model training and optimization"},{"inline":false,"label":"Fine-tuning","permalink":"/zh-Hans/blog/tags/fine-tuning","description":"Model fine-tuning techniques"},{"inline":false,"label":"Local Deployment","permalink":"/zh-Hans/blog/tags/local-deployment","description":"Local model deployment"},{"inline":false,"label":"Machine Learning","permalink":"/zh-Hans/blog/tags/machine-learning","description":"Machine learning techniques and applications"}],"readingTime":14.86,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"Ph.D. Candidate @ SHZU @CAS-Cemps","url":"https://github.com/smiler488","page":{"permalink":"/zh-Hans/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"local-llm-training-guide-en","title":"Guide to Local LLM","authors":["liangchao"],"tags":["LLM","AI","training","fine-tuning","local deployment","machine learning"]},"unlisted":false,"prevItem":{"title":"Guide to 3D Reconstruction with AI","permalink":"/zh-Hans/blog/hunyuan3d-plant-reconstruction-guide"},"nextItem":{"title":"Canopy Photosynthesis Modeling","permalink":"/zh-Hans/blog/canopy-photosynthesis-modeling-en"}}')}}]);