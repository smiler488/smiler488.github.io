"use strict";(self.webpackChunkliangchao_website=self.webpackChunkliangchao_website||[]).push([[7985],{261:n=>{n.exports=JSON.parse('{"permalink":"/zh-Hans/blog/pytorch-ml-dl-tutorial","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-09-20-pytorch-ml-dl-tutorial.md","source":"@site/blog/2022-09-20-pytorch-ml-dl-tutorial.md","title":"PyTorch Tutorial","description":"Introduction","date":"2022-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"PyTorch","permalink":"/zh-Hans/blog/tags/pytorch","description":"PyTorch deep learning framework"},{"inline":false,"label":"Machine Learning","permalink":"/zh-Hans/blog/tags/machine-learning","description":"Machine learning techniques and applications"},{"inline":false,"label":"Deep Learning","permalink":"/zh-Hans/blog/tags/deep-learning","description":"Deep learning techniques and neural networks"},{"inline":false,"label":"Neural Networks","permalink":"/zh-Hans/blog/tags/neural-networks","description":"Neural network architectures and implementations"},{"inline":false,"label":"Tutorial","permalink":"/zh-Hans/blog/tags/tutorial","description":"Tutorial tag description"}],"readingTime":25.58,"hasTruncateMarker":true,"authors":[{"name":"Liangchao Deng","title":"Ph.D. Candidate @ SHZU @CAS-Cemps","url":"https://github.com/smiler488","page":{"permalink":"/zh-Hans/blog/authors/liangchao"},"socials":{"x":"https://x.com/smiler488","github":"https://github.com/smiler488"},"imageURL":"https://github.com/smiler488.png","key":"liangchao"}],"frontMatter":{"slug":"pytorch-ml-dl-tutorial","title":"PyTorch Tutorial","authors":["liangchao"],"tags":["pytorch","machine learning","deep learning","neural networks","tutorial"]},"unlisted":false,"prevItem":{"title":"Canopy Photosynthesis Modeling","permalink":"/zh-Hans/blog/canopy-photosynthesis-modeling-en"},"nextItem":{"title":"GitHub Beginner Guide","permalink":"/zh-Hans/blog/gitHub-beginner-guide"}}')},5379:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>d});var t=s(261),a=s(4848),r=s(8453);const i={slug:"pytorch-ml-dl-tutorial",title:"PyTorch Tutorial",authors:["liangchao"],tags:["pytorch","machine learning","deep learning","neural networks","tutorial"]},o="Complete PyTorch Tutorial for Machine Learning and Deep Learning",l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Table of Contents",id:"table-of-contents",level:2},{value:"PyTorch Fundamentals",id:"pytorch-fundamentals",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Tensor Operations",id:"basic-tensor-operations",level:3},{value:"Data Handling and Preprocessing",id:"data-handling-and-preprocessing",level:2},{value:"Dataset and DataLoader",id:"dataset-and-dataloader",level:3},{value:"Building Neural Networks",id:"building-neural-networks",level:2},{value:"Basic Neural Network Components",id:"basic-neural-network-components",level:3},{value:"Training and Optimization",id:"training-and-optimization",level:2},{value:"Training Loop Implementation",id:"training-loop-implementation",level:3},{value:"Computer Vision with PyTorch",id:"computer-vision-with-pytorch",level:2},{value:"Transfer Learning and Fine-tuning",id:"transfer-learning-and-fine-tuning",level:3},{value:"Natural Language Processing",id:"natural-language-processing",level:2},{value:"Text Processing and RNN/Transformer Models",id:"text-processing-and-rnntransformer-models",level:3},{value:"Advanced Topics",id:"advanced-topics",level:2},{value:"Custom Loss Functions and Metrics",id:"custom-loss-functions-and-metrics",level:3},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Model Optimization and Deployment",id:"model-optimization-and-deployment",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Core Concepts Covered",id:"core-concepts-covered",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"Next Steps",id:"next-steps",level:3}];function c(n){const e={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"PyTorch is one of the most popular deep learning frameworks, known for its dynamic computation graphs, intuitive API, and strong community support. This comprehensive tutorial covers everything from basic tensor operations to advanced deep learning architectures, providing practical examples and best practices for both machine learning and deep learning applications."}),"\n",(0,a.jsx)(e.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#pytorch-fundamentals",children:"PyTorch Fundamentals"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#data-handling-and-preprocessing",children:"Data Handling and Preprocessing"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#building-neural-networks",children:"Building Neural Networks"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#training-and-optimization",children:"Training and Optimization"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#computer-vision-with-pytorch",children:"Computer Vision with PyTorch"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#natural-language-processing",children:"Natural Language Processing"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#advanced-topics",children:"Advanced Topics"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#production-deployment",children:"Production Deployment"})}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"pytorch-fundamentals",children:"PyTorch Fundamentals"}),"\n",(0,a.jsx)(e.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Install PyTorch with CUDA support\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CPU-only installation\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Additional packages\npip install numpy matplotlib scikit-learn pandas seaborn jupyter\n"})}),"\n",(0,a.jsx)(e.h3,{id:"basic-tensor-operations",children:"Basic Tensor Operations"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version and CUDA availability\nprint(f"PyTorch version: {torch.__version__}")\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"CUDA version: {torch.version.cuda}")\n\n# Creating tensors\ndef tensor_basics():\n    # Different ways to create tensors\n    x1 = torch.tensor([1, 2, 3, 4, 5])\n    x2 = torch.zeros(3, 4)\n    x3 = torch.ones(2, 3)\n    x4 = torch.randn(2, 3)  # Random normal distribution\n    x5 = torch.arange(0, 10, 2)  # Range tensor\n    \n    print("Basic tensor creation:")\n    print(f"x1: {x1}")\n    print(f"x2 shape: {x2.shape}")\n    print(f"x4: {x4}")\n    \n    # Tensor properties\n    print(f"\\nTensor properties:")\n    print(f"Data type: {x4.dtype}")\n    print(f"Device: {x4.device}")\n    print(f"Shape: {x4.shape}")\n    print(f"Number of dimensions: {x4.ndim}")\n    \n    # Moving tensors to GPU\n    if torch.cuda.is_available():\n        x4_gpu = x4.cuda()\n        print(f"GPU tensor device: {x4_gpu.device}")\n    \n    return x1, x2, x3, x4, x5\n\n# Tensor operations\ndef tensor_operations():\n    # Basic arithmetic operations\n    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n    \n    # Element-wise operations\n    add_result = a + b\n    mul_result = a * b\n    div_result = a / b\n    \n    # Matrix operations\n    matmul_result = torch.matmul(a, b)\n    transpose_result = a.t()\n    \n    print("Tensor operations:")\n    print(f"Addition: \\n{add_result}")\n    print(f"Matrix multiplication: \\n{matmul_result}")\n    print(f"Transpose: \\n{transpose_result}")\n    \n    # Reshaping and indexing\n    x = torch.randn(4, 6)\n    x_reshaped = x.view(2, 12)  # Reshape\n    x_slice = x[:2, :3]  # Slicing\n    \n    print(f"\\nOriginal shape: {x.shape}")\n    print(f"Reshaped: {x_reshaped.shape}")\n    print(f"Sliced: {x_slice.shape}")\n    \n    return a, b, add_result, matmul_result\n\n# Automatic differentiation\ndef autograd_example():\n    # Enable gradient computation\n    x = torch.tensor(2.0, requires_grad=True)\n    y = torch.tensor(3.0, requires_grad=True)\n    \n    # Forward pass\n    z = x**2 + y**3\n    \n    # Backward pass\n    z.backward()\n    \n    print("Automatic differentiation:")\n    print(f"x.grad: {x.grad}")  # dz/dx = 2x = 4\n    print(f"y.grad: {y.grad}")  # dz/dy = 3y^2 = 27\n    \n    # More complex example\n    x = torch.randn(3, requires_grad=True)\n    y = x * 2\n    while y.data.norm() < 1000:\n        y = y * 2\n    \n    print(f"\\nFinal y: {y}")\n    \n    # Compute gradients\n    v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n    y.backward(v)\n    print(f"x.grad: {x.grad}")\n\n# Run basic examples\ntensor_basics()\ntensor_operations()\nautograd_example()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"data-handling-and-preprocessing",children:"Data Handling and Preprocessing"}),"\n",(0,a.jsx)(e.h3,{id:"dataset-and-dataloader",children:"Dataset and DataLoader"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\nimport os\nfrom PIL import Image\n\n# Custom Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        target = self.targets[idx]\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample, target\n\n# Image dataset example\nclass ImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        \n        # Assuming directory structure: root_dir/class_name/image.jpg\n        for class_idx, class_name in enumerate(os.listdir(root_dir)):\n            class_path = os.path.join(root_dir, class_name)\n            if os.path.isdir(class_path):\n                for img_name in os.listdir(class_path):\n                    if img_name.endswith(('.png', '.jpg', '.jpeg')):\n                        self.images.append(os.path.join(class_path, img_name))\n                        self.labels.append(class_idx)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Data preprocessing and augmentation\ndef create_data_loaders():\n    # Define transforms\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    # For demonstration, using CIFAR-10\n    train_dataset = datasets.CIFAR10(\n        root='./data', \n        train=True, \n        download=True, \n        transform=train_transform\n    )\n    \n    val_dataset = datasets.CIFAR10(\n        root='./data', \n        train=False, \n        download=True, \n        transform=val_transform\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=32, \n        shuffle=True, \n        num_workers=4,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=32, \n        shuffle=False, \n        num_workers=4,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader\n\n# Data exploration\ndef explore_data(data_loader):\n    # Get a batch of data\n    data_iter = iter(data_loader)\n    images, labels = next(data_iter)\n    \n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n    print(f\"Data type: {images.dtype}\")\n    print(f\"Label range: {labels.min()} to {labels.max()}\")\n    \n    # Visualize some samples\n    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n    for i in range(8):\n        row, col = i // 4, i % 4\n        img = images[i].permute(1, 2, 0)  # Change from CHW to HWC\n        # Denormalize for visualization\n        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n        img = torch.clamp(img, 0, 1)\n        \n        axes[row, col].imshow(img)\n        axes[row, col].set_title(f'Label: {labels[i].item()}')\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Create and explore data\ntrain_loader, val_loader = create_data_loaders()\nexplore_data(train_loader)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"building-neural-networks",children:"Building Neural Networks"}),"\n",(0,a.jsx)(e.h3,{id:"basic-neural-network-components",children:"Basic Neural Network Components"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Simple feedforward neural network\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n# Convolutional Neural Network\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # Pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Batch normalization\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        \n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        # First conv block\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        \n        # Second conv block\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        \n        # Third conv block\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        \n        # Flatten\n        x = x.view(-1, 128 * 4 * 4)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# ResNet-like block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                              stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                              stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Shortcut connection\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n                         stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        residual = x\n        \n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        \n        out += self.shortcut(residual)\n        out = F.relu(out)\n        \n        return out\n\n# Custom ResNet\nclass CustomResNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CustomResNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        # Residual layers\n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n    \n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(ResidualBlock(in_channels, out_channels, stride))\n        \n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        \n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n\n# Model initialization and summary\ndef initialize_model(model_type='cnn', num_classes=10):\n    if model_type == 'simple':\n        model = SimpleNN(input_size=32*32*3, hidden_size=512, num_classes=num_classes)\n    elif model_type == 'cnn':\n        model = CNN(num_classes=num_classes)\n    elif model_type == 'resnet':\n        model = CustomResNet(num_classes=num_classes)\n    else:\n        raise ValueError(\"Unknown model type\")\n    \n    # Initialize weights\n    def init_weights(m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n        elif isinstance(m, nn.Conv2d):\n            torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    \n    model.apply(init_weights)\n    \n    # Model summary\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"Model: {model_type}\")\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    return model\n\n# Test model creation\nmodel = initialize_model('resnet')\nprint(model)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"training-and-optimization",children:"Training and Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"training-loop-implementation",children:"Training Loop Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import time\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.device = device\n        \n        # Move model to device\n        self.model.to(device)\n        \n        # Training history\n        self.train_losses = []\n        self.train_accuracies = []\n        self.val_losses = []\n        self.val_accuracies = []\n    \n    def train_epoch(self):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(self.train_loader, desc='Training')\n        for batch_idx, (data, target) in enumerate(pbar):\n            data, target = data.to(self.device), target.to(self.device)\n            \n            # Zero gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            output = self.model(data)\n            loss = self.criterion(output, target)\n            \n            # Backward pass\n            loss.backward()\n            self.optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        epoch_loss = running_loss / len(self.train_loader)\n        epoch_acc = 100. * correct / total\n        \n        return epoch_loss, epoch_acc\n    \n    def validate_epoch(self):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc='Validation')\n            for data, target in pbar:\n                data, target = data.to(self.device), target.to(self.device)\n                \n                output = self.model(data)\n                loss = self.criterion(output, target)\n                \n                running_loss += loss.item()\n                _, predicted = output.max(1)\n                total += target.size(0)\n                correct += predicted.eq(target).sum().item()\n                \n                pbar.set_postfix({\n                    'Loss': f'{running_loss/(len(pbar.iterable)):.4f}',\n                    'Acc': f'{100.*correct/total:.2f}%'\n                })\n        \n        epoch_loss = running_loss / len(self.val_loader)\n        epoch_acc = 100. * correct / total\n        \n        return epoch_loss, epoch_acc\n    \n    def train(self, num_epochs, scheduler=None, early_stopping_patience=None):\n        best_val_acc = 0.0\n        patience_counter = 0\n        \n        for epoch in range(num_epochs):\n            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n            print('-' * 50)\n            \n            # Training phase\n            train_loss, train_acc = self.train_epoch()\n            \n            # Validation phase\n            val_loss, val_acc = self.validate_epoch()\n            \n            # Update learning rate\n            if scheduler:\n                scheduler.step(val_loss)\n            \n            # Save metrics\n            self.train_losses.append(train_loss)\n            self.train_accuracies.append(train_acc)\n            self.val_losses.append(val_loss)\n            self.val_accuracies.append(val_acc)\n            \n            # Print epoch results\n            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n            \n            # Early stopping\n            if early_stopping_patience:\n                if val_acc > best_val_acc:\n                    best_val_acc = val_acc\n                    patience_counter = 0\n                    # Save best model\n                    torch.save(self.model.state_dict(), 'best_model.pth')\n                else:\n                    patience_counter += 1\n                    if patience_counter >= early_stopping_patience:\n                        print(f'Early stopping triggered after {epoch+1} epochs')\n                        break\n        \n        print(f'\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%')\n    \n    def plot_training_history(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        \n        # Plot losses\n        ax1.plot(self.train_losses, label='Train Loss')\n        ax1.plot(self.val_losses, label='Validation Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True)\n        \n        # Plot accuracies\n        ax2.plot(self.train_accuracies, label='Train Accuracy')\n        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n        ax2.set_title('Training and Validation Accuracy')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.legend()\n        ax2.grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n\n# Advanced optimization techniques\ndef setup_training(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Optimizer with different options\n    optimizer = optim.AdamW(\n        model.parameters(), \n        lr=0.001, \n        weight_decay=0.01,\n        betas=(0.9, 0.999)\n    )\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=5, \n        verbose=True\n    )\n    \n    # Alternative schedulers\n    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n    \n    # Create trainer\n    trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    return trainer, scheduler\n\n# Mixed precision training\nclass MixedPrecisionTrainer(Trainer):\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n        super().__init__(model, train_loader, val_loader, criterion, optimizer, device)\n        self.scaler = torch.cuda.amp.GradScaler()\n    \n    def train_epoch(self):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(self.train_loader, desc='Training (Mixed Precision)')\n        for batch_idx, (data, target) in enumerate(pbar):\n            data, target = data.to(self.device), target.to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast():\n                output = self.model(data)\n                loss = self.criterion(output, target)\n            \n            # Mixed precision backward pass\n            self.scaler.scale(loss).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n            \n            pbar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        epoch_loss = running_loss / len(self.train_loader)\n        epoch_acc = 100. * correct / total\n        \n        return epoch_loss, epoch_acc\n\n# Example training setup and execution\nmodel = initialize_model('resnet', num_classes=10)\ntrainer, scheduler = setup_training(model, train_loader, val_loader)\n\n# Train the model\ntrainer.train(num_epochs=50, scheduler=scheduler, early_stopping_patience=10)\n\n# Plot training history\ntrainer.plot_training_history()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"computer-vision-with-pytorch",children:"Computer Vision with PyTorch"}),"\n",(0,a.jsx)(e.h3,{id:"transfer-learning-and-fine-tuning",children:"Transfer Learning and Fine-tuning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torchvision.models as models\nfrom torchvision.models import ResNet50_Weights\n\n# Transfer learning with pre-trained models\nclass TransferLearningModel(nn.Module):\n    def __init__(self, num_classes, model_name=\'resnet50\', pretrained=True):\n        super(TransferLearningModel, self).__init__()\n        \n        if model_name == \'resnet50\':\n            self.backbone = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n            num_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Linear(num_features, num_classes)\n            \n        elif model_name == \'efficientnet\':\n            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n            num_features = self.backbone.classifier[1].in_features\n            self.backbone.classifier[1] = nn.Linear(num_features, num_classes)\n            \n        elif model_name == \'vit\':\n            self.backbone = models.vit_b_16(pretrained=pretrained)\n            num_features = self.backbone.heads.head.in_features\n            self.backbone.heads.head = nn.Linear(num_features, num_classes)\n    \n    def forward(self, x):\n        return self.backbone(x)\n    \n    def freeze_backbone(self):\n        """Freeze backbone parameters for feature extraction"""\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        \n        # Unfreeze classifier\n        if hasattr(self.backbone, \'fc\'):\n            for param in self.backbone.fc.parameters():\n                param.requires_grad = True\n        elif hasattr(self.backbone, \'classifier\'):\n            for param in self.backbone.classifier.parameters():\n                param.requires_grad = True\n    \n    def unfreeze_backbone(self):\n        """Unfreeze all parameters for fine-tuning"""\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n# Object detection with YOLO-style architecture\nclass SimpleYOLO(nn.Module):\n    def __init__(self, num_classes, num_anchors=3):\n        super(SimpleYOLO, self).__init__()\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        \n        # Backbone\n        self.backbone = models.resnet18(pretrained=True)\n        self.backbone.fc = nn.Identity()  # Remove final FC layer\n        \n        # Detection head\n        self.conv1 = nn.Conv2d(512, 256, 3, padding=1)\n        self.conv2 = nn.Conv2d(256, 128, 3, padding=1)\n        self.conv3 = nn.Conv2d(128, num_anchors * (5 + num_classes), 1)\n        \n    def forward(self, x):\n        # Extract features\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        \n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n        \n        # Detection head\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.conv3(x)\n        \n        return x\n\n# Semantic segmentation with U-Net\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, num_classes=1):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.enc1 = self.conv_block(in_channels, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n        \n        # Decoder\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n        self.dec4 = self.conv_block(1024, 512)\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.dec3 = self.conv_block(512, 256)\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.dec2 = self.conv_block(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.dec1 = self.conv_block(128, 64)\n        \n        # Final layer\n        self.final = nn.Conv2d(64, num_classes, 1)\n        \n        self.pool = nn.MaxPool2d(2)\n    \n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool(enc1))\n        enc3 = self.enc3(self.pool(enc2))\n        enc4 = self.enc4(self.pool(enc3))\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(self.pool(enc4))\n        \n        # Decoder\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.dec4(dec4)\n        \n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.dec3(dec3)\n        \n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.dec2(dec2)\n        \n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.dec1(dec1)\n        \n        return torch.sigmoid(self.final(dec1))\n\n# Image augmentation and preprocessing\nclass AdvancedAugmentation:\n    def __init__(self):\n        self.train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.2),\n            transforms.RandomRotation(degrees=15),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.RandomGrayscale(p=0.1),\n            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n        ])\n        \n        self.val_transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n# Example: Fine-tuning a pre-trained model\ndef fine_tune_model():\n    # Create transfer learning model\n    model = TransferLearningModel(num_classes=10, model_name=\'resnet50\', pretrained=True)\n    \n    # Phase 1: Feature extraction (freeze backbone)\n    model.freeze_backbone()\n    \n    # Setup optimizer for feature extraction\n    optimizer = optim.Adam(model.backbone.fc.parameters(), lr=0.001)\n    \n    print("Phase 1: Feature extraction training")\n    # Train for a few epochs...\n    \n    # Phase 2: Fine-tuning (unfreeze backbone)\n    model.unfreeze_backbone()\n    \n    # Setup optimizer for fine-tuning with lower learning rate\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    \n    print("Phase 2: Fine-tuning training")\n    # Continue training...\n    \n    return model\n\n# Test computer vision models\ntransfer_model = fine_tune_model()\nunet_model = UNet(in_channels=3, num_classes=21)  # For Pascal VOC\nyolo_model = SimpleYOLO(num_classes=80)  # For COCO\n\nprint(f"Transfer learning model parameters: {sum(p.numel() for p in transfer_model.parameters()):,}")\nprint(f"U-Net model parameters: {sum(p.numel() for p in unet_model.parameters()):,}")\nprint(f"YOLO model parameters: {sum(p.numel() for p in yolo_model.parameters()):,}")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"natural-language-processing",children:"Natural Language Processing"}),"\n",(0,a.jsx)(e.h3,{id:"text-processing-and-rnntransformer-models",children:"Text Processing and RNN/Transformer Models"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch.nn.utils.rnn as rnn_utils\nfrom collections import Counter\nimport re\n\n# Text preprocessing utilities\nclass TextPreprocessor:\n    def __init__(self, vocab_size=10000, min_freq=2):\n        self.vocab_size = vocab_size\n        self.min_freq = min_freq\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab = set()\n    \n    def build_vocab(self, texts):\n        # Tokenize and count words\n        word_counts = Counter()\n        for text in texts:\n            tokens = self.tokenize(text)\n            word_counts.update(tokens)\n        \n        # Build vocabulary\n        vocab_words = [word for word, count in word_counts.most_common(self.vocab_size-4) \n                      if count >= self.min_freq]\n        \n        # Special tokens\n        special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n        self.vocab = set(special_tokens + vocab_words)\n        \n        # Create mappings\n        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n        \n        print(f\"Vocabulary size: {len(self.vocab)}\")\n    \n    def tokenize(self, text):\n        # Simple tokenization\n        text = text.lower()\n        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n        return text.split()\n    \n    def text_to_indices(self, text):\n        tokens = self.tokenize(text)\n        return [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n    \n    def indices_to_text(self, indices):\n        return ' '.join([self.idx2word.get(idx, '<UNK>') for idx in indices])\n\n# LSTM-based language model\nclass LSTMLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.2):\n        super(LSTMLanguageModel, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Layers\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n                           batch_first=True, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, x, hidden=None):\n        # Embedding\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n        \n        # LSTM\n        lstm_out, hidden = self.lstm(embedded, hidden)\n        lstm_out = self.dropout(lstm_out)\n        \n        # Output projection\n        output = self.fc(lstm_out)\n        \n        return output, hidden\n    \n    def init_hidden(self, batch_size, device):\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return (h0, c0)\n\n# Transformer-based model\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_len, dropout=0.1):\n        super(TransformerModel, self).__init__()\n        \n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.max_seq_len = max_seq_len\n        \n        # Embeddings\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = self.create_positional_encoding(max_seq_len, d_model)\n        \n        # Transformer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        \n        # Output layer\n        self.fc = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def create_positional_encoding(self, max_seq_len, d_model):\n        pe = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(torch.log(torch.tensor(10000.0)) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        return pe.unsqueeze(0)\n    \n    def forward(self, x, mask=None):\n        seq_len = x.size(1)\n        \n        # Embedding + positional encoding\n        x = self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)\n        x = self.dropout(x)\n        \n        # Transformer\n        if mask is None:\n            mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n        \n        output = self.transformer(x, mask)\n        output = self.fc(output)\n        \n        return output\n    \n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n# Attention mechanism\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear transformations\n        Q = self.W_q(query)\n        K = self.W_k(key)\n        V = self.W_v(value)\n        \n        # Reshape for multi-head attention\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Apply attention\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model\n        )\n        \n        # Final linear transformation\n        output = self.W_o(attention_output)\n        \n        return output, attention_weights\n\n# Text classification model\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers=2):\n        super(TextClassifier, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n                           batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n    \n    def forward(self, x):\n        # Embedding\n        embedded = self.embedding(x)\n        \n        # LSTM\n        lstm_out, (hidden, _) = self.lstm(embedded)\n        \n        # Use last hidden state from both directions\n        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n        hidden = self.dropout(hidden)\n        \n        # Classification\n        output = self.fc(hidden)\n        \n        return output\n\n# Example usage\ndef train_nlp_model():\n    # Sample data\n    texts = [\n        \"This is a sample sentence for training.\",\n        \"Natural language processing with PyTorch is powerful.\",\n        \"Deep learning models can understand text patterns.\"\n    ]\n    \n    # Preprocess text\n    preprocessor = TextPreprocessor(vocab_size=1000)\n    preprocessor.build_vocab(texts)\n    \n    # Create models\n    vocab_size = len(preprocessor.vocab)\n    \n    # LSTM Language Model\n    lstm_model = LSTMLanguageModel(\n        vocab_size=vocab_size,\n        embed_size=128,\n        hidden_size=256,\n        num_layers=2\n    )\n    \n    # Transformer Model\n    transformer_model = TransformerModel(\n        vocab_size=vocab_size,\n        d_model=128,\n        nhead=8,\n        num_layers=6,\n        dim_feedforward=512,\n        max_seq_len=100\n    )\n    \n    # Text Classifier\n    classifier_model = TextClassifier(\n        vocab_size=vocab_size,\n        embed_size=128,\n        hidden_size=256,\n        num_classes=3\n    )\n    \n    print(f\"LSTM model parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n    print(f\"Transformer model parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\n    print(f\"Classifier model parameters: {sum(p.numel() for p in classifier_model.parameters()):,}\")\n    \n    return lstm_model, transformer_model, classifier_model\n\n# Test NLP models\nlstm_model, transformer_model, classifier_model = train_nlp_model()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-topics",children:"Advanced Topics"}),"\n",(0,a.jsx)(e.h3,{id:"custom-loss-functions-and-metrics",children:"Custom Loss Functions and Metrics"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Custom loss functions\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    \n    def forward(self, inputs, targets):\n        inputs = torch.sigmoid(inputs)\n        \n        # Flatten tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n        \n        return 1 - dice\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n    \n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n                                    label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n        return loss_contrastive\n\n# Custom metrics\nclass MetricsCalculator:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.reset()\n    \n    def reset(self):\n        self.confusion_matrix = torch.zeros(self.num_classes, self.num_classes)\n        self.total_samples = 0\n        self.correct_predictions = 0\n    \n    def update(self, predictions, targets):\n        _, predicted = torch.max(predictions, 1)\n        self.total_samples += targets.size(0)\n        self.correct_predictions += (predicted == targets).sum().item()\n        \n        # Update confusion matrix\n        for t, p in zip(targets.view(-1), predicted.view(-1)):\n            self.confusion_matrix[t.long(), p.long()] += 1\n    \n    def accuracy(self):\n        return self.correct_predictions / self.total_samples\n    \n    def precision(self, class_idx=None):\n        if class_idx is not None:\n            tp = self.confusion_matrix[class_idx, class_idx]\n            fp = self.confusion_matrix[:, class_idx].sum() - tp\n            return tp / (tp + fp) if (tp + fp) > 0 else 0\n        else:\n            precisions = []\n            for i in range(self.num_classes):\n                precisions.append(self.precision(i))\n            return sum(precisions) / len(precisions)\n    \n    def recall(self, class_idx=None):\n        if class_idx is not None:\n            tp = self.confusion_matrix[class_idx, class_idx]\n            fn = self.confusion_matrix[class_idx, :].sum() - tp\n            return tp / (tp + fn) if (tp + fn) > 0 else 0\n        else:\n            recalls = []\n            for i in range(self.num_classes):\n                recalls.append(self.recall(i))\n            return sum(recalls) / len(recalls)\n    \n    def f1_score(self, class_idx=None):\n        if class_idx is not None:\n            p = self.precision(class_idx)\n            r = self.recall(class_idx)\n            return 2 * p * r / (p + r) if (p + r) > 0 else 0\n        else:\n            f1_scores = []\n            for i in range(self.num_classes):\n                f1_scores.append(self.f1_score(i))\n            return sum(f1_scores) / len(f1_scores)\n\n# Model interpretability\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks\n        self.target_layer.register_forward_hook(self.save_activation)\n        self.target_layer.register_backward_hook(self.save_gradient)\n    \n    def save_activation(self, module, input, output):\n        self.activations = output\n    \n    def save_gradient(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0]\n    \n    def generate_cam(self, input_image, class_idx):\n        # Forward pass\n        output = self.model(input_image)\n        \n        # Backward pass\n        self.model.zero_grad()\n        class_loss = output[0, class_idx]\n        class_loss.backward()\n        \n        # Generate CAM\n        gradients = self.gradients[0]\n        activations = self.activations[0]\n        \n        weights = torch.mean(gradients, dim=(1, 2))\n        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)\n        \n        for i, w in enumerate(weights):\n            cam += w * activations[i]\n        \n        cam = F.relu(cam)\n        cam = cam / torch.max(cam)\n        \n        return cam\n\n# Regularization techniques\nclass DropBlock2D(nn.Module):\n    def __init__(self, drop_rate, block_size):\n        super(DropBlock2D, self).__init__()\n        self.drop_rate = drop_rate\n        self.block_size = block_size\n    \n    def forward(self, x):\n        if not self.training:\n            return x\n        \n        gamma = self.drop_rate / (self.block_size ** 2)\n        mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n        \n        # Expand mask\n        mask = mask.unsqueeze(1)\n        mask = F.max_pool2d(mask, (self.block_size, self.block_size), \n                           stride=(1, 1), padding=self.block_size // 2)\n        \n        mask = 1 - mask\n        normalize_factor = mask.numel() / mask.sum()\n        \n        return x * mask * normalize_factor\n\nclass MixUp:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n    \n    def __call__(self, x, y):\n        if self.alpha > 0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1\n        \n        batch_size = x.size(0)\n        index = torch.randperm(batch_size)\n        \n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n        \n        return mixed_x, y_a, y_b, lam\n\n# Knowledge distillation\nclass DistillationLoss(nn.Module):\n    def __init__(self, alpha=0.5, temperature=4):\n        super(DistillationLoss, self).__init__()\n        self.alpha = alpha\n        self.temperature = temperature\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n    \n    def forward(self, student_outputs, teacher_outputs, targets):\n        # Soft targets from teacher\n        soft_targets = F.softmax(teacher_outputs / self.temperature, dim=1)\n        soft_student = F.log_softmax(student_outputs / self.temperature, dim=1)\n        \n        # Distillation loss\n        distillation_loss = self.kl_div(soft_student, soft_targets) * (self.temperature ** 2)\n        \n        # Student loss\n        student_loss = self.ce_loss(student_outputs, targets)\n        \n        # Combined loss\n        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n        \n        return total_loss\n\n# Example usage of advanced techniques\ndef advanced_training_example():\n    # Create model\n    model = CNN(num_classes=10)\n    \n    # Custom loss\n    focal_loss = FocalLoss(alpha=1, gamma=2)\n    \n    # Metrics calculator\n    metrics = MetricsCalculator(num_classes=10)\n    \n    # MixUp augmentation\n    mixup = MixUp(alpha=1.0)\n    \n    # Training loop with advanced techniques\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Apply MixUp\n        mixed_data, target_a, target_b, lam = mixup(data, target)\n        \n        # Forward pass\n        output = model(mixed_data)\n        \n        # Calculate loss with MixUp\n        loss = lam * focal_loss(output, target_a) + (1 - lam) * focal_loss(output, target_b)\n        \n        # Update metrics\n        metrics.update(output, target)\n        \n        # Backward pass\n        loss.backward()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n            print(f'Accuracy: {metrics.accuracy():.4f}')\n            print(f'F1 Score: {metrics.f1_score():.4f}')\n\n# Test advanced techniques\nadvanced_training_example()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"production-deployment",children:"Production Deployment"}),"\n",(0,a.jsx)(e.h3,{id:"model-optimization-and-deployment",children:"Model Optimization and Deployment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Model optimization techniques\ndef optimize_model_for_inference(model, example_input):\n    \"\"\"Optimize model for inference\"\"\"\n    \n    # 1. Set to evaluation mode\n    model.eval()\n    \n    # 2. Trace the model\n    traced_model = torch.jit.trace(model, example_input)\n    \n    # 3. Optimize for inference\n    traced_model = torch.jit.optimize_for_inference(traced_model)\n    \n    # 4. Save optimized model\n    traced_model.save('optimized_model.pt')\n    \n    return traced_model\n\n# Quantization\ndef quantize_model(model, data_loader):\n    \"\"\"Apply post-training quantization\"\"\"\n    \n    # Prepare model for quantization\n    model.eval()\n    model_fp32 = model\n    model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model\n    model_fp32_prepared = torch.quantization.prepare(model_fp32)\n    \n    # Calibrate with representative data\n    with torch.no_grad():\n        for data, _ in data_loader:\n            model_fp32_prepared(data)\n            break  # Use only one batch for calibration\n    \n    # Convert to quantized model\n    model_int8 = torch.quantization.convert(model_fp32_prepared)\n    \n    return model_int8\n\n# ONNX export\ndef export_to_onnx(model, example_input, onnx_path):\n    \"\"\"Export model to ONNX format\"\"\"\n    \n    model.eval()\n    \n    torch.onnx.export(\n        model,\n        example_input,\n        onnx_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'output': {0: 'batch_size'}\n        }\n    )\n    \n    print(f\"Model exported to {onnx_path}\")\n\n# TensorRT optimization (requires TensorRT)\ndef optimize_with_tensorrt(onnx_path, trt_path):\n    \"\"\"Optimize ONNX model with TensorRT\"\"\"\n    try:\n        import tensorrt as trt\n        \n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, logger)\n        \n        # Parse ONNX model\n        with open(onnx_path, 'rb') as model:\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return None\n        \n        # Build engine\n        config = builder.create_builder_config()\n        config.max_workspace_size = 1 << 30  # 1GB\n        config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision\n        \n        engine = builder.build_engine(network, config)\n        \n        # Save engine\n        with open(trt_path, 'wb') as f:\n            f.write(engine.serialize())\n        \n        print(f\"TensorRT engine saved to {trt_path}\")\n        return engine\n        \n    except ImportError:\n        print(\"TensorRT not available\")\n        return None\n\n# Model serving with Flask\nfrom flask import Flask, request, jsonify\nimport base64\nfrom PIL import Image\nimport io\n\nclass ModelServer:\n    def __init__(self, model_path, device='cpu'):\n        self.device = device\n        self.model = torch.jit.load(model_path, map_location=device)\n        self.model.eval()\n        \n        # Define preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize(224),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def predict(self, image):\n        # Preprocess image\n        if isinstance(image, str):  # Base64 encoded\n            image_data = base64.b64decode(image)\n            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n        \n        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n        \n        # Inference\n        with torch.no_grad():\n            output = self.model(input_tensor)\n            probabilities = F.softmax(output, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            confidence = probabilities[0][predicted_class].item()\n        \n        return {\n            'predicted_class': predicted_class,\n            'confidence': confidence,\n            'probabilities': probabilities[0].tolist()\n        }\n\n# Flask app\napp = Flask(__name__)\nmodel_server = ModelServer('optimized_model.pt')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        data = request.json\n        image_data = data['image']\n        \n        result = model_server.predict(image_data)\n        return jsonify(result)\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'healthy'})\n\n# Batch inference optimization\nclass BatchInferenceEngine:\n    def __init__(self, model_path, batch_size=32, device='cuda'):\n        self.model = torch.jit.load(model_path, map_location=device)\n        self.model.eval()\n        self.batch_size = batch_size\n        self.device = device\n        self.pending_requests = []\n    \n    def add_request(self, image, request_id):\n        self.pending_requests.append((image, request_id))\n        \n        if len(self.pending_requests) >= self.batch_size:\n            return self.process_batch()\n        return None\n    \n    def process_batch(self):\n        if not self.pending_requests:\n            return []\n        \n        # Prepare batch\n        images = []\n        request_ids = []\n        \n        for image, request_id in self.pending_requests:\n            images.append(image)\n            request_ids.append(request_id)\n        \n        # Convert to tensor\n        batch_tensor = torch.stack(images).to(self.device)\n        \n        # Inference\n        with torch.no_grad():\n            outputs = self.model(batch_tensor)\n            probabilities = F.softmax(outputs, dim=1)\n        \n        # Prepare results\n        results = []\n        for i, request_id in enumerate(request_ids):\n            predicted_class = torch.argmax(probabilities[i]).item()\n            confidence = probabilities[i][predicted_class].item()\n            \n            results.append({\n                'request_id': request_id,\n                'predicted_class': predicted_class,\n                'confidence': confidence\n            })\n        \n        # Clear pending requests\n        self.pending_requests = []\n        \n        return results\n\n# Performance monitoring\nclass PerformanceMonitor:\n    def __init__(self):\n        self.inference_times = []\n        self.memory_usage = []\n        self.throughput = []\n    \n    def log_inference(self, inference_time, memory_used, batch_size):\n        self.inference_times.append(inference_time)\n        self.memory_usage.append(memory_used)\n        self.throughput.append(batch_size / inference_time)\n    \n    def get_stats(self):\n        if not self.inference_times:\n            return {}\n        \n        return {\n            'avg_inference_time': np.mean(self.inference_times),\n            'p95_inference_time': np.percentile(self.inference_times, 95),\n            'avg_memory_usage': np.mean(self.memory_usage),\n            'avg_throughput': np.mean(self.throughput),\n            'total_requests': len(self.inference_times)\n        }\n\n# Example deployment workflow\ndef deployment_workflow():\n    # 1. Load trained model\n    model = CNN(num_classes=10)\n    model.load_state_dict(torch.load('best_model.pth'))\n    \n    # 2. Create example input\n    example_input = torch.randn(1, 3, 224, 224)\n    \n    # 3. Optimize model\n    optimized_model = optimize_model_for_inference(model, example_input)\n    \n    # 4. Quantize model (optional)\n    # quantized_model = quantize_model(model, val_loader)\n    \n    # 5. Export to ONNX\n    export_to_onnx(optimized_model, example_input, 'model.onnx')\n    \n    # 6. Optimize with TensorRT (optional)\n    # optimize_with_tensorrt('model.onnx', 'model.trt')\n    \n    # 7. Test inference\n    test_inference_performance(optimized_model, example_input)\n    \n    print(\"Deployment workflow completed!\")\n\ndef test_inference_performance(model, example_input, num_runs=100):\n    \"\"\"Test inference performance\"\"\"\n    model.eval()\n    \n    # Warmup\n    for _ in range(10):\n        with torch.no_grad():\n            _ = model(example_input)\n    \n    # Measure performance\n    start_time = time.time()\n    \n    for _ in range(num_runs):\n        with torch.no_grad():\n            _ = model(example_input)\n    \n    end_time = time.time()\n    \n    avg_time = (end_time - start_time) / num_runs\n    throughput = 1 / avg_time\n    \n    print(f\"Average inference time: {avg_time*1000:.2f} ms\")\n    print(f\"Throughput: {throughput:.2f} inferences/second\")\n\n# Run deployment workflow\ndeployment_workflow()\n\nif __name__ == '__main__':\n    # Start Flask server\n    app.run(host='0.0.0.0', port=5000, debug=False)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(e.p,{children:"This comprehensive PyTorch tutorial covers the essential aspects of machine learning and deep learning implementation, from basic tensor operations to production deployment. Key takeaways include:"}),"\n",(0,a.jsx)(e.h3,{id:"core-concepts-covered",children:"Core Concepts Covered"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"PyTorch Fundamentals"}),": Tensors, autograd, and basic operations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Handling"}),": Custom datasets, data loaders, and preprocessing"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Neural Networks"}),": From simple MLPs to advanced architectures"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Training"}),": Optimization, loss functions, and training loops"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Computer Vision"}),": CNNs, transfer learning, and specialized architectures"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"NLP"}),": RNNs, Transformers, and text processing"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Advanced Topics"}),": Custom losses, regularization, and interpretability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Production"}),": Model optimization, quantization, and deployment"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use appropriate data augmentation and preprocessing"}),"\n",(0,a.jsx)(e.li,{children:"Implement proper validation and early stopping"}),"\n",(0,a.jsx)(e.li,{children:"Monitor training with comprehensive metrics"}),"\n",(0,a.jsx)(e.li,{children:"Apply regularization techniques to prevent overfitting"}),"\n",(0,a.jsx)(e.li,{children:"Optimize models for production deployment"}),"\n",(0,a.jsx)(e.li,{children:"Use mixed precision training for efficiency"}),"\n",(0,a.jsx)(e.li,{children:"Implement proper error handling and logging"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Explore domain-specific applications"}),"\n",(0,a.jsx)(e.li,{children:"Implement state-of-the-art architectures"}),"\n",(0,a.jsx)(e.li,{children:"Experiment with distributed training"}),"\n",(0,a.jsx)(e.li,{children:"Learn about model compression techniques"}),"\n",(0,a.jsx)(e.li,{children:"Practice with real-world datasets"}),"\n",(0,a.jsx)(e.li,{children:"Contribute to open-source projects"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"This tutorial provides a solid foundation for building and deploying machine learning models with PyTorch. Continue practicing with different datasets and architectures to master these concepts."}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.em,{children:"Last updated: September 2025"})})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>i,x:()=>o});var t=s(6540);const a={},r=t.createContext(a);function i(n){const e=t.useContext(r);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);